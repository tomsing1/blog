{
  "hash": "94e48abe47222ab8c6719aa74ead5689",
  "result": {
    "markdown": "---\ntitle: \"Querying JSON files with AWS Athena and the noctua R package\"\nauthor: \"Thomas Sandmann\"\ndate: \"2024-04-17\"\nfreeze: true\ncategories: [R, AWS, TIL]\neditor:\n  markdown:\n    wrap: 72\nformat:\n  html:\n    toc: true\n    toc-depth: 4\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\neditor_options: \n  chunk_output_type: console\n---\n\n\n## tl;dr\n\nThis week, I learned how to query JSON files stored on AWS S3 with the\n[noctua R package](https://cran.r-project.org/package=noctua),\nan API for the \n[AWS Athena](https://docs.aws.amazon.com/athena/)\nservice.\n\n## Overview\n\nNormally, \n[duckdb](https://duckdb.org/docs/extensions/json.html)\nis my tool of choice for parsing and querying large numbers of JSON files,\nand when the files are available on my local system, it makes easy work of \nthis task. \n\nBut this week, I needed to process more than 20 thousand small\nJSON files stored on AWS S3. Instead of retrieving them first, I used the\nopportunity to learn about \n[AWS Athena](https://docs.aws.amazon.com/athena/),\na severless query service that makes it easy to analyze data in Amazon S3 using\nstandard SQL. (In other words, I am using a query engine that is located\nclose to the data, instead of downloading the data to bring it closer to my\nlocal `duckdb` query engine.)\n\nAthena supports CSV, JSON, or columnar data formats such as Apache \nParquet and Apache ORC, and enables ad-hoc queries without the need to set up\na database beforehand.\n\nThere are multiple ways to interact with AWS Athena from within R [^1]. Here, \nI am using the\n[noctua R package](https://dyfanjones.github.io/noctua/), which leverages\nthe \n[paws R package](https://www.paws-r-sdk.com/)\nunder the hood.\n\n[^1]: Dyfan Jones's blog features great introductions on how to get started\nwith the\n[RAthena R package](https://dyfanjones.me/post/athena-and-r-there-is-another-way/),\nwhich leverages the python \n[boto3 AWS API](https://aws.amazon.com/sdk-for-python/), \nor the\n[noctua R package](https://dyfanjones.me/post/r-owl-of-athena/) I am using in\nthis post.\n\n### Authentication with AWS\n\n::: {.callout-warning}\n\nPlease note that use of AWS services, including the S3 and Athena, \n[is not free](https://aws.amazon.com/pricing/)\nand requires you to create an account first. Storing the small example data and\nrunning the Athena queries in this tutorial may be free if you haven't exhausted\nthe free tier of your AWS services, yet. But please be aware of the potential\ncost of cloud computing with AWS.\n\n:::\n\nThe `paws` R package recognizes \n[numerous ways of authenticating with AWS](https://github.com/paws-r/paws/blob/main/docs/credentials.md).\nFor the following code to run, please ensure that you provided one of them, e.g.\nexported your `key` and `secret key` available as environmental variables, or\ncreated a credentials file, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(glue)\nlibrary(jsonlite)      # to create example JSON files\nlibrary(paws.storage)  # to copy our example JSON file on AWS S3\nlibrary(noctua)        # to interface with AWS Athena\n```\n:::\n\n\nIn this example, I am using the same AWS S3 bucket to store the JSON files\nI want to query (under the `example` prefix), and to store files generated\nby Athena (under the `athena` prefix), but you can use any location on S3 you\nhave permission to access.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkRegion <- \"us-west-2\"\nkDataBucket <- \"my-scratch-bucket\"\nkDataDir <- \"s3://my-scratch-bucket/example/\"\nkStagingDir <- \"s3://my-scratch-bucket/athena/\"\n```\n:::\n\n\n## Creating a set of example JSON files\n\nFirst, let's create a small set of JSON files, each containing a single record,\nby \n\n1. looping over the rows of the `mtcars` data.frame,\n2. writing a JSON file to the temporary directory, and \n3. copying it to AWS S3. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsvc <- paws.storage::s3(region = kRegion)   # <1>\n\ndata(mtcars)\nmtcars$name <- row.names(mtcars) # <2>\nfor (n in seq.int(nrow(mtcars))) {\n  # export the row to a json file\n  temp_file <- tempfile(fileext = \".json\")\n  jsonlite::write_json(  # <3>\n    x = mtcars[n, , drop = TRUE],\n    path = temp_file,\n    pretty = FALSE,  # <4>\n    auto_unbox = TRUE)\n  \n  # upload the JSON file to S3\n  svc$put_object( # <5>\n    Body = temp_file,\n    Bucket = kDataBucket,\n    Key = paste0(\"example/\", basename(temp_file))\n  )\n  unlink(temp_file)\n}\n```\n:::\n\n\n1. Establish a connection to the AWS S3 service.\n2. The `write_json` does not export the `row.names` of the data.frame, so \nwe store them in a regular column first.\n3. Export each row of the `mtcars` data.frame into a separate JSON file.\n4. Athena does not accept pretty JSON format. Instead, each JSON-encoded record\nmust be represented on a separate line as outlined in the\n[Best practices for reading JSON data](https://docs.aws.amazon.com/athena/latest/ug/parsing-JSON.html)\ndocumentation page.\n5. Copy the JSON file to AWS S3.\n\n## Connecting to the AWS Athena service\n\nNext, we establish a connection to the AWS Athena service, pointing it to\nour staging location on S3. The `noctua` package provides methods to connect\nto (`dbConnect`) and query (e.g. `dbQuery`) Athena, extending generic methods\ndefined in the\n[DBI R package](https://dbi.r-dbi.org/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(noctua::athena(), s3_staging_dir = kStagingDir)\ndbGetQuery(con, \"show databases\")\n```\n:::\n\n\n```\n## INFO: (Data scanned: 0 Bytes)\n\n##    database_name\n##           <char>\n## 1:       default\n```\n\nIf this is your first interaction with Athena, only the `default` database\nwill be available.\n\n## Creating an external table with AWS Athena\n\nNext, we point Athena to our JSON files, by defining an _external table_ with \na schema that matches the column types of the original `mtcars` data.frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql(  # <1>\n  \"CREATE EXTERNAL TABLE IF NOT EXISTS mtcars (\n      mpg float,\n      cyl tinyint,\n      disp float,\n      hp smallint,\n      drat float,\n      wt float,\n      qsec float,\n      vs tinyint,\n      am tinyint,\n      gear tinyint,\n      carb tinyint,\n      name string\n )\n ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n LOCATION {kDataDir};\", .con = con)\ndbExecute(con, sql)\n```\n:::\n\n\n1. The [glue_sql() command](https://solutions.posit.co/connections/db/best-practices/run-queries-safely/#using-glue_sql)\nfacilitates inserting user-defined variables into a SQL query.\n\n```\n# <AthenaResult>\n##   SQL  CREATE EXTERNAL TABLE IF NOT EXISTS mtcars (\n##      mpg float,\n##      cyl tinyint,\n##      disp float,\n##      hp smallint,\n##      drat float,\n##      wt float,\n##      qsec float,\n##      vs tinyint,\n##      am tinyint,\n##      gear tinyint,\n##      carb tinyint,\n##      name string\n## )\n## ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n## LOCATION 's3://my-scratch-bucket/example/';\n```\n\nThe `dbListTables` command confirms that our `default` database now\ncontains the `mtcars` table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbListTables(con)\n```\n:::\n\n\n```\n## [1] \"mtcars\"\n```\n\n## Querying across all JSON files\n\nNow we are ready to issue queries across our collection of JSON files, using\nstandard SQL. For example, we can retrieve a subset of rows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql('SELECT * FROM \"mtcars\" LIMIT 5', .con = con)\ndbGetQuery(con, sql)\n```\n:::\n\n\n```\n## INFO: (Data scanned: 2.15 KB)\n\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    <num> <int> <num> <int> <num> <num> <num> <int> <int> <int> <int>\n## 1:  19.2     8 400.0   175  3.08 3.845 17.05     0     0     3     2\n## 2:  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1\n## 3:  22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1\n## 4:  19.2     6 167.6   123  3.92 3.440 18.30     1     0     4     4\n## 5:  22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2\n##                name\n##              <char>\n## 1: Pontiac Firebird\n## 2:          Valiant\n## 3:       Datsun 710\n## 4:         Merc 280\n## 5:         Merc 230\n```\n\nfilter for specific values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql('SELECT * FROM \"mtcars\" WHERE \"gear\" = 5;', .con = con)\ndbGetQuery(con, sql)\n```\n:::\n\n\n```\n## INFO: (Data scanned: 4.05 KB)\n\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    <num> <int> <num> <int> <num> <num> <num> <int> <int> <int> <int>\n## 1:  19.7     6 145.0   175  3.62 2.770  15.5     0     1     5     6\n## 2:  26.0     4 120.3    91  4.43 2.140  16.7     0     1     5     2\n## 3:  15.8     8 351.0   264  4.22 3.170  14.5     0     1     5     4\n## 4:  30.4     4  95.1   113  3.77 1.513  16.9     1     1     5     2\n## 5:  15.0     8 301.0   335  3.54 3.570  14.6     0     1     5     8\n##              name\n##            <char>\n## 1:   Ferrari Dino\n## 2:  Porsche 914-2\n## 3: Ford Pantera L\n## 4:   Lotus Europa\n## 5:  Maserati Bora\n```\n\nor match strings\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql('SELECT * FROM \"mtcars\" WHERE \"name\" like \\'Ferrari%\\';', \n                .con = con)\ndbGetQuery(con, sql)\n```\n:::\n\n\n```\n## INFO: (Data scanned: 4.05 KB)\n\n##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    <num> <int> <num> <int> <num> <num> <num> <int> <int> <int> <int>\n## 1:  19.7     6   145   175  3.62  2.77  15.5     0     1     5     6\n##            name\n##          <char>\n## 1: Ferrari Dino\n```\n\nWe can also read the full table into our R session, reconstituting the contents\nof the original `mtcars` data.frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- DBI::dbReadTable(con, \"mtcars\")\nhead(df)\n```\n:::\n\n\n```\n## INFO: (Data scanned: 4.05 KB)\n#      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##    <num> <int> <num> <int> <num> <num> <num> <int> <int> <int> <int>\n## 1:  17.3     8 275.8   180  3.07 3.730 17.60     0     0     3     3\n## 2:  24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2\n## 3:  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1\n## 4:  21.4     4 121.0   109  4.11 2.780 18.60     1     1     4     2\n## 5:  21.0     6 160.0   110  3.90 2.875 17.02     0     1     4     4\n## 6:  30.4     4  95.1   113  3.77 1.513 16.90     1     1     5     2\n##             name\n##           <char>\n## 1:    Merc 450SL\n## 2:     Merc 240D\n## 3:     Fiat X1-9\n## 4:    Volvo 142E\n## 5: Mazda RX4 Wag\n## 6:  Lotus Europa\n```\n\n## Cleaning up\n\nOnce our analysis is complete, we disconnect from the service\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\nand, if we don't want to query the same JSON files again in the future, we can\nalso remove the table from the database:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbRemoveTable(con, \"mtcars\", delete_data = FALSE)  # <1>\n```\n:::\n\n\n1. Set the `delete_data = FALSE` argument to remove the Athena database, but\nleave the JSON files in place.\n\n## Conclusions\n\n- The `noctua` R package made it easy to interface with AWS Athena, because it\n  allowed me to use the familiar `DBI` API implemented for many database back \n  ends.\n- Defining the schema for the example table was informed by examining the\n  `mtcars` data set.\n- Querying the collection of JSON files required Athena to read _all_ of them. \n  To reduce the amount of data that needs to be scanned, you might want to \n  [partition your data](https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing.html) -\n  e.g. split it by date, country, etc - both speeding up queries and reducing\n  cost.\n- The `mtcars` data set is a highly structured, and could easily be stored as\n  a single table on AWS S3, e.g. in a CSV or \n  [parquet file](https://www.databricks.com/glossary/what-is-parquet). \n  The latter is highly optimized for columnar data storage, and can be queried\n  in a highly efficient way - definitely something I will consider for large,\n  structured data in the future.\n\n## Reproducibility\n\n<details>\n<summary>\nSession Information\n</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(\"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Debian GNU/Linux 12 (bookworm)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2024-04-17\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version date (UTC) lib source\n P DBI          * 1.1.3   2022-06-18 [?] CRAN (R 4.3.1)\n P glue         * 1.6.2   2022-02-24 [?] CRAN (R 4.3.1)\n P jsonlite     * 1.8.7   2023-06-29 [?] CRAN (R 4.3.1)\n P noctua       * 2.6.2   2023-08-08 [?] RSPM\n P paws.storage * 0.5.0   2024-01-09 [?] RSPM\n\n [1] /home/sandmann/repositories/blog/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/sandmann/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\n</details>\n   ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}