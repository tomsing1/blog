{
  "hash": "7c4a4b49e2719436bf59aec9a1dff162",
  "result": {
    "markdown": "---\ntitle: \"Adventures with parquet: Storing & quering gene expression data\"\nauthor: \"Thomas Sandmann\"\ndate: \"2023-08-31\"\nfreeze: true\ncategories: [R, parquet, TIL]\neditor:\n  markdown:\n    wrap: 72\nformat:\n  html:\n    toc: true\n    toc-depth: 4\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\neditor_options: \n  chunk_output_type: console\n---\n\n\n## tl;dr\n\nToday I explored storing gene expression data in\n[parquet files](https://parquet.apache.org/),\nand querying them with the `arrow`, `duckdb` or `sparklyr` R packages.\n\n## Introduction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (lib in c(\"arrow\", \"dplyr\", \"duckdb\", \"edgeR\", \"fs\", \"glue\", \"memoise\",\n              \"rnaseqExamples\", \"sparklyr\", \"tictoc\", \"tidyr\", \"DESeq2\", \n              \"Homo.sapiens\", \"Mus.musculus\")) {\n  suppressPackageStartupMessages(\n    library(lib, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)\n  )\n}\n```\n:::\n\n\nA while back, I created the\n[rnaseq-examples](https://tomsing1.github.io/rnaseq-examples/index.html)\nR package with serialized `SummarizedExperiment` objects for three\npublished RNA-seq experiments [^1]. Here, I will \n\n1. Load the data from all three experiments into my R session,\n2. Extract the raw counts and TMM-normalized gene expression measurements\n   (counts per million) into (tidy) R data.frames,\n3. Store the data in separate parquet files, one for each experiment and\n4. Query the parquet files with the\n  - [arrow](https://arrow.apache.org/docs/r/),\n  - [duckdb](https://duckdb.org/docs/archive/0.8.1/api/r) and\n  - [sparklyr](https://spark.rstudio.com/)\n  R packages.\n\nI was amazed by the speed of `duckdb` on my local system, and am looking forward\nto distributing large datasets across nodes of a Spark cluster in the future. \nEither way, parquet files are a highly portable and language-agnostic file \nformat that I am happy to add to my toolkit.\n\n[^1]: If you are curious, please consult the \n[vignettes](https://tomsing1.github.io/rnaseq-examples/articles/index.html)\nto see examples of differential expression analyses for each dataset.\n\n## Loading gene expression datasets\n\nThe `rnaseqExamplesR` package is \n[available from github](https://github.com/tomsing1/rnaseq-examples) and \ncan be installed via \n\n\n::: {.cell}\n\n```{.r .cell-code}\nremotes::install_github(\"tomsing1/rnaseq-examples\")\n```\n:::\n\n\nOnce installed, we can load the three `SummarizedExperiment` objects it contains\ninto our R session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatasets <- data(package = \"rnaseqExamples\")\nknitr::kable(data.frame(datasets$results[, c(\"Item\", \"Title\")]))\n```\n\n::: {.cell-output-display}\n|Item  |Title                                                             |\n|:-----|:-----------------------------------------------------------------|\n|rnai  |Naguib et al: Global effects of SUPT4H1 RNAi on gene expression   |\n|sarm1 |Zhu et al: Effects of SARM1 deficiency on gene expression         |\n|tau   |Wang et al: time course of microglia from the rTg4510 mouse model |\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"tau\")\n```\n:::\n\n\nNext, I define the `tidy()` helper function to calculate `sizeFactors`\nfor normalization with \n[Robinson's and Oshlack's TMM method](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-3-r25)\nand export raw counts alongside (normalized) counts per million (CPM) in a\ndata.frame.\n\n::: {.callout-note collapse=\"true\"}\n\n- Note that I am not exporting sample annotations in this example. This\n  information could be added to the data.frame (with a join operation) or \n  stored in a different format, e.g. for efficient row-wise queries.\n- The helper function is wrapped in a to `memoise::memoise()`, which caches\n  the outputs in memory. That way, repeated calling of the same function avoids\n  additional computations / joins. This functionality is not really required\n  here, but I will try to remember it for future reference!\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"A helper function to coerce a SummarizedExperiment into a tibbbe\"}\n#' Coerce a DGEList or a SummarizedExperiment into a tibble\n#' \n#' @param x Either a `DGEList` or a `SummarizedExperiment`\n#' @return A tibble\n.tidy <- function(x) {\n  \n  # remove non-ensembl feature identifiers (e.g. spike-ins)\n  x <- x[grepl(\"^ENS\", row.names(x)), ]\n  annotation <- dplyr::case_when(\n    all(grepl(\"^ENSG\", row.names(x))) ~ \"Homo.sapiens\",\n    all(grepl(\"^ENSMUS\", row.names(x))) ~ \"Mus.musculus\"\n  )\n  stopifnot(!is.na(annotation))\n  require(annotation, character.only = TRUE)\n  \n  # extract raw counts\n  y <- edgeR::calcNormFactors(x)\n  counts <- y$counts %>%\n    as.data.frame() %>%\n    tibble::rownames_to_column(\"feature_id\") %>%\n    tidyr::pivot_longer(cols = colnames(y), \n                        names_to = \"sample_id\", values_to = \"count\")\n  \n  # extract cpms\n  cpms <- edgeR::cpm(y, normalized.lib.sizes = TRUE, log = FALSE) %>%\n    as.data.frame() %>%\n    tibble::rownames_to_column(\"feature_id\") %>%\n    tidyr::pivot_longer(cols = colnames(y), \n                        names_to = \"sample_id\", values_to = \"cpm\")\n  \n  # add gene annotations & alternative identifiers\n  dplyr::inner_join(\n    counts, cpms, by = c(\"feature_id\", \"sample_id\")\n  ) %>%\n    dplyr::left_join(\n      tibble::rownames_to_column(y$genes, \"feature_id\"),\n      by = \"feature_id\") %>%\n    dplyr::rename(ensembl = \"gene_id\") %>%\n    dplyr::mutate(\n      entrez = suppressMessages({\n        mapIds(get(annotation), keys = .data$ensembl, \n               column = \"ENTREZID\", keytype = \"ENSEMBL\",\n               multiVals = \"first\")\n      })\n    ) %>%\n    dplyr::select(-any_of(\"spikein\"))\n}\n\ntidy <- memoise(.tidy)\n```\n:::\n\n\n## Write parquet files\n\nNow we extract the gene expression measurements from each of our \n`SummarizedExperiment` objects and then write it to a parquet file in a \ntemporary directory on the local file system. Alternatively, I could store\nthe files on a cloud system, e.g. AWS S3, and access them remotely.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a temporary directory\nout_dir <- file.path(tempdir(), \"parquet\")\nfs::dir_create(out_dir)\n\nfor (dataset in c(\"tau\", \"rnai\", \"sarm1\")) {\n  df <- tidy(get(dataset))\n  df$study <- dataset  # add a columsn with the name of the experiment\n  arrow::write_parquet(\n    x = df, \n    sink = file.path(out_dir, paste0(dataset, \".parquet\"))\n  )\n}\n\n# list the contents of the temporary directory\nfs::dir_info(out_dir) %>%\n  dplyr::select(path, size) %>%\n  dplyr::mutate(path = basename(path))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  path                 size\n  <chr>         <fs::bytes>\n1 rnai.parquet        5.22M\n2 sarm1.parquet       5.29M\n3 tau.parquet         8.22M\n```\n:::\n:::\n\n\n## Querying\n\n### Arrow\n\nEven though we created three separate files, the\n[arrow R package](https://arrow.apache.org/docs/r/)\ncan abstract them into a single `Dataset`. We simply point the `open_dataset()`\nfunction at the directory containing the `.parquet` files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds <- arrow::open_dataset(out_dir)\nds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 3 Parquet files\nfeature_id: string\nsample_id: string\ncount: int32\ncpm: double\nensembl: string\nsymbol: string\ngene_type: string\nentrez: string\nstudy: string\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nWe can use `dbplr` verbs to query this `FileSystemDataset`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic(\"arrow\")\nds %>%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %>%\n  group_by(symbol, study) %>%\n  tally() %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n# Groups:   symbol [2]\n  symbol study     n\n  <chr>  <chr> <int>\n1 GAPDH  rnai     12\n2 Gapdh  sarm1    16\n3 Gapdh  tau      32\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narrow: 1.717 sec elapsed\n```\n:::\n:::\n\n\nOn my system, it takes ~ 1 second to retrieve the results, and about the\nsame amount of time is needed to retrieve the full `rnai` dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nds %>%\n  filter(study == \"rnai\") %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 694,236 × 9\n   feature_id      sample_id count     cpm ensembl symbol gene_type entrez study\n * <chr>           <chr>     <int>   <dbl> <chr>   <chr>  <chr>     <chr>  <chr>\n 1 ENSG00000163106 S320          0  0      ENSG00… HPGDS  protein_… 27306  rnai \n 2 ENSG00000163106 S168          2  0.0318 ENSG00… HPGDS  protein_… 27306  rnai \n 3 ENSG00000163106 S255          1  0.0111 ENSG00… HPGDS  protein_… 27306  rnai \n 4 ENSG00000163106 S190          0  0      ENSG00… HPGDS  protein_… 27306  rnai \n 5 ENSG00000163110 S912       1607 26.0    ENSG00… PDLIM5 protein_… 10611  rnai \n 6 ENSG00000163110 S832       2320 29.2    ENSG00… PDLIM5 protein_… 10611  rnai \n 7 ENSG00000163110 S783       1022 22.7    ENSG00… PDLIM5 protein_… 10611  rnai \n 8 ENSG00000163110 S623       1159 28.9    ENSG00… PDLIM5 protein_… 10611  rnai \n 9 ENSG00000163110 S622       1384 20.7    ENSG00… PDLIM5 protein_… 10611  rnai \n10 ENSG00000163110 S487       1023 32.6    ENSG00… PDLIM5 protein_… 10611  rnai \n# ℹ 694,226 more rows\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.09 sec elapsed\n```\n:::\n:::\n\n\n### Duckdb\n\nAlternatively, we can use \n[duckdb](https://duckdb.org/)\nto query our parquet files. The\n[duckdb R API](https://duckdb.org/docs/api/r)\nsupports both dbplyr verbs and raw SQL queries.\n\nFirst, we establish a connection to the `duckdb` backend:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(duckdb::duckdb())\n```\n:::\n\n\n#### Using dbplyr\n\nFirst, we execute the same dbplyr query we used above, which translates it\ninto SQL for us and passes it on to duckdb.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic(\"duckdb\")\ntbl(con, sprintf(\"read_parquet('%s/*.parquet')\", out_dir)) %>%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %>%\n  group_by(symbol, study) %>%\n  tally() %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  symbol study     n\n  <chr>  <chr> <dbl>\n1 GAPDH  rnai     12\n2 Gapdh  tau      32\n3 Gapdh  sarm1    16\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nduckdb: 0.065 sec elapsed\n```\n:::\n:::\n\n\nOn my system, `duckdb` returns results more than 10x faster than arrow's\nimplementation (see above).\n\nRetrieval of the full dataset for the `rnai` study is also completed in less\nthan half a second:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ntbl(con, glue(\"read_parquet('{out_dir}/*.parquet')\")) %>%\n  filter(study == \"rnai\") %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 694,236 × 9\n   feature_id      sample_id count   cpm ensembl   symbol gene_type entrez study\n   <chr>           <chr>     <int> <dbl> <chr>     <chr>  <chr>     <chr>  <chr>\n 1 ENSG00000000003 S912       1942  31.4 ENSG0000… TSPAN6 protein_… 7105   rnai \n 2 ENSG00000000003 S832       2942  37.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 3 ENSG00000000003 S783        905  20.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 4 ENSG00000000003 S623        960  24.0 ENSG0000… TSPAN6 protein_… 7105   rnai \n 5 ENSG00000000003 S622       2002  29.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 6 ENSG00000000003 S487        792  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 7 ENSG00000000003 S458       1549  20.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 8 ENSG00000000003 S420       1021  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 9 ENSG00000000003 S320       1432  21.5 ENSG0000… TSPAN6 protein_… 7105   rnai \n10 ENSG00000000003 S168       1773  28.2 ENSG0000… TSPAN6 protein_… 7105   rnai \n# ℹ 694,226 more rows\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.244 sec elapsed\n```\n:::\n:::\n\n\n#### Using SQL\n\nBecause `duckdb` is a SQL database, we can also query the parquet files directly\nwith raw SQL, realizing another gain in execution speed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ndbGetQuery(\n  con = con,\n  glue_sql(\n    \"SELECT symbol, study, COUNT(*) AS n \n     FROM read_parquet({paste0(out_dir, '/*.parquet')}) \n     WHERE UPPER(symbol) = 'GAPDH' \n     GROUP BY symbol, study\", \n    .con = con)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  symbol study  n\n1  GAPDH  rnai 12\n2  Gapdh sarm1 16\n3  Gapdh   tau 32\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.024 sec elapsed\n```\n:::\n:::\n\n\nSimilarly, reading all data for the `rnai` dataset into memory is faster than\nwith arrow's implementation as well:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ndbGetQuery(\n  con = con,\n  glue_sql(\n    \"SELECT * \n     FROM read_parquet({paste0(out_dir, '/*.parquet')}) \n     WHERE study = 'rnai'\", \n    .con = con)\n) %>%\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       feature_id sample_id count      cpm         ensembl symbol\n1 ENSG00000000003      S912  1942 31.43893 ENSG00000000003 TSPAN6\n2 ENSG00000000003      S832  2942 37.08823 ENSG00000000003 TSPAN6\n3 ENSG00000000003      S783   905 20.10531 ENSG00000000003 TSPAN6\n4 ENSG00000000003      S623   960 23.97706 ENSG00000000003 TSPAN6\n5 ENSG00000000003      S622  2002 29.94743 ENSG00000000003 TSPAN6\n6 ENSG00000000003      S487   792 25.26281 ENSG00000000003 TSPAN6\n       gene_type entrez study\n1 protein_coding   7105  rnai\n2 protein_coding   7105  rnai\n3 protein_coding   7105  rnai\n4 protein_coding   7105  rnai\n5 protein_coding   7105  rnai\n6 protein_coding   7105  rnai\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.211 sec elapsed\n```\n:::\n:::\n\n\n## Spark\n\nFinally, and mainly just for future reference, the\n[sparklyr R package](https://spark.rstudio.com/)\nprovides an R interface to leverage a Spark cluster and its\ndistributed analysis libraries.\n\nFirst, we establish a connection to the Spark cluster. Here am running a local\nSpark cluster (e.g. a single local node) on my laptop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsc <- spark_connect(master = \"local\")\n```\n:::\n\n\nNext, we import the datasets into the cluster by creating a new \n`Spark DataFrame` with the name `gene_expression`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsdf <- spark_read_parquet(sc = sc, name = \"gene_expression\",\n                          path = paste0(out_dir, '/*.parquet'))\n```\n:::\n\n\n### dbplyr\n\nThe `tbl_spark` object returned by `sparklyr::spark_read_parquet()`\ncan be queried with `dbplyr` verbs, e.g. to translate our now familiar queries\ninto `Spark SQL` statements on the fly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nsdf %>%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %>%\n  group_by(symbol, study) %>%\n  tally() %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  symbol study     n\n  <chr>  <chr> <int>\n1 Gapdh  sarm1    16\n2 GAPDH  rnai     12\n3 Gapdh  tau      32\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.472 sec elapsed\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nsdf %>%\n  filter(study == \"rnai\") %>%\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 694,236 × 9\n   feature_id      sample_id count   cpm ensembl   symbol gene_type entrez study\n   <chr>           <chr>     <int> <dbl> <chr>     <chr>  <chr>     <chr>  <chr>\n 1 ENSG00000000003 S912       1942  31.4 ENSG0000… TSPAN6 protein_… 7105   rnai \n 2 ENSG00000000003 S832       2942  37.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 3 ENSG00000000003 S783        905  20.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 4 ENSG00000000003 S623        960  24.0 ENSG0000… TSPAN6 protein_… 7105   rnai \n 5 ENSG00000000003 S622       2002  29.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 6 ENSG00000000003 S487        792  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 7 ENSG00000000003 S458       1549  20.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 8 ENSG00000000003 S420       1021  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 9 ENSG00000000003 S320       1432  21.5 ENSG0000… TSPAN6 protein_… 7105   rnai \n10 ENSG00000000003 S168       1773  28.2 ENSG0000… TSPAN6 protein_… 7105   rnai \n# ℹ 694,226 more rows\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.329 sec elapsed\n```\n:::\n:::\n\n\n### SQL\n\nAlternatively, we can use SQL queries to query the cluster's `gene_expression`\ntable directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ndbGetQuery(\n  con = sc,\n  glue(\n    \"SELECT symbol, study, COUNT(*) AS n \n     FROM gene_expression \n     WHERE UPPER(symbol) = 'GAPDH' \n     GROUP BY symbol, study\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  symbol study  n\n1  Gapdh sarm1 16\n2  GAPDH  rnai 12\n3  Gapdh   tau 32\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.493 sec elapsed\n```\n:::\n:::\n\n\nMy local Spark instance performs these queries more slowly than e.g. duckdb.\nBut Spark's _real power_ is in deploying \n[Machine learning models](https://spark.rstudio.com/guides/mlib.html)\nacross a (potentially large) cluster, enabling parallel processing of very\nlarge datasets by distributing both data and computation across nodes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspark_disconnect(sc)\n```\n:::\n\n\n## Reproducibility\n\n<details>\n<summary>\nSession Information\n</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(\"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-09-01\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package                            * version    date (UTC) lib source\n P AnnotationDbi                      * 1.62.2     2023-07-02 [?] Bioconductor\n P arrow                              * 13.0.0     2023-08-30 [?] CRAN (R 4.3.0)\n P Biobase                            * 2.60.0     2023-05-08 [?] Bioconductor\n P BiocGenerics                       * 0.46.0     2023-06-04 [?] Bioconductor\n P DBI                                * 1.1.3      2022-06-18 [?] CRAN (R 4.3.0)\n P DESeq2                             * 1.40.2     2023-07-02 [?] Bioconductor\n P dplyr                              * 1.1.2      2023-04-20 [?] CRAN (R 4.3.0)\n P duckdb                             * 0.8.1-3    2023-09-01 [?] CRAN (R 4.3.1)\n P edgeR                              * 3.42.4     2023-06-04 [?] Bioconductor\n P fs                                 * 1.6.3      2023-07-20 [?] CRAN (R 4.3.0)\n P GenomeInfoDb                       * 1.36.1     2023-07-02 [?] Bioconductor\n P GenomicFeatures                    * 1.52.2     2023-08-27 [?] Bioconductor\n P GenomicRanges                      * 1.52.0     2023-05-08 [?] Bioconductor\n P glue                               * 1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n P GO.db                              * 3.17.0     2023-08-25 [?] Bioconductor\n P Homo.sapiens                       * 1.3.1      2023-09-01 [?] Bioconductor\n P IRanges                            * 2.34.1     2023-07-02 [?] Bioconductor\n P limma                              * 3.56.2     2023-06-04 [?] Bioconductor\n P MatrixGenerics                     * 1.12.3     2023-07-30 [?] Bioconductor\n P matrixStats                        * 1.0.0      2023-06-02 [?] CRAN (R 4.3.0)\n P memoise                            * 2.0.1      2021-11-26 [?] CRAN (R 4.3.0)\n P Mus.musculus                       * 1.3.1      2023-09-01 [?] Bioconductor\n P org.Hs.eg.db                       * 3.17.0     2023-08-25 [?] Bioconductor\n P org.Mm.eg.db                       * 3.17.0     2023-08-23 [?] Bioconductor\n P OrganismDbi                        * 1.42.0     2023-05-08 [?] Bioconductor\n P rnaseqExamples                     * 0.0.0.9000 2023-09-01 [?] Github (tomsing1/rnaseq-examples@ac35304)\n P S4Vectors                          * 0.38.1     2023-05-08 [?] Bioconductor\n P sparklyr                           * 1.8.2      2023-07-01 [?] CRAN (R 4.3.0)\n P SummarizedExperiment               * 1.30.2     2023-06-11 [?] Bioconductor\n P tictoc                             * 1.2        2023-04-23 [?] CRAN (R 4.3.0)\n P tidyr                              * 1.3.0      2023-01-24 [?] CRAN (R 4.3.0)\n P TxDb.Hsapiens.UCSC.hg19.knownGene  * 3.2.2      2023-09-01 [?] Bioconductor\n P TxDb.Mmusculus.UCSC.mm10.knownGene * 3.10.0     2023-09-01 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\n</details>\n   ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}