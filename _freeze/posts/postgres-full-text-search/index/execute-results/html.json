{
  "hash": "0ea289c17a55eeb0f09a596bd760ebb3",
  "result": {
    "markdown": "---\ntitle: \"Full text search in Postgres - the R way\"\nauthor: \"Thomas Sandmann\"\ndate: \"2022-12-12\"\nfreeze: true\ncategories: [TIL, R, postgres]\neditor: \n  markdown: \n    wrap: 72\n---\n\n\nI have been learning how to organize, search and modify data in a \n[Postgres](https://www.postgresql.org/) \ndatabase by working through \n[Anthony DeBarros'](https://www.wsj.com/news/author/anthony-debarros) \nexcellent book \n[Practical SQL](https://nostarch.com/practical-sql-2nd-edition).\n\nBecause I currently perform most of my data analyses in R, I am using the \ngreat \n[RPostgres](https://cran.r-project.org/package=RPostgres),\n[DBI](https://cran.r-project.org/package=DBI)\nand\n[glue](https://cran.r-project.org/package=glue)\npackages to interface with Postgres - without ever leaving my R session. \n\nToday I learned how to create a full text search index and how to search it\nwith one or more search terms.\n\n## Connecting to Postgres\n\nFor this example, I created a toy database `full_text_search` in my local\nPostgres server. I connect to it with the `DBI::dbConnect` command, and by \npassing it the `RPostgres::Postgres()` driver.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DBI)\nlibrary(glue)\nlibrary(RPostgres)\nlibrary(sessioninfo)\n\n# Connect to a (prexisting) postgres database called `full_text_search`\ncon <- DBI::dbConnect(\n  dbname = \"full_text_search\",\n  drv = RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432L,\n  user = \"postgres\"\n  )\n```\n:::\n\n\n## Creating and populating a table\n\nBecause this is a toy example, I start with a fresh table `datasets`. (In case\nit already exists from previous experimentation, I drop the table if necessary).\n\nLet's define four fields for the table:\n\n- `id`: the unique identifier\n- `name`: the short name of each entry\n- `title`: a longer title\n- `description`: a paragraph describing the entry\n- `created`: a date and time the entry was added to the database \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# drop the `datasets` table if it already exists\nif (DBI::dbExistsTable(con, \"datasets\")) DBI::dbRemoveTable(con, \"datasets\")\n\n# create the empty `datasets` table\nsql <- glue_sql(\"\n      CREATE TABLE IF NOT EXISTS datasets (\n      id bigserial PRIMARY KEY,\n      name text,\n      title text,\n      description text,\n      created timestamp with time zone default current_timestamp not null\n    );\", .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbReadTable(con, \"datasets\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] id          name        title       description created    \n<0 rows> (or 0-length row.names)\n```\n:::\n:::\n\n\nInitially, our new database is empty. Let's populate them with three entries,\neach describing a popular dataset shipped with R's built-in\n[datasets](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html)\npackage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some example entries\nbuildin_datasets <- list(\n  mtcars = list(\n    \"name\" = \"mtcars\", \n    \"title\" = \"The built-in mtcars dataset from the datasets R package.\",\n    \"description\" = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The data was extracted from the 1974 Motor Trend US magazine, and \ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973â€“74 models).\")\n  ), \n  airmiles = list(\n    name = \"airmiles\",\n    title = \"The built-in airmiles dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The revenue passenger miles flown by commercial airlines in the United\nStates for each year from 1937 to 1960.\")\n  ),\n  attitude = list(\n    name = \"attitude\", \n    title = \"The built-in attitude dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"From a survey of the clerical employees of a large financial\norganization, the data are aggregated from the questionnaires of the\napproximately 35 employees for each of 30 (randomly selected) departments. \nThe numbers give the percent proportion of favourable responses to seven\nquestions in each department.\")\n  )\n)\n```\n:::\n\n\nNext, we loop over each element of the list and use the `glue_sql()` command\nto unpack both the names (`names(dataset)`) and the values of each field for \nthis entry. Then we update the `datasets` table with this new information.\n\nAfterward, we retrieve the `name` and `title` fields to verify the\ncorrect import:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (dataset in buildin_datasets) {\n  sql <- glue_sql(\n    \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({dataset*});\", \n    .con = con)\n  res <- suppressMessages(DBI::dbSendStatement(con, sql))\n  DBI::dbClearResult(res)\n}\nDBI::dbGetQuery(con, \"SELECT name, title from datasets;\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      name                                                     title\n1   mtcars  The built-in mtcars dataset from the datasets R package.\n2 airmiles The built-in airmiles dataset from the datasets R package\n3 attitude The built-in attitude dataset from the datasets R package\n```\n:::\n:::\n\n\n## Searching!\n\nOur goal is to enable full-text search for the `description` field. \nLet's look up the term `data`. To perform full-text search, both the records to\nsearch and our query need to be tokinzed first, with the `to_tsvector` and\n`to_tsquery` functions, respectively.\n\nHere is an example of the tokens that are generated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql(\n  \"SELECT to_tsvector('This is a my test phrase, and what \n                       a beautiful phrase it is.')\n   to_tsquery\", con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                          to_tsquery\n1 'beauti':10 'phrase':6,11 'test':5\n```\n:::\n:::\n\n\nThe following query correctly returns all records whose descriptions\ncontain the word `data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name\n1  1   mtcars\n2  3 attitude\n```\n:::\n:::\n\n\nWe can enrich the output by returning the output of the `ts_headline` function,\nhighlighting the location / context of the the matched term:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n```\n:::\n:::\n\n\nWe can also combine search terms, e.g. searching for either `employee` _or_ \n`motor` terms:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using multiple search terms\nterm <- \"employee | motor\"  # OR\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  1   mtcars                from the 1974 <Motor> Trend US magazine\n2  3 attitude clerical <employees> of a large financial organization\n```\n:::\n:::\n\n\nSimilarly, we can narrow our search by requiring both `data` _and_ `employee`\nterms to appear in the same description:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterm <- \"data & employee\"  # AND\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  3 attitude clerical <employees> of a large financial organization\n```\n:::\n:::\n\n\n## Creating indices\n\nIn the examples above, we performed tokenization of the search term and the \n`description` field at run time, e.g. when the query was executed. As our\ndatabase grows, this will soon become too cumbersome and degrade performance.\n\nAdding an index to our database will maintain full-text search speed even with\nlarge datasets. We have two different options:\n\n1. Create an [index based on an expression](https://www.postgresql.org/docs/8.3/indexes-expressional.html).\n2. Create a new field to hold the output of the `to_tsvector` function, and then\n[index this new field](https://www.postgresql.org/docs/8.3/textsearch-tables.html).\n\n### Creating an expression index\n\nA simple way to create a full-text index is to include the `to_tsvector()`\nexpression in the definition of the index itself. Here, we add a\n[Generalized Inverted Index (GIN)](https://www.postgresql.org/docs/current/gin-intro.html#:~:text=GIN%20stands%20for%20Generalized%20Inverted,appear%20within%20the%20composite%20items.)\nindex for the `description` column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql = glue_sql(\n  \"CREATE INDEX description_idx ON datasets \n  USING gin(to_tsvector('english', description));\",\n  con = con\n)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nThe same type of query we issued above will now take advantage of the \n`description_idx`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field using its index\nterm <- \"questioning\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                       ts_headline\n1  3 attitude responses to seven <questions> in each department\n```\n:::\n:::\n\n\nThe `description` fields of new records, e.g those that are added later, will\nautomatically be added to the index. Let's create a new record for the `euro` \ndataset, for example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data = list(\n  name = \"euro\", \n  title = \"The built-in euro dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"The data set euro contains the value of 1 Euro in all currencies\nparticipating in the European monetary union (Austrian Schilling ATS, \nBelgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, \nFrench Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, \nDutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were \nfixed by the European Union on December 31, 1998. To convert old prices to \nEuro prices, divide by the respective rate and round to 2 digits.\")\n)\nsql <- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({new_data*});\", \n  .con = con)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nThis new record will now be included in the search results for the term `data`,\nfor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field using its index\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n```\n:::\n:::\n\n\n### Adding a tokenized field for full-text searches\n\nAlternatively, another option is to create a new column to hold the output of \nthe `to_tsvector()` function, and then to index it for future use. \nLet's create a new column `search_description_text`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a column to hold tokens for full text search\nsql <- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nDBI::dbListFields(con, \"datasets\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n```\n:::\n:::\n\n\nNext, we tokenize the `descriptions` field, and store the output in our\nnew `search_description_text` column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\nHere are the tokens generated from the `description` of the first record, for\nexample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n```\n:::\n:::\n\n\nAs before, we can add an index - but this time, we index the pre-tokenized\n`search_description_text` column instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the search index\nsql <- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nTime to run our search again. When we search the `search_description_text` \nfield, we can omit the `to_tsvector()` call, because its has been tokenized\nalready:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n```\n:::\n:::\n\n\nðŸš¨ But _beware_: because we have precalculated the \ntokens, any new records added to the database will _not_ automatically be \nprocessed, nor will they be indexed!\n\nLet's add a final record, the `morely` dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmore_data = list(\n  name = \"morley\", \n  title = \"The built-in morley dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"A classical data of Michelson (but not this one with Morley) on \nmeasurements done in 1879 on the speed of light. The data consists of five \nexperiments, each consisting of 20 consecutive â€˜runsâ€™. The response is the speed\nof light measurement, suitably coded (km/sec, with 299000 subtracted).\")\n)\n```\n:::\n\n\nTo enter _this_ record, we not only have to populate the `name`, `title` and\n`description` fields - but also the list of tokens derived from the \n`description` in the `search_description_text` column. In other words, we have\nto execute the `to_tsvector` function inside our `INSERT` statement:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsql <- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*}, search_description_text)\n   VALUES ({more_data*}, to_tsvector({more_data[['description']]}));\", \n  .con = con)\nDBI::dbExecute(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nNow, our query returns both the original matches and the new record:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n4  5   morley            classical <data> of Michelson (but not this\n```\n:::\n:::\n\n\n### Choosing between indexing strategies\n\nAccording to the\n[Postgres documentation](https://www.postgresql.org/docs/8.3/textsearch-tables.html):\n\n> One advantage of the separate-column approach over an expression index is that\nit is not necessary to explicitly specify the text search configuration in \nqueries in order to make use of the index. Another advantage is that searches \nwill be faster, since it will not be necessary to redo the to_tsvector calls to\nverify index matches. The expression-index approach is simpler to set up, \nhowever, and it requires less disk space since the tsvector representation is \nnot stored explicitly.\n\nThat's it. Thanks again to \n[Anthony DeBarros'](https://www.wsj.com/news/author/anthony-debarros) \nfor his excellent introduction to \n[Practical SQL](https://nostarch.com/practical-sql-2nd-edition)!\n\n## Reproducibility\n\n<details>\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package     * version date (UTC) lib source\n askpass       1.1     2019-01-13 [1] CRAN (R 4.2.0)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.2.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.2.0)\n blob          1.2.3   2022-04-10 [1] CRAN (R 4.2.0)\n cli           3.5.0   2022-12-20 [1] CRAN (R 4.2.0)\n credentials   1.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.0)\n evaluate      0.19    2022-12-13 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n hms           1.1.2   2022-08-19 [1] CRAN (R 4.2.0)\n htmltools     0.5.4   2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.2)\n jsonlite      1.8.4   2022-12-06 [1] CRAN (R 4.2.0)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.2.0)\n lubridate     1.9.0   2022-11-06 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n openssl       2.0.5   2022-12-06 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n Rcpp          1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.19    2022-12-15 [1] CRAN (R 4.2.0)\n RPostgres   * 1.4.4   2022-05-02 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n sys           3.4.1   2022-10-18 [1] CRAN (R 4.2.0)\n timechange    0.1.1   2022-11-04 [1] CRAN (R 4.2.0)\n vctrs         0.5.1   2022-11-16 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n```\n:::\n:::\n\n</details>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}