{
  "hash": "36033602c171186266239e733deae006",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring recipes with LLMs, Ollama and R\"\nauthor: \"Thomas Sandmann\"\ndate: \"2024-08-21\"\nfreeze: true\ncategories: [R, Ollama, LLM]\neditor:\n  markdown:\n    wrap: 72\nformat:\n  html:\n    toc: true\n    toc-depth: 4\n    code-tools:\n      source: true\n      toggle: false\n      caption: none\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n## tl;dr\n\nToday I learned about\n\n- Running LLMs locally via Ollama\n- Creating embeddings for a corpus of recipes\n- Exploring the embedding space using PCA, clustering and UMAP\n\n## Acknowledgements\n\nThis post is heavily inspired by @hrbrmstr's \n[DuckDB VSS & CISA KEV](https://dailydrop.hrbrmstr.dev/2024/08/09/drop-514-2024-08-09-duckdb-vector-search/)\npost, and benefited greatly from Joseph Martinez' tutorial on \n[Semantic Search using Datasette](https://github.com/josephrmartinez/recipe-dataset). \nAs always, all errors are mine.\n\n## Introduction\n\nLarge language models (LLMs) are everywhere right now, from chat bots to search\nengines. Today, inspired by\n[Bob Rudis'](https://rud.is/) recent post on exploring a large set of \"Known\nExploited Vulnerabilities by \n[creating and searching text embeddings](https://dailydrop.hrbrmstr.dev/2024/08/09/drop-514-2024-08-09-duckdb-vector-search/),\nI am using local LLM to explore a large set of \n[food recipes](https://github.com/josephrmartinez/recipe-dataset?tab=readme-ov-file)\nby\n\n- Embedding each recipe (title, ingredients & instructions) into a high \n  dimensional space using the `nomic-embed-text` LLM running locally via\n  [Ollama](https://ollama.com/).\n- Exploring the embedding space using principal components (PCs) and \n  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)\n- Cluster recipes and summarize each cluster with the (smallest version of)\n  Meta's `llama3.1` model.\n\n## Installing Ollama\n\nFor this post, I am using LLMs that run locally on my M2 Macbook Pro with 16Gb\nRAM. (Some larger LLMs require more memory, so I will stick to the smaller\nmodels here.)\n\nFirst, I downloaded and installed the [Ollama application](https://ollama.com/),\nwhich makes it easy to retrieve and run different models. Once Ollama is\nrunning (indicated by the llama 🦙 menu item in my Mac's main menu bar), it\nserves \n[REST endpoints](https://github.com/ollama/ollama/blob/main/docs/api.md),\nincluding calls to\n\n- generate text based on a prompt: `POST /api/generate`\n- return the numerical embedding for an input: `POST /api/embed`\n\nall from the comfort of my own laptop.\n\n## Downloading models\n\nNext, let's download a few models, including some that only provide embeddings\n(e.g. they output numerical vectors representing the input) and the latest\n[llama 3.1 model](https://ollama.com/library/llama3.1) released by Meta[^1]\nvia ollama's command line interface:\n\n```\nollama pull nomic-embed-text  # embeddings only\nollama pull nomic-embed-text  # embeddings only\nollama pull llama3.1:latest   # 8 billion parameters\n```\n\n[^1]: The Llama license allows for redistribution, fine-tuning, and creation of\nderivative work, but requires derived models to include \"Llama\" at the\nbeginning of their name, and any derivative works or services must mention\n\"Built with Llama\". For more information, see the \n[original license](https://llama.meta.com/llama3_1/license/).\n\n## Interacting with Ollama from R\n\nFollowing \n[Bob's example](https://companion.hrbrmstr.dev/posts/2024-08-09-duckdb-vss/)\nwe can submit queries to our `Ollama` server by issuing POST requests via the\n[httr2 package](https://cran.r-project.org/package=httr2). Because we will do\nthis many times, the following helper R functions will be useful - one to \nretrieve embeddings, the other to generate text. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(httr2)\n\nollama_embed <- function(input = \"This text will be embedded.\",\n                         model = \"nomic-embed-text:latest\") {\n  resp <- httr2::request(\"http://localhost:11434\") |> \n    httr2::req_url_path(\"/api/embed\") |>\n    httr2::req_headers(\"Content-Type\" = \"application/json\") |>\n    httr2::req_body_json(\n      list(\n        model = model,\n        input = input,\n        truncate = TRUE,\n        stream = FALSE,\n        keep_alive = \"10s\",\n        options = list(seed = 123)\n      )\n    ) |> \n   httr2::req_perform()\n\n  m <- resp |> httr2::resp_body_json(simplifyVector = TRUE) |>\n    getElement(\"embeddings\")\n  m[1, ]\n}\n\nollama_generate <- function(prompt = \"Who is Super Mario's best friend?\",\n                            model = \"llama3.1:latest\") {\n  resp <- httr2::request(\"http://localhost:11434\") |> \n    httr2::req_url_path(\"/api/generate\") |>\n    httr2::req_headers(\"Content-Type\" = \"application/json\") |>\n    httr2::req_body_json(\n      list(\n        model = model,\n        prompt = prompt,\n        stream = FALSE,\n        keep_alive = \"10s\",  # keep the model in memory for 10s after the call\n        options = list(seed = 123)  # reproducible seed\n      )\n    ) |> \n   httr2::req_perform()\n\n  resp |> httr2::resp_body_json() |>\n    getElement(\"response\")\n}\n```\n:::\n\n\n\nOllama offers many different models to choose from, differing in architecture,\nnumber of parameters, and of course the training data they were built from.\nTypically, larger models take longer to run and require more memory. For \nexample, the following benchmark profiles the turnaround time for our three\nmodels, averaged over 20 requests:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(ggplot2)\n  library(microbenchmark)\n})\nset.seed(123)\n# the beginning of Shakespeare's Sonnet 18\ntest_input <- paste(\"Shall I compare thee to a summer's day?\", \n                    \"Thou art more lovely and more temperate:\",\n                    \"Rough winds do shake the darling buds of May,\",\n                    \"And summer's lease hath all too short a date:\")\nmicrobenchmark(\n    snowflake = ollama_embed(model = \"snowflake-arctic-embed:latest\",\n                             input = test_input),\n    nomic = ollama_embed(model = \"nomic-embed-text:latest\",\n                             input = test_input),\n    llama = ollama_embed(model = \"llama3.1:latest\",\n                             input = test_input),\n    times = 20, unit = \"ms\"\n) |>\n  ggplot2::autoplot() +\n  theme_linedraw(14)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nThe `nomic-embed-text` (v1.5) model with 22 million parameters \nis (usually) faster than `snowflake-arctic-embed:latest` model with 335 million \nparameters, and both are faster than the `llama3.1` model with 8 billion.\n\nBecause it is fast and supports long inputs, and I will stick with the \n`nomic-embed-text:latest` model here. Speed of course doesn't reflect\nthe _quality_ of the embeddings. If you are curious how the choice of model\ninfluences the results, just swap out the  `model` argument in the calls to the\n`ollama_embed` helper function below.\n\n::: {.callout-note collapse=\"true\"}\n\n## {ollamar} R package\n\nThe [ollamar R package](https://cran.r-project.org/package=ollamar)\noffers convenience functions to interact with the `ollama` application.\n\nFor example, we can use them to prompt a model of our choice\nand extract its response from the returned object [^2].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ollamar)\nollamar::test_connection()\nresp <- ollamar::generate(\"llama3.1\", \"tell me a 5-word story\")\nollamar::resp_process(resp, \"df\")$response\n```\n:::\n\n\n\n[^2]: {ollamar} version 1.1.1 available from CRAN does not support returning the \nresponse as plain text from a request, yet, but that feature seems to be\nincluded in the latest version \n[on github](https://github.com/hauselin/ollama-r). Here we extract the\n`response` column ourselves.\n\nThe `embeddings` function directs requests to the `embed` endpoint instead [^3].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemb <- ollamar::embeddings(\"llama3.1\", \"Hello, how are you?\")\nlength(emb)\n```\n:::\n\n\n\n[^3]: Currently, the functions from the {ollamar} package print all `httr2`\nresponses (via cat). If that get's annoying, you can silence them e.g. with\n`generate_quietly <- purrr::quietly(ollamar::generate)`, etc. \n\n:::\n\n## The recipe corpus\n\nKaggle hosts the \n[Food Ingredients and Recipes Dataset with Images](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images)\ndataset, which was originally scraped from\n[the Epicurious Website](https://www.epicurious.com/). \nThe original dataset includes images for each recipe as well, but \nJoseph Martinez has generously shared a CSV file with just the text information \n[in this github repository](https://github.com/josephrmartinez/recipe-dataset).\n\nLet's read the full dataset into our R session:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nrecipes <- readr::read_csv(\n  paste0(\"https://raw.githubusercontent.com/josephrmartinez/recipe-dataset/\",\n         \"main/13k-recipes.csv\"),\n  col_types = \"_c_ccc\")\nrecipes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13,501 × 4\n   Title                             Instructions Image_Name Cleaned_Ingredients\n   <chr>                             <chr>        <chr>      <chr>              \n 1 Miso-Butter Roast Chicken With A… \"Pat chicke… miso-butt… \"['1 (3½–4-lb.) wh…\n 2 Crispy Salt and Pepper Potatoes   \"Preheat ov… crispy-sa… \"['2 large egg whi…\n 3 Thanksgiving Mac and Cheese       \"Place a ra… thanksgiv… \"['1 cup evaporate…\n 4 Italian Sausage and Bread Stuffi… \"Preheat ov… italian-s… \"['1 (¾- to 1-poun…\n 5 Newton's Law                      \"Stir toget… newtons-l… \"['1 teaspoon dark…\n 6 Warm Comfort                      \"Place 2 ch… warm-comf… \"['2 chamomile tea…\n 7 Apples and Oranges                \"Add 3 oz. … apples-an… \"['3 oz. Grand Mar…\n 8 Turmeric Hot Toddy                \"For the tu… turmeric-… \"['¼ cup granulate…\n 9 Instant Pot Lamb Haleem           \"Combine da… instant-p… \"['¾ cup assorted …\n10 Spiced Lentil and Caramelized On… \"Place an o… spiced-le… \"['1 (14.5-ounce) …\n# ℹ 13,491 more rows\n```\n\n\n:::\n:::\n\n\n\nThis corpus is very large. For this example, I sample 5000 random recipes to\nspeed up the calculation of the embeddings (see below).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nkeep <- sample(seq.int(nrow(recipes)), size = 5000, replace = FALSE)\nrecipes <- recipes[keep, ]\n```\n:::\n\n\n\nThe list of ingredients is included as a _python_ list, including square\nbrackets and quotes. Let's use the `reticulate` R package to coerce it into a\ncharacter vector and then collapse it into a single comma-separated string:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(purrr)\n  library(reticulate)\n  library(stringr)\n})\nrecipes$Cleaned_Ingredients <- reticulate::py_eval(\n  # combine ingredients from all recipes into a single string to avoid\n  # looping over each one separately\n  paste(\"[\", recipes$Cleaned_Ingredients, \"]\", collapse = \", \")) |>\n  unlist(recursive = FALSE) |>\n  purrr::map_chr(~ stringr::str_flatten_comma(.)) |>\n  # double quotes, denoting inches, were escaped in the original list\n  stringr::str_replace_all(pattern = stringr::fixed('\\\"'), \n                           replacement = ' inch')\n```\n:::\n\n\n\nSome `Titles` contain escaped quotes, let's remove them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipes$Title <- stringr::str_replace_all(\n  string = recipes$Title, \n  pattern = stringr::fixed('\\\"'), replacement = \"\")\n```\n:::\n\n\n\nI also replace all newlines (or tabs) in the `Instructions` with spaces:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipes$Instructions <- stringr::str_replace_all(\n  string = recipes$Instructions, \n  pattern = stringr::regex(\"[[:space:]]\"), replacement = \" \")\n```\n:::\n\n\n\n## Generating embeddings\n\nNow I am ready to pass each recipe to the \n[nomic-embed-text v1.5](https://ollama.com/library/nomic-embed-text) \nmodel via Ollama's `embed` endpoint, which returns a numerical vector for our\nquery.\n\nWe pass each recipe to the LLM one by one, combining the Title, \nIngredients and Instructions of each recipe into a single string. (Now is an\nexcellent time to grab a cup of coffee ☕️ - on my M2 Macbook Pro it takes about\na minute to calculate 1000 embeddings.) \n\nTo keep things organized, I add the embeddings to the original data.frame, as\nthe `Embedding` list column.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\ntic(\"Calculating embeddings\")\nrecipes$Embedding <- lapply(\n   glue::glue_data(\n    recipes,\n    paste(\n      \"{Title}\", \n      \"Ingredients: {Cleaned_Ingredients}\",\n      \"Instructions: {Instructions}\"\n    )\n   ), \\(x) {\n     ollama_embed(\n       input = x,\n       model = \"nomic-embed-text:latest\"\n     )\n   })\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCalculating embeddings: 367.923 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nSometimes I encounter recipes where the embeddings are all zero, so I remove\nthose from the data.frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (any(sapply(recipes$Embedding, var) > 0)) {\n  recipes <- recipes[sapply(recipes$Embedding, var) > 0,]\n}\n```\n:::\n\n\n\n## Exploring the recipes in the embedding space\n\nWe now have a representation of each recipe in the high-dimensional embedding\nspace. How high dimensional? Let's check:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(recipes$Embedding[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 768\n```\n\n\n:::\n:::\n\n\n\nTo explore the relationship between the recipes in this space, we combine the\nembeddings into a matrix with one column per recipe, and one row for each\nelement of the embedding vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- do.call(cbind, recipes$Embedding)\ncolnames(m) <- recipes$Title\ndim(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  768 4999\n```\n\n\n:::\n:::\n\n\n\n## Exploring the embedding matrix\n\n### Cosine similarity\n\nTo compare pairs of recipes to each other, we can calculate a distance metric,\ne.g. Euclidean distance or Cosine similarity between their embedding vectors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coop)  # Fast Covariance, Correlation, and Cosine Similarity Operations\nsimilarity_cosine <- cosine(m)\n```\n:::\n\n\n\nThe cosine similarity for most pairs of recipes clusters around 0.6, but there\nare some that are much more or much less similar to each other\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(as.vector(similarity_cosine), main = \"Cosine similarity (all pairs)\",\n     xlab = \"Cosine similarity\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=480}\n:::\n:::\n\n\n\nFor example, let's retrieve the titles of the recipes that are most similar\nto `Marbleized Eggs` or `Spinach Salad with Dates`. Reassuringly the best \nmatches sound very similar:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(similarity_cosine[\"Marbleized Eggs\", ], decreasing = TRUE) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Marbleized Eggs          To Dye Easter Eggs \n                  1.0000000                   0.8432208 \n  Chocolate-Filled Delights      Uncle Angelo's Egg Nog \n                  0.8060919                   0.7770789 \nOlive Oil–Basted Fried Eggs            Foster's Omelets \n                  0.7673314                   0.7645979 \n```\n\n\n:::\n\n```{.r .cell-code}\nsort(similarity_cosine[\"Spinach Salad with Dates\", ], decreasing = TRUE) |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            Spinach Salad with Dates \n                                           1.0000000 \nSpinach Salad with Pecorino, Pine Nuts, and Currants \n                                           0.7962855 \n                                  Simple Spinach Dip \n                                           0.7797199 \n Carrot Ribbon Salad With Ginger, Parsley, and Dates \n                                           0.7742124 \n                                    Garlicky Spinach \n                                           0.7711255 \n         Kale Salad with Dates, Parmesan and Almonds \n                                           0.7664300 \n```\n\n\n:::\n:::\n\n\n\n### Principal component analysis \n\nNow that we have a high-dimensional numerical representation of our recipes, we\ncan use tried-and-true methods that are frequently used to explore datasets from\ndifferent domains, e.g. \n[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca <- prcomp(m, center = TRUE, scale. = TRUE)\npairs(pca$rotation[, 1:3], cex = 0.5, pch = 19, col = adjustcolor(\"black\", 0.2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=576}\n:::\n:::\n\n\n\nThe first few principal components separate the recipes into large clumps, \nand the recipes with the highest loadings on PC2 and PC3 seem to have\nidentifiable themes in common. (I wasn't able to guess at what PC1 picked up.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PC2 separates deserts from mains\npc2 <- sort(pca$rotation[, 2])\nrecipes[recipes$Title %in% names(head(pc2)), ]$Title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Southeast Asian Beef and Rice-Noodle Soup\"            \n[2] \"Spaghetti Sauce Chicken Afritada\"                     \n[3] \"Grilled Chicken with Roasted Tomato and Oregano Salsa\"\n[4] \"Spicy Asian Noodle and Chicken Salad\"                 \n[5] \"Grilled Pork Shoulder Steaks With Herb Salad\"         \n[6] \"Thai Beef with Basil\"                                 \n```\n\n\n:::\n\n```{.r .cell-code}\nrecipes[recipes$Title %in% names(tail(pc2)), ]$Title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mini Black-and-White Cookies\"        \n[2] \"Pastel Butter Cookies\"               \n[3] \"Cream Puffs with Lemon-Cream Filling\"\n[4] \"Iced Stars\"                          \n[5] \"Black-and-White-and-Green Cookies\"   \n[6] \"Blackberry Walnut Cookies\"           \n```\n\n\n:::\n\n```{.r .cell-code}\n# PC3 separates poultry from cocktails\npc3 <- sort(pca$rotation[, 3])\nrecipes[recipes$Title %in% names(head(pc3)), ]$Title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slow-Smoked Barbecue Chicken\"                             \n[2] \"Chicken Under a Brick\"                                    \n[3] \"Grilled Indian-Spiced Butter Chicken\"                     \n[4] \"Spiced Matzo-Stuffed Chicken Breasts\"                     \n[5] \"Cambodian Grilled Chicken (Mann Oeng K'tem Sor, Marech)\"  \n[6] \"Cornbread-Stuffed Cornish Game Hens with Corn Maque Choux\"\n```\n\n\n:::\n\n```{.r .cell-code}\nrecipes[recipes$Title %in% names(tail(pc3)), ]$Title\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Ramos Gin Fizz\"                      \n[2] \"Orange, Jícama, and Watercress Salad\"\n[3] \"Berry Dangerous Fix Cocktail\"        \n[4] \"Aqua Pearl\"                          \n[5] \"Peach and Fizzy Grapefruit Float\"    \n[6] \"Ramos Gin Fizz\"                      \n[7] \"José's Gin & Tonic\"                  \n```\n\n\n:::\n:::\n\n\n\nThe principal components are ranked according to how much variance they explain\nin our data. Let's focus on the first 50 components and identify clusters of\nrecipes in this space using the Partitioning around medoids (PAM) algorithm,\na more robust version of k-means clustering [^5]. Here, I am asking for 50\nclusters, an arbitrary number that should give us sufficiently high resolution\nto explore (see below).\n\n[^5]: The `cluster::pam()` function offers a number of optional shortcuts that\nreduce the run time, specified via the `pamonce` argument. Check the functions\nhelp page if want to learn more!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cluster)\nset.seed(123)\nrecipes$cluster <- cl <- factor(\n  cluster::pam(pca$rotation[, 1:50], k = 50, cluster.only = TRUE, pamonce = 6)\n)\n```\n:::\n\n\n\nNow I can color the PCA plots according to the assignment of each recipe to one\nof the 50 clusters. Unsurprisingly, there is a lot of overlap when only the\nfirst three PCs are plotted:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncl_cols <- setNames(rainbow(50), 1:50)\npairs(pca$rotation[, 1:3], col = adjustcolor(cl_cols[as.character(cl)], 0.3),\n      cex = 0.5, pch = 19)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=576}\n:::\n:::\n\n\n\nAnother way to visualize high dimensional data is to allow non-linear \ntransformations, e.g. via t-distributed stochastic neighbor embedding (tSNE)\nor Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP).\n\n🚨 It is important to remember that distances after dimensionality reductions\nare hard to interpret, and that the choice of parameters can drastically change\nthe final visualization [^6].\n\n[^6]: You can also read more about tSNE and UMAP, and the pitfalls of their\ninterpretation \n[here](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288)\nand\n[here](https://www.nature.com/articles/s41592-024-02301-x). \n\nHere, I am creating a UMAP embedding based on the first 50 principal components.\nMost of the parameters are left at their default values, except for the number\nof neighbors, which I increased to create a (to me) visually more pleasing\nplot. Each point represents a recipe and is colored by the PAM clusters defined\nabove.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(umap)\n})\ncustom.config <- umap.defaults\ncustom.config$random_state <- 123L\ncustom.config$n_neighbors <- 25  # default: 15\ncustom.config$min_dist <- 0.1 # default 0.1\ncustom.config$n_components <- 2 # default 2\ncustom.config$metric <- \"euclidean\"  # default \"euclidean\"\num <- umap::umap(pca$rotation[, 1:50], config=custom.config)\n\nrecipes$Dim1 <- um$layout[, 1]\nrecipes$Dim2 <- um$layout[, 2]\n\nrecipes_medians <- recipes |>\n  dplyr::group_by(cluster) |>\n  dplyr::summarise(Dim1 = median(Dim1), \n                   Dim2 = median(Dim2))\nrecipes |>\n  ggplot() +\n  aes(x = Dim1, y = Dim2) +\n  geom_point(aes(color = cluster), show.legend = FALSE, \n             alpha = 0.7) + \n  geom_text(aes(label = cluster), data = recipes_medians) +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=624}\n:::\n:::\n\n\n\nThe UMAP plot shows one large component and a number of smaller clusters, e.g.\nPAM cluster 35 (at the top of the plot), cluster 3 (on the left) or cluster 50\n(on the bottom).\n\n## Summarizing cluster membership\n\nWith 50 different clusters, there is a lot to explore in this dataset. Sampling\na few examples from each cluster provides starting hypotheses about what the\nrecipes they contain might have in common.\n\nFor example, it seems that cluster 35 contains lamb dishes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipes |>\n  dplyr::filter(cluster == 35) |>\n  dplyr::sample_n(size = 10) |>\n  dplyr::pull(Title)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Spiced Lamb Hand Pies\"                                          \n [2] \"Lamb Tagine With Chickpeas and Apricots\"                        \n [3] \"Easy Provençal Lamb\"                                            \n [4] \"Grilled Saffron Rack of Lamb\"                                   \n [5] \"Lamb Chops with Red Onion, Grape Tomatoes, and Feta\"            \n [6] \"Braised Lamb Shanks with Swiss Chard\"                           \n [7] \"Grilled Leg of Lamb with Ancho Chile Marinade\"                  \n [8] \"Roasted Lamb Shoulder (Agnello de Latte Arrosto)\"               \n [9] \"Lamb Kebabs with Mint Pesto\"                                    \n[10] \"Braised Lamb Shanks with Spring Vegetables and Spring Gremolata\"\n```\n\n\n:::\n:::\n\n\n\nAnd cluster 4 captured pork recipes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipes |>\n  dplyr::filter(cluster == 4) |>\n  dplyr::sample_n(size = 10) |>\n  dplyr::pull(Title)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Hoisin-Marinated Pork Chops\"                                   \n [2] \"Grilled Rib Pork Chops with Sweet and Tangy Peach Relish\"      \n [3] \"Pork Shoulder Al'Diavolo\"                                      \n [4] \"Shanghai Soup Dumplings\"                                       \n [5] \"Candy Pork\"                                                    \n [6] \"Perfect Pork Chops\"                                            \n [7] \"Pork Chops with Mustard Sauce\"                                 \n [8] \"Stuffed Poblano Chiles with Walnut Sauce and Pomegranate Seeds\"\n [9] \"Rolled Pork Loin Roast Stuffed With Olives and Herbs\"          \n[10] \"My Boudin\"                                                     \n```\n\n\n:::\n:::\n\n\n\nLet's get `llama3.1`'s help to identify the theme that recipes in each cluster\nhave in common, based on their title alone:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_themes <- recipes |>\n  group_by(cluster) |>\n  summarize(\n    theme = ollama_generate(\n    prompt = glue::glue(\n      \"Identify the common theme among the following recipes, \", \n      \"return fewer than 5 words: \", \n      \"{ paste(Title, collapse = ';') }\")\n    )\n  )\n```\n:::\n\n\n\nThe LLM agrees with our manual exploration of clusters 3 and 35 above.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_themes |>\n  dplyr::filter(cluster %in% c(4, 35))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  cluster theme                   \n  <fct>   <chr>                   \n1 4       Pork Recipes Galore     \n2 35      Meat, particularly lamb.\n```\n\n\n:::\n:::\n\n\n\nIt also provides concise labels for the remaining ones:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_themes |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  cluster theme                   \n  <fct>   <chr>                   \n1 1       Global Vegetable Salads.\n2 2       Beans.                  \n3 3       Main Course             \n4 4       Pork Recipes Galore     \n5 5       Salads.                 \n6 6       Chicken dishes.         \n```\n\n\n:::\n:::\n\n\n\nIn a \n[recent blog post](https://blog.stephenturner.us/p/use-r-to-prompt-a-local-llm-with)\nStephen Turner uses the `llama3.1` model via Ollama to annotated gene sets in \na similar way, check it out!\n\n## Reproducibility\n\n<details>\n<summary>\nSession Information\n</summary>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24)\n os       macOS Sonoma 14.6.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2024-08-22\n pandoc   3.1.11 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package        * version date (UTC) lib source\n P cluster        * 2.1.6   2023-12-01 [?] RSPM\n P coop           * 0.6-3   2021-09-19 [?] RSPM\n P dplyr          * 1.1.4   2023-11-17 [?] CRAN (R 4.4.0)\n P ggplot2        * 3.5.1   2024-04-23 [?] CRAN (R 4.4.0)\n P httr2          * 1.0.2   2024-07-16 [?] CRAN (R 4.4.0)\n P microbenchmark * 1.4.10  2023-04-28 [?] RSPM\n P purrr          * 1.0.2   2023-08-10 [?] CRAN (R 4.4.0)\n P readr          * 2.1.5   2024-01-10 [?] CRAN (R 4.4.0)\n P reticulate     * 1.38.0  2024-06-19 [?] CRAN (R 4.4.0)\n P stringr        * 1.5.1   2023-11-14 [?] CRAN (R 4.4.0)\n\n [1] /Users/sandmann/repositories/blog/renv/library/macos/R-4.4/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/macos/R-4.4/aarch64-apple-darwin20/f7156815\n\n P ── Loaded and on-disk path mismatch.\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         /Users/sandmann/miniconda3/bin/python\n libpython:      /Users/sandmann/miniconda3/lib/libpython3.11.dylib\n pythonhome:     /Users/sandmann/miniconda3:/Users/sandmann/miniconda3\n version:        3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ]\n numpy:          /Users/sandmann/miniconda3/lib/python3.11/site-packages/numpy\n numpy_version:  1.26.2\n \n NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n</details>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}