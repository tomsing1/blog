[
  {
    "objectID": "posts/interactive-gene-set-results/index.html",
    "href": "posts/interactive-gene-set-results/index.html",
    "title": "Interactive GSEA results: visualizations with reactable & plotly",
    "section": "",
    "text": "As a Computational Biologist, I frequently analyze data from high throughput experiments, including transcriptomics, proteomics or metabolomics results. As a first step, I usually examine the behavior of individual analysis - genes, proteins or metabolites - and obtain a long list of effect sizes, p- or q-values.\nFrequently, another layer of analysis focuses on the behavior of predefined gene sets, e.g. groups of genes whose up- or down-regulation reflects the activity of a biological process, a metabolic pathway or is indicative of a cellular state.\nThere are numerous methods to perform gene set enrichment (GSEA), over-representation (ORA) or pathway analysis, with more than 140 R packages on Bioconductor alone.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/interactive-gene-set-results/index.html#sharing-analysis-results",
    "href": "posts/interactive-gene-set-results/index.html#sharing-analysis-results",
    "title": "Interactive GSEA results: visualizations with reactable & plotly",
    "section": "Sharing analysis results",
    "text": "Sharing analysis results\nRegardless of the chosen statistical approach, GSEA or ORA analyses typically produce set-level statistics, e.g. a summary of the effect size across all members of a gene set alongside a statistic, p-value, etc.\nTo share results with my collaborators, I would like to enable them to\n\nBrowse set-level results to hone in on specific pathways / processes of interest.\nVisualize the behavior of the analytes in the set.\nDrill down to a subset of analytes and export e.g. gene-level results\n\nThe pioneering ReportingTools Bioconductor package creates static web pages for gene-set enrichment results, including gene- and set-level plots and statistics. But all of the plots are generated in advance, and interactivity is limited.\nIn this blog post, I take advantage of the reactable, plotly, crosstalk and htmlwidgets R packages to create a stand-alone interactive HTML report, allowing my collaborators to explore the results without the need for a server.\nI learned a lot about these incredibly useful packages!\n\n\n\n\n\n\nNote\n\n\n\nAt the time of writing, the current release of the reactable R package (v0.4.1) is not compatible with the latest release of the htmlwidgets (v1.6.0). This issue has already been fixed in reactable’s development version, which is available from github Alternatively, you can use the previous release of htmlwidgets (v1.5.4), e.g.  by installing it with remotes::install_version(\"htmlwidgets\", version = \"1.5.4\").\n\n\n\nFeatures\nHere, I am combining several interactive elements, linked through SharedData objects via crosstalk:\n\nAt the top, an interactive volcano plot showing the effects sizes (mean trimmed log2 fold changes) and nominal p-values for each tested gene set.\nBelow, a nested reactable table displays the results for each set. When a row is expanded\n\nIt shows a volcano plot with gene-level results, as well as a linked table with the corresponding statistics.\nThe reader can hone in on specific genes by selecting points in the volcano plot, or by searching the table.\n\n\nFirst, we define a set of helper functions are, which are composed into the main gene_set_report() function.\n\n\nShow the code\nlibrary(Biobase)\nlibrary(crosstalk)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(plotly)\nlibrary(reactable)\nlibrary(sparrow)\nlibrary(stringr)\nlibrary(htmlwidgets)  \nlibrary(V8)  # to create static HTML\n\n#' Retrieve gene-level statistics for a single gene set\n#' \n#' @param stats named list of data.frames with gene-level statistics, one for\n#' each gene set \n#' @gene_set_name Scalar character, the name of an element of `stats`.\n#' in `data`.\n#' @return A data.frame with results for a single gene set\n.get_gene_data &lt;- function(mg, gene_set_name, keep.cols = c(\n  \"symbol\", \"entrez_id\", \"logFC\", \"pval\", \"CI.L\", \"CI.R\", \"pval\", \"padj\")) {\n  sparrow::geneSet(mg, name = gene_set_name) %&gt;%\n    dplyr::select(tidyselect::any_of(keep.cols)) %&gt;%\n    dplyr::arrange(pval)\n}\n\n#' @importFrom htmltools tags\n.entrez_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\"http://www.ncbi.nlm.nih.gov/gene/%s\",\n                   value)\n    return(htmltools::tags$a(href = url, target = \"_blank\", \n                             as.character(value)))\n  } else {\n    return(value)\n  }\n}\n\n#' @importFrom htmltools tags\n.symbol_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\n      \"https://www.genenames.org/tools/search/#!/?query=%s\",\n      value)\n    return(\n      htmltools::tags$a(href = url, target = \"_blank\", as.character(value))\n    )\n  } else {\n    return(value)\n  }\n}\n\n#' @importFrom htmltools tags\n.msigdb_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\n      \"https://www.gsea-msigdb.org/gsea/msigdb/human/geneset/%s.html\",\n      value)\n    return(\n      htmltools::tags$a(href = url, target = \"_blank\", as.character(value))\n    )\n  } else {\n    return(value)\n  }\n}\n\n#' Create a reactable table with gene-level results\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param defaultColDef A list that defines the default configuration for a \n#' column, typically the output of the [reactable::colDef] function.\n#' @param columns A list of column definitions, each generated with the \n#' [reactable::colDef] function.\n#' @param theme A `reactableTheme` object, typically generated with a call to\n#' the [reactable::reactableTheme] function.\n#' @param striped Scalar flag, display stripes?\n#' @param bordered Scalar flag, display borders?\n#' @param highlight Scalar flag, highlight selected rows?\n#' @param searchable Scalar flag, add search box?\n#' @param defaultPageSize Scalar integer, the default number of rows to display.\n#' @param elementId Scalar character, an (optional) element identifier\n#' @param ... Additional arguments for the [reactable::reactable] function.\n#' @return A `reactable` object.\n#' @export\n#' @importFrom reactable colDef reactable colFormat\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    symbol = c(\"TP53\", \"KRAS\", \"PIK3CA\"),\n#'    pval = runif(3, 0, 1),\n#'    logFC = rnorm(3)\n#' )\n#' stats_table(df)\n#' }\nstats_table &lt;- function(\n    data, \n    defaultColDef = reactable::colDef(\n      align = \"center\",\n      minWidth = 100,\n      sortNALast = TRUE\n    ),\n    columns = list(\n      symbol = reactable::colDef(\n        name = \"Symbol\",\n        cell = .symbol_url\n      ),\n      entrezid = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      entrez_id = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      entrez = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      pval = reactable::colDef(\n        name = \"P-value\",\n        format = reactable::colFormat(digits = 4)),\n      padj = reactable::colDef(\n        name = \"P-value\",\n        format = reactable::colFormat(digits = 4)),\n      t = reactable::colDef(\n        name = \"t-statistic\",\n        format = reactable::colFormat(digits = 2)),\n      B = reactable::colDef(\n        name = \"log-odds\",\n        format = reactable::colFormat(digits = 2)),\n      AveExpr = reactable::colDef(\n        name = \"Mean expr\",\n        format = reactable::colFormat(digits = 2)),\n      CI.L = reactable::colDef(\n        name = \"Lower 95% CI\",\n        format = reactable::colFormat(digits = 2)),\n      CI.R = reactable::colDef(\n        name = \"Upper 95% CI\",\n        format = reactable::colFormat(digits = 2)),\n      logFC = reactable::colDef(\n        name = \"logFC\", \n        format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- \"firebrick\"\n          } else if (value &lt; 0) {\n            color &lt;- \"navy\"\n          } else {\n            color &lt;- \"lightgrey\"\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      )\n    ),\n    theme = reactable::reactableTheme(\n      stripedColor = \"#f6f8fa\",\n      highlightColor = \"#f0f5f9\",\n      cellPadding = \"8px 12px\",\n      style = list(\n        fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, \n        Arial, sans-serif\")\n    ),\n    striped = FALSE,\n    bordered = FALSE,\n    highlight = TRUE,\n    searchable = TRUE,\n    defaultPageSize = 10L,\n    elementId = NULL,\n    ...\n) {\n  reactable::reactable(\n    data = data,\n    searchable = searchable,\n    striped = striped,\n    bordered = bordered,\n    highlight = highlight,\n    selection = \"multiple\",\n    onClick = \"select\",\n    rowStyle = list(cursor = \"pointer\"),\n    theme = theme,\n    defaultPageSize = defaultPageSize,\n    defaultColDef = defaultColDef,\n    columns = columns,\n    elementId = elementId,\n    ...\n  )\n}\n\n#' Wrap stats_table() output in a div html tag\n#' \n#' @param style Scalar character, the style tag for the tag\n#' @param elementId Scalar character, the element identifier\n#' @param ... Arguments passed on to the `stats_table` function.\n#' @return A `shiny.tag` object.\n#' @importFrom htmltools div tags\n.stats_table_div &lt;- function(\n    style = paste0(\n      \"width: 50%;\",\n      \"float: right;\",\n      \"padding-top: 1rem;\"\n    ),\n    elementId = NULL,\n    ...) {\n  if (is.null(elementId)) {\n    elementId &lt;- basename(tempfile(pattern = \"id\"))\n  }    \n  htmltools::div(\n    style = style,\n    htmltools::tagList(\n      stats_table(..., elementId = elementId),\n      # download button\n      htmltools::tags$button(\n        \"\\u21E9 Download as CSV\",\n        onclick = sprintf(\"Reactable.downloadDataCSV('%s', 'gene-results.csv')\",\n                          elementId)\n      )\n    )    \n  )\n}\n\n#' Create an interactive volcano plot\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param x A `formula` defining the column of `data` mapped to the x-axis.\n#' @param y A `formula` defining the column of `data` mapped to the y-axis.\n#' @param text A `formula` defining the column of `data` mapped to the tooltip.\n#' @param title Scalar character, the title of the plot.\n#' @param xlab Scalar character, the title of the x-axis\n#' @param ylab Scalar character, the title pf the y-axis\n#' @param title.width Scalar integer, the target line width (passed on to the\n#' [stringr::str_wrap] function.\n#' @param opacity Scalar numeric between 0 and 1, the opacity of the points.\n#' @param marker A list defining the size, line and color limits of the points.\n#' @param colors Character vector of colors used to shade the points.\n#' @param highlight.color Scalar character, the color used to highlight selected\n#' points.\n#' @param webGL Scalar flag, use webGL to render the plot?\n#' @param width Scalar numeric or scalar character, width of the plot\n#' @param height Scalar numeric or scalar character, height of the plot\n#' @param ... Additional arguments passed to the [plotly::plot_ly] function.\n#' @return A `plotly` object.\n#' @importFrom plotly plot_ly add_trace config layout highlight toWebGL\n#' @importFrom grDevices colorRampPalette\n#' @export\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    symbol = letters,\n#'    pval = runif(length(letters), 0, 1),\n#'    logFC = rnorm(length(letters))\n#' )\n#' volcano_plot(df)\n#' }\nvolcano_plot &lt;- function(\n    data, \n    x = ~logFC,\n    y = ~-log10(pval),\n    text = ~symbol,\n    title = \"\",\n    xlab = \"Fold change (log2)\",\n    ylab = \"-log10(pval)\",\n    title.width = 35L,\n    opacity = 0.5,\n    marker = list(\n      color = ~logFC,\n      size = 10, \n      cmax = 3,\n      cmid = 0,\n      cmin = -3,\n      line = list(color = \"grey\", width = 1)),\n    colors = grDevices::colorRampPalette(\n      c('navy', 'lightgrey', 'firebrick'))(15),\n    highlight.color = \"red\",\n    webGL = FALSE,\n    width = NULL,\n    height = NULL,\n    ...) {\n  p &lt;- plotly::plot_ly(\n    width = width,\n    height = height\n  ) %&gt;% \n    plotly::add_trace(\n      data = data, \n      name = \"\",\n      type = 'scatter',\n      mode = 'markers',\n      x = x,\n      y = y,\n      text = text,\n      hoverinfo =\"text\",\n      opacity = opacity,\n      colors = colors,\n      marker = marker,\n      ...\n    ) %&gt;%\n    plotly::config(displaylogo = FALSE) %&gt;%\n    plotly::layout(\n        xaxis = list(title = xlab),\n        yaxis = list(title = ylab),\n        title = stringr::str_wrap(\n          stringr::str_replace_all(title, \"_\", \" \"),\n          width = title.width)\n    ) %&gt;%\n    plotly::highlight(\n      color = highlight.color,\n      on = \"plotly_selected\",\n      off = \"plotly_deselect\"\n    )\n  if (isTRUE(webGL)) p &lt;- plotly::toWebGL(p)\n  return(p)\n}\n\n#' Create an interactive volcano plot for gene-set results\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param x A `formula` defining the column of `data` mapped to the x-axis.\n#' @param y A `formula` defining the column of `data` mapped to the y-axis.\n#' @param text A `formula` defining the column of `data` mapped to the tooltip.\n#' @param xlab Scalar character, the title of the x-axis\n#' @param text.width Scalar integer, the target line width (passed on to the\n#' [stringr::str_wrap] function.\n#' @param hovertemplate Scalar character defining the tooltip template.\n#' @param marker A list defining the size, line and color limits of the points.\n#' @param width Scalar numeric or scalar character, width of the plot\n#' @param height Scalar numeric or scalar character, height of the plot\n#' @param ... Additional arguments passed to the [volcano_plot] function.\n#' @return A `plotly` object.\n#' @importFrom grDevices colorRampPalette\n#' @importFrom stringr str_wrap str_replace_all\n#' @export\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    name = paste(\"Set\", letters),\n#'    pval = runif(length(letters), 0, 1),\n#'    mean.logFC.trim = rnorm(length(letters)),\n#'    n = sample(1:100, size = length(letters))\n#' )\n#' volcano_gene_set_plot(df)\n#' }\nvolcano_gene_set_plot &lt;- function(\n    data,\n    text = ~stringr::str_wrap(\n          stringr::str_replace_all(name, \"_\", \" \"),\n          width = text.width),\n    text.width = 25,\n    x = ~mean.logFC.trim,\n    y = ~-log10(pval),\n    marker = list(\n      color = ~mean.logFC.trim,\n      size = ~n, \n      sizemode = 'area', \n      cmax = 2,\n      cmid = 0,\n      cmin = -2,\n      line = list(color = \"grey\", width = 1)\n    ), \n    hovertemplate = paste(\n            '&lt;b&gt;%{text}&lt;/b&gt;',\n            '&lt;br&gt;&lt;i&gt;logFC&lt;/i&gt;: %{x:.2f}',\n            '&lt;br&gt;&lt;i&gt;-log10(pval)&lt;/i&gt;: %{y:.2f}',\n            '&lt;br&gt;&lt;i&gt;n&lt;/i&gt;: %{marker.size}',\n            '&lt;br&gt;'),\n    xlab = \"Fold change (log2)\",\n    width = NULL,\n    height = NULL,\n    ...)\n{\n  volcano_plot(\n    data = data, \n    text = text, \n    x = x, \n    y = y,\n    marker = marker, \n    xlab = xlab,\n    hovertemplate = hovertemplate,\n    width = width,\n    height = height,\n    ...)\n}\n\n#' Wrap volcano_plot() output in a div html tag\n#' \n#' @param helptext Scalar character, text to display below the plot.\n#' @param style Scalar character, the style tag for the tag\n#' @param ... Arguments passed on to the `volcano_plot` function.\n#' @return A `shiny.tag` object.\n#' @importFrom htmltools div tagList p\n.volcano_plot_div &lt;- function(\n    helptext = paste(\"Draw a rectangle / use the lasso tool to select points,\",\n                     \"double-click to deselect all.\"), \n    style = paste0(\n      \"width: 50%;\",\n      \"float: left;\",\n      \"padding-right: 1rem;\",\n      \"padding-top: 4rem;\"\n    ), \n    ...) {\n  htmltools::div(\n    style = style, {\n      htmltools::tagList(\n        volcano_plot(...),\n        htmltools::p(helptext)\n      )\n    }\n  )\n}\n\n#' Helper function to combine gene-level outputs into a single div\n#' \n#' @param data A data.frame with gene-set results.\n#' @param stats A named list of data.frames whose names much match the `name`\n#' column of `data`.\n#' @param index Scalar count, the row of `data` to plot.\n#' @return A `shiny.tag` object containing the output of the \n#' `.volcano_plot_div()` and `.stats_table_div()` functions.\n#' @importFrom crosstalk SharedData\n#' @importfrom htmltools tagList div\n.row_details &lt;- function(data, mg, index) {\n  gene_data &lt;- .get_gene_data(mg = mg, gene_set_name = data$name[index])\n  gd &lt;- crosstalk::SharedData$new(gene_data)\n  htmltools::div(\n    htmltools::tagList(\n      # volcano plot\n      .volcano_plot_div(data = gd, title = data$name[index]),\n      # interactive gene-stat table\n      .stats_table_div(data = gd)\n    )\n  )\n}\n\n#' Create a nested gene set result table\n#' \n#' @param mg A `SparrowResult` object\n#' @param max.pval Scalar numeric, the largest (uncorrected) p-value for which\n#' to return results.\n#' @param max.results Scalar integer, the top number of rows to return\n#' (ordered by p-value).\n#' @param color.up Scalar character, the color for positive log2 fold changes.\n#' @param color.down Scalar character, the color for negative log2 fold changes.\n#' @param color.ns Scalar character, the color for zero log2 fold change.\n#' @param theme A `reactableTheme` object, typically generated with a call to\n#' the [reactable::reactableTheme] function.\n#' @param defaultColDef A list that defines the default configuration for a \n#' column, typically the output of the [reactable::colDef] function.\n#' @param columns A list of column definitions, each generated with the \n#' [reactable::colDef] function.\n#' @param bordered Scalar flag, display borders?\n#' @param highlight Scalar flag, highlight selected rows?\n#' @param searchable Scalar flag, add search box?\n#' @param striped Scalar flag, alternate row shading?\n#' @param defaultPageSize Scalar integer, the default number of rows to display.\n#' @param pageSizeOptions Integer vector that will populate the pagination menu.\n#' @param paginationType Scalar character, the pagination control to use. Either\n#' `numbers` for page number buttons (the default), `jump` for a page jump, or \n#' `simple` to show 'Previous' and 'Next' buttons only.\n#' @param elementId Scalar character, an (optional) element identifier\n#' @param defaultSorted Character vector of column names to sort by default. Or\n#' to customize sort order, a named list with values of `asc` or `desc`.\n#' @param name_url A function that returns a `shiny.tag` (usually an \n#' `&lt;a href&gt;&lt;/a&gt;` tag) for each element of the `name` column of `data` to link\n#' to more information about the gene set (e.g. on the MSigDb website, etc).\n#' @param ... Additional arguments for the [reactable::reactable] function.\n#' @importFrom reactable reactable reactableTheme colDef colFormat\n#' @return A `reactable` object with one row for each row in `data`, each of\n#' which can be expanded into the output of the `.row_details()` function\n#' for that specific gene set.\n#' @export\n#' @examples\n#' \\dontrun{\n#' library(sparrow)\n#' vm &lt;- sparrow::exampleExpressionSet()\n#' gdb &lt;- sparrow::exampleGeneSetDb()\n#' mg &lt;- sparrow::seas(vm, gdb, c('fry'), design = vm$design, \n#'                     contrast = 'tumor')\n#' gene_set_table(mg, max.results = 10)\n#' }\ngene_set_table &lt;- function(\n    mg,\n    max.pval = 0.05,\n    max.results = Inf,\n    keep.cols = c(\"collection\", \"name\", \"n\", \"pval\", \"padj\", \n                      \"mean.logFC.trim\"),\n    method = resultNames(mg)[1],\n    color.up = \"firebrick\", \n    color.down = \"navy\",\n    color.ns = \"grey50\",\n    theme = reactable::reactableTheme(\n      stripedColor = \"grey95\",\n      highlightColor = \"grey80\",\n      cellPadding = \"8px 12px\",\n      style = list(\n        fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, \n        Arial, sans-serif\")\n    ),\n    defaultColDef = reactable::colDef(\n      header = function(value) value,\n      align = \"center\",\n      minWidth = 100,\n      headerStyle = list(background = \"#f7f7f8\"),\n      sortNALast = TRUE\n    ),\n    name_url = function(value) {value},\n    columns = list(\n      collection = reactable::colDef(\n        name = \"Collection\"),\n      name = reactable::colDef(\n        name = \"Gene set\",\n        cell = name_url,\n        minWidth = 150),\n      pval = reactable::colDef(\n        name = \"P-value\", aggregate = \"min\",\n        format = reactable::colFormat(digits = 4)),\n      padj = reactable::colDef(\n        name = \"FDR\", aggregate = \"min\",\n        format = reactable::colFormat(digits = 4)),\n      Direction =  reactable::colDef(\n        name = \"dir\", minWidth = 45, \n        cell = function(value) {\n          if (value == \"Up\")  \"\\u2B06\" else \"\\u2B07\"\n        }),\n      logFC = reactable::colDef(\n        name = \"logFC\", format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- color.up\n          } else if (value &lt; 0) {\n            color &lt;- color.down\n          } else {\n            color &lt;- color.ns\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      ),\n      mean.logFC.trim = reactable::colDef(\n        name = \"logFC\", format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- color.up\n          } else if (value &lt; 0) {\n            color &lt;- color.down\n          } else {\n            color &lt;- color.ns\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      )\n    ),\n    elementId = \"expansion-table\",\n    static = TRUE,\n    filterable = TRUE,\n    searchable = TRUE,\n    bordered = TRUE,\n    striped = FALSE,\n    highlight = TRUE,\n    defaultPageSize = 25L,\n    showPageSizeOptions = TRUE,\n    pageSizeOptions = sort(unique(c(25, 50, 100, nrow(data)))),\n    paginationType = \"simple\",\n    defaultSorted = list(pval = \"asc\")\n) {\n  data = sparrow::result(mg, method) %&gt;%\n    dplyr::slice_min(n = max.results, order_by = pval) %&gt;%\n    dplyr::filter(pval &lt;= max.pval) %&gt;%\n    dplyr::select(tidyselect::any_of(keep.cols))\n  if (nrow(data) == 0) {\n    warning(\"None of the gene sets pass the `max.pval` threshold.\")\n    return(NULL)\n  }\n  reactable::reactable(\n    data,\n    elementId = elementId,\n    defaultColDef = defaultColDef,\n    static = static,\n    filterable = filterable,\n    searchable = searchable,\n    bordered = bordered,\n    highlight = highlight,\n    theme = theme,\n    defaultPageSize = defaultPageSize,\n    showPageSizeOptions = showPageSizeOptions,\n    pageSizeOptions = pageSizeOptions,\n    paginationType = paginationType,\n    defaultSorted = defaultSorted,\n    columns = columns,\n    details = function(index) {\n      .row_details(data = data, mg = mg, index)\n    }\n  )\n}\n\n#' Wrapper to create a div HTML tag\n#' @param mg A `SparrowResult` object\n#' @param method Scalar character, which results to return from `mg`.\n#' @param max.pal Scalar numeric, return only results wiht an (uncorrected)\n#' &lt;= `max.pal`.\n#' @param verbose Scalar flag, show messages?\n#' @param title Scalar character, the `h1` title for the element\n#' @param elementId Scalar character, the element identifier for the interactive\n#' table.\n#' @param style Scalar character, the style tag for the tag\n#' @param ... Additional arguments passed on to the `gene_set_table` function.\n#' @return A `shiny.tag` object containing the output of the \n#' `gene_set_table()` function.\n#' @importFrom htmltools div h1 tagList tags\n#' @export\ngene_set_report &lt;- function(\n    mg,\n    method = resultNames(mg)[1], \n    max.pval = 0.05,\n    max.results = Inf,\n    verbose = TRUE,\n    title = \"Gene set enrichment analysis\",\n    elementId = \"expansion-table\",\n    style = \"\",\n    ...\n) {\n  if (!is.finite(max.results)) {\n    message.log &lt;- sprintf(\n      paste(\"Reporting all '%s' results with (uncorrected)\",\n            \"p-value &lt;= %s\"), \n      method, max.pval)\n  } else {\n    message.log &lt;- sprintf(\n      paste(\"Reporting up to %s '%s' results with (uncorrected)\",\n            \"p-value &lt;= %s\"), \n      max.results, method, max.pval)\n  }\n  if (isTRUE(verbose)) {\n    message(message.log)\n  }\n  htmltools::div(\n    style = style, \n    {\n      htmltools::tagList(\n        htmltools::h1(title),\n        htmltools::p(message.log),\n        # volcano plot\n        sparrow::result(mg, method) %&gt;%\n          dplyr::slice_min(n = max.results, order_by = pval) %&gt;%\n          volcano_gene_set_plot(width = \"50%\"),\n        htmltools::br(),\n        # expansion button\n        htmltools::tags$button(\n          \"Expand/collapse all rows\",\n          onclick = sprintf(\"Reactable.toggleAllRowsExpanded('%s')\", elementId)\n        ),\n        gene_set_table(mg = mg, max.pval = max.pval, max.results = max.results,\n                       ...)\n      )\n    })\n}\n\n\nNext, we perform a gene set enrichment analysis with sparrow’s seas function. It returns a convenient SparrowResult S4 object with both gene-set statistics and gene-level differential expression results.\n\n# gene set enrichment analysis with the sparrow Bioconductor package\nvm &lt;- exampleExpressionSet()\ngdb &lt;- exampleGeneSetDb()\nmg &lt;- seas(vm, gdb, c('fry'), design = vm$design, contrast = 'tumor')\n\nFinally, we pass the mg object to our gene_set_report() function, together with arguments requesting that all gene-set results passing a p-value threshold of &lt; 0.05 are included. We also pass the .msigdb_url helper function to the name_url argument, to link the name of each gene set to its description on the msigdb website.\n\n# create an interactive report\nhtmltools::browsable(\n  gene_set_report(mg, method = \"fry\", max.pval = 0.05, max.results = Inf, \n                  name_url = .msigdb_url)\n)\n\nReporting all 'fry' results with (uncorrected) p-value &lt;= 0.05\n\n\n\n\nGene set enrichment analysis\nReporting all 'fry' results with (uncorrected) p-value &lt;= 0.05\n\n\n\nExpand/collapse all rows\nCollectionGene setnP-valueFDRlogFC​​c2LOPEZ_MESOTELIOMA_SURVIVAL_TIME_UP140.00070.02141.92​​c2BOYAULT_LIVER_CANCER_SUBCLASS_G123_DN410.00090.0214-0.65​​c2BIOCARTA_AGPCR_PATHWAY110.00130.0214-0.50​​c2GUTIERREZ_WALDENSTROEMS_MACROGLOBULINEMIA_1_UP90.00150.0214-0.60​​c6KRAS.LUNG.BREAST_UP.V1_DN960.00160.0214-0.34​​c2DARWICHE_PAPILLOMA_RISK_HIGH_VS_LOW_DN260.00170.0214-0.39​​c6KRAS.600.LUNG.BREAST_UP.V1_DN1890.00240.0259-0.26​​c2YAMASHITA_LIVER_CANCER_STEM_CELL_DN470.00310.0278-0.44​​c7GSE12845_IGD_POS_BLOOD_VS_PRE_GC_TONSIL_BCELL_UP1770.00350.0278-0.13​​c2SOTIRIOU_BREAST_CANCER_GRADE_1_VS_3_UP1490.00370.02781.83​​c6KRAS.BREAST_UP.V1_DN900.00450.0314-0.31​​c6KRAS.50_UP.V1_UP330.00540.0341-0.67​​c6GLI1_UP.V1_DN240.00600.03520.18​​c2SCHAEFFER_PROSTATE_DEVELOPMENT_48HR_DN3560.00730.0377-0.33​​c7GSE22886_IGG_IGA_MEMORY_BCELL_VS_BLOOD_PLASMA_CELL_UP1760.00760.0377-0.29​​c6CRX_NRL_DN.V1_DN990.00790.0377-0.21​​c6ATM_DN.V1_UP890.01130.0504-0.41​​c6CAMP_UP.V1_DN1830.01350.0569-0.23​​c6KRAS.DF.V1_DN1460.01630.0622-0.19​​c2REACTOME_MRNA_SPLICING_MINOR_PATHWAY420.01640.06220.34​​c2HUPER_BREAST_BASAL_VS_LUMINAL_UP520.01920.0679-0.76​​c6MEL18_DN.V1_DN1250.01960.0679-0.35​​c6KRAS.LUNG_UP.V1_DN980.02320.0741-0.43​​c6TBK1.DN.48HRS_DN500.02430.07410.17​​c2TURASHVILI_BREAST_LOBULAR_CARCINOMA_VS_DUCTAL_NORMAL_DN910.02450.0741-0.941–25 of 34 rowsShow 253450100Previous1 of 2Next\n\n\n\n\n\n\n\nDetails\n\nGene set enrichment analysis\nIn this example, I am using Steve Lianoglou’s sparrow Bioconductor package to perform gene set enrichment analysis. But any other method could be used, as long as both set-level and gene-level differential expression statistics can be obtained.\nsparrow supports numerous GSEA and ORA methods. Here, I am using the fry algorithm from the limma Bioconductor package for illustration.\n\n\nReproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package          * version     date (UTC) lib source\n annotate           1.76.0      2022-11-01 [1] Bioconductor\n AnnotationDbi      1.60.0      2022-11-01 [1] Bioconductor\n askpass            1.1         2019-01-13 [1] CRAN (R 4.2.0)\n assertthat         0.2.1       2019-03-21 [1] CRAN (R 4.2.0)\n babelgene          22.9        2022-09-29 [1] CRAN (R 4.2.0)\n backports          1.4.1       2021-12-13 [1] CRAN (R 4.2.0)\n Biobase          * 2.58.0      2022-11-01 [1] Bioconductor\n BiocGenerics     * 0.44.0      2022-11-01 [1] Bioconductor\n BiocIO             1.8.0       2022-11-01 [1] Bioconductor\n BiocParallel       1.32.4      2022-12-01 [1] Bioconductor\n BiocSet            1.12.0      2022-11-01 [1] Bioconductor\n Biostrings         2.66.0      2022-11-01 [1] Bioconductor\n bit                4.0.5       2022-11-15 [1] CRAN (R 4.2.0)\n bit64              4.0.5       2020-08-30 [1] CRAN (R 4.2.0)\n bitops             1.0-7       2021-04-24 [1] CRAN (R 4.2.0)\n blob               1.2.3       2022-04-10 [1] CRAN (R 4.2.0)\n cachem             1.0.6       2021-08-19 [1] CRAN (R 4.2.0)\n checkmate          2.1.0       2022-04-21 [1] CRAN (R 4.2.0)\n circlize           0.4.15      2022-05-10 [1] CRAN (R 4.2.0)\n cli                3.5.0       2022-12-20 [1] CRAN (R 4.2.0)\n clue               0.3-63      2022-11-19 [1] CRAN (R 4.2.0)\n cluster            2.1.4       2022-08-22 [2] CRAN (R 4.2.2)\n codetools          0.2-18      2020-11-04 [2] CRAN (R 4.2.2)\n colorspace         2.0-3       2022-02-21 [1] CRAN (R 4.2.0)\n ComplexHeatmap     2.14.0      2022-11-01 [1] Bioconductor\n crayon             1.5.2       2022-09-29 [1] CRAN (R 4.2.0)\n credentials        1.3.2       2021-11-29 [1] CRAN (R 4.2.0)\n crosstalk        * 1.2.0       2021-11-04 [1] CRAN (R 4.2.0)\n curl               4.3.3       2022-10-06 [1] CRAN (R 4.2.0)\n data.table         1.14.6      2022-11-16 [1] CRAN (R 4.2.0)\n DBI                1.1.3       2022-06-18 [1] CRAN (R 4.2.0)\n digest             0.6.31      2022-12-11 [1] CRAN (R 4.2.0)\n doParallel         1.0.17      2022-02-07 [1] CRAN (R 4.2.0)\n dplyr            * 1.0.10      2022-09-01 [1] CRAN (R 4.2.0)\n edgeR              3.40.1      2022-12-14 [1] Bioconductor\n ellipsis           0.3.2       2021-04-29 [1] CRAN (R 4.2.0)\n evaluate           0.19        2022-12-13 [1] CRAN (R 4.2.0)\n fansi              1.0.3       2022-03-24 [1] CRAN (R 4.2.0)\n fastmap            1.1.0       2021-01-25 [1] CRAN (R 4.2.0)\n foreach            1.5.2       2022-02-02 [1] CRAN (R 4.2.0)\n generics           0.1.3       2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb       1.34.4      2022-12-01 [1] Bioconductor\n GenomeInfoDbData   1.2.9       2022-12-12 [1] Bioconductor\n GetoptLong         1.0.5       2020-12-15 [1] CRAN (R 4.2.0)\n ggplot2          * 3.4.0       2022-11-04 [1] CRAN (R 4.2.0)\n GlobalOptions      0.1.2       2020-06-10 [1] CRAN (R 4.2.0)\n glue               1.6.2       2022-02-24 [1] CRAN (R 4.2.0)\n graph              1.76.0      2022-11-01 [1] Bioconductor\n gridExtra          2.3         2017-09-09 [1] CRAN (R 4.2.0)\n GSEABase           1.60.0      2022-11-01 [1] Bioconductor\n gtable             0.3.1       2022-09-01 [1] CRAN (R 4.2.0)\n htmltools        * 0.5.4       2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets      * 1.5.4       2021-09-08 [1] CRAN (R 4.2.2)\n httpuv             1.6.7       2022-12-14 [1] CRAN (R 4.2.0)\n httr               1.4.4       2022-08-17 [1] CRAN (R 4.2.0)\n IRanges            2.32.0      2022-11-01 [1] Bioconductor\n irlba              2.3.5.1     2022-10-03 [1] CRAN (R 4.2.0)\n iterators          1.0.14      2022-02-05 [1] CRAN (R 4.2.0)\n jsonlite           1.8.4       2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST           1.38.0      2022-11-01 [1] Bioconductor\n knitr              1.41        2022-11-18 [1] CRAN (R 4.2.0)\n later              1.3.0       2021-08-18 [1] CRAN (R 4.2.0)\n lattice            0.20-45     2021-09-22 [2] CRAN (R 4.2.2)\n lazyeval           0.2.2       2019-03-15 [1] CRAN (R 4.2.0)\n lifecycle          1.0.3       2022-10-07 [1] CRAN (R 4.2.0)\n limma              3.54.0      2022-11-01 [1] Bioconductor\n locfit             1.5-9.6     2022-07-11 [1] CRAN (R 4.2.0)\n magrittr           2.0.3       2022-03-30 [1] CRAN (R 4.2.0)\n Matrix             1.5-3       2022-11-11 [1] CRAN (R 4.2.0)\n matrixStats        0.63.0      2022-11-18 [1] CRAN (R 4.2.0)\n memoise            2.0.1       2021-11-26 [1] CRAN (R 4.2.0)\n mime               0.12        2021-09-28 [1] CRAN (R 4.2.0)\n munsell            0.5.0       2018-06-12 [1] CRAN (R 4.2.0)\n ontologyIndex      2.10        2022-08-24 [1] CRAN (R 4.2.0)\n openssl            2.0.5       2022-12-06 [1] CRAN (R 4.2.0)\n pillar             1.8.1       2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig          2.0.3       2019-09-22 [1] CRAN (R 4.2.0)\n plotly           * 4.10.1.9000 2022-12-20 [1] Github (plotly/plotly.R@3a33b1a)\n plyr               1.8.8       2022-11-11 [1] CRAN (R 4.2.0)\n png                0.1-8       2022-11-29 [1] CRAN (R 4.2.0)\n promises           1.2.0.1     2021-02-11 [1] CRAN (R 4.2.0)\n purrr              1.0.0       2022-12-20 [1] CRAN (R 4.2.0)\n R6                 2.5.1       2021-08-19 [1] CRAN (R 4.2.0)\n RColorBrewer       1.1-3       2022-04-03 [1] CRAN (R 4.2.0)\n Rcpp               1.0.9       2022-07-08 [1] CRAN (R 4.2.0)\n RCurl              1.98-1.9    2022-10-03 [1] CRAN (R 4.2.0)\n reactable        * 0.4.1.9000  2022-12-20 [1] Github (glin/reactable@cf500a1)\n reactR             0.4.4       2021-02-22 [1] CRAN (R 4.2.0)\n rjson              0.2.21      2022-01-09 [1] CRAN (R 4.2.0)\n rlang              1.0.6       2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown          2.19        2022-12-15 [1] CRAN (R 4.2.0)\n RSQLite            2.2.19      2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi         0.14        2022-08-22 [1] CRAN (R 4.2.0)\n S4Vectors          0.36.1      2022-12-05 [1] Bioconductor\n scales             1.2.1       2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo        1.2.2       2021-12-06 [1] CRAN (R 4.2.0)\n shape              1.4.6       2021-05-19 [1] CRAN (R 4.2.0)\n shiny              1.7.4       2022-12-15 [1] CRAN (R 4.2.0)\n sparrow          * 1.4.0       2022-11-01 [1] Bioconductor\n statmod            1.4.37      2022-08-12 [1] CRAN (R 4.2.0)\n stringi            1.7.8       2022-07-11 [1] CRAN (R 4.2.0)\n stringr          * 1.5.0       2022-12-02 [1] CRAN (R 4.2.0)\n sys                3.4.1       2022-10-18 [1] CRAN (R 4.2.0)\n tibble             3.1.8       2022-07-22 [1] CRAN (R 4.2.0)\n tidyr              1.2.1       2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect         1.2.0       2022-10-10 [1] CRAN (R 4.2.0)\n utf8               1.2.2       2021-07-24 [1] CRAN (R 4.2.0)\n V8               * 4.2.2       2022-11-03 [1] CRAN (R 4.2.0)\n vctrs              0.5.1       2022-11-16 [1] CRAN (R 4.2.0)\n viridis            0.6.2       2021-10-13 [1] CRAN (R 4.2.0)\n viridisLite        0.4.1       2022-08-22 [1] CRAN (R 4.2.0)\n withr              2.5.0       2022-03-03 [1] CRAN (R 4.2.0)\n xfun               0.35        2022-11-16 [1] CRAN (R 4.2.0)\n XML                3.99-0.13   2022-12-04 [1] CRAN (R 4.2.0)\n xtable             1.8-4       2019-04-21 [1] CRAN (R 4.2.0)\n XVector            0.38.0      2022-11-01 [1] Bioconductor\n yaml               2.3.6       2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc           1.44.0      2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the fourth of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis analysis compares the performance of the nf-core/rnaseq workflow for QuantSeq FWD 3’ tag RNAseq data with unique molecular identifiers (UMIs).\nThe differential expression analysis results are highly concordant with those obtained in the original publication.\nWith the appropriate settings, the nf-core/rnaseq workflow is a valid data processing pipeline for this data type.\n\nThe first post in this series walked through the preprocesssing of QuantSeq FWD data published in a preprint by Nugent et al, 2020, who used the QuantSeq FWD library preparation protocol and added unique molecular identifiers (UMIs). The UMIs were used to identify and remove PCR duplicates during the data preprocessing steps.\nHere, we use Bioconductor/R packages to reproduce the downstream results. We perform the same analysis twice with either\n\nthe original counts matrix published by the authors 1\nthe output of the nf-core/rnaseq workflow\n\n\nlibrary(dplyr)\nlibrary(edgeR)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(org.Mm.eg.db)\nlibrary(SummarizedExperiment)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#sample-annotations",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#sample-annotations",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Sample annotations",
    "text": "Sample annotations\nWe start by retrieving the sample annotation table, listing e.g. the sex, and genotype for each mouse, and the batch for each collected sample.\nThis information is available in the SRA Run Explorer. (I saved it in the sample_metadata.csv CSV file if you want to follow along&gt;.\n\nsample_sheet &lt;- file.path(work_dir, \"sample_metadata.csv\")\nsample_anno &lt;- read.csv(sample_sheet, row.names = \"Experiment\")\nhead(sample_anno[, c(\"Run\", \"Animal.ID\", \"Age\", \"age_unit\", \"Cell_type\",\n                     \"sex\", \"Genotype\", \"Sample.Name\")])\n\n                  Run Animal.ID Age age_unit Cell_type    sex  Genotype\nSRX6420531 SRR9659551       IL1   2   months astrocyte female TREM2 +/+\nSRX6420532 SRR9659552       IL1   2   months microglia female TREM2 +/+\nSRX6420533 SRR9659553      IL10   2   months astrocyte female TREM2 -/-\nSRX6420534 SRR9659554      IL10   2   months microglia female TREM2 -/-\nSRX6420535 SRR9659555      IL11  16   months astrocyte female TREM2 +/+\nSRX6420536 SRR9659556      IL11  16   months microglia female TREM2 +/+\n           Sample.Name\nSRX6420531  GSM3933549\nSRX6420532  GSM3933550\nSRX6420533  GSM3933551\nSRX6420534  GSM3933552\nSRX6420535  GSM3933553\nSRX6420536  GSM3933554\n\n\nBecause our SRA metadata doesn’t include the GEO sample title, I saved the identifier mappings in the GEO_sample_ids.csv CSV file.\n\ngeo_ids &lt;- read.csv(file.path(work_dir, \"GEO_sample_ids.csv\"))\nhead(geo_ids)\n\n  sample_name sample_id\n1  GSM3933549     IL1_A\n2  GSM3933550     IL1_M\n3  GSM3933551    IL10_A\n4  GSM3933552    IL10_M\n5  GSM3933553    IL11_A\n6  GSM3933554    IL11_M\n\n\n\n\nCode\ncolnames(sample_anno)&lt;- tolower(colnames(sample_anno))\ncolnames(sample_anno) &lt;- sub(\".\", \"_\", colnames(sample_anno), \n                             fixed = TRUE) \nsample_anno &lt;- sample_anno[, c(\"sample_name\", \"animal_id\", \"genotype\", \"sex\",\n                               \"age\", \"cell_type\")]\nsample_anno$genotype &lt;- factor(sample_anno$genotype, \n                               levels = c(\"TREM2 +/+\", \"TREM2 -/-\"))\nsample_anno$genotype &lt;- dplyr::recode_factor(\n  sample_anno$genotype,\"TREM2 +/+\" = \"WT\", \"TREM2 -/-\" = \"KO\")\nsample_anno$age &lt;- factor(sample_anno$age)\nsample_anno$sample_title &lt;- geo_ids[\n  match(sample_anno$sample_name, geo_ids$sample_name), \"sample_id\"]\nhead(sample_anno)\n\n\n           sample_name animal_id genotype    sex age cell_type sample_title\nSRX6420531  GSM3933549       IL1       WT female   2 astrocyte        IL1_A\nSRX6420532  GSM3933550       IL1       WT female   2 microglia        IL1_M\nSRX6420533  GSM3933551      IL10       KO female   2 astrocyte       IL10_A\nSRX6420534  GSM3933552      IL10       KO female   2 microglia       IL10_M\nSRX6420535  GSM3933553      IL11       WT female  16 astrocyte       IL11_A\nSRX6420536  GSM3933554      IL11       WT female  16 microglia       IL11_M\n\n\nThis experiment includes 56 samples of astrocytes or microglia cells obtained from 28 female mice that were either 2- or 16 months of age.\nThe animals are either wildtype (WT) or homozygous knockouts (KO) for the Trem2 gene."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#nugent-et-als-original-count-data",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#nugent-et-als-original-count-data",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Nugent et al’s original count data",
    "text": "Nugent et al’s original count data\nFirst, we retrieve the authors’ count matrix from NCBI GEO, available as a Supplementary tab-delimited text file.\n\ngeo_url &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE134031&\",\n                  \"format=file&file=GSE134031%5FDST120%2Etab%2Egz\")\nraw_counts &lt;- read.delim(textConnection(readLines(gzcon(url(geo_url)))))\nhead(colnames(raw_counts), 10)\n\n [1] \"mgi_symbol\"              \"gene_biotype\"           \n [3] \"ensembl_gene_id_version\" \"IL1_A\"                  \n [5] \"IL1_M\"                   \"IL10_A\"                 \n [7] \"IL10_M\"                  \"IL11_A\"                 \n [9] \"IL11_M\"                  \"IL12_A\"                 \n\n\nThe raw_counts data.frame contains information about the detected genes ( mgi_symbol, ensembl_gene_id_version) and the samples are identified by a shorthand of their GEO title (e.g. IL1_M, IL1_A).\nWe use the raw counts to populate a new DGEList object and perform Library Size Normalization with the TMM approach.\n\n\nCode\ncount_data &lt;- as.matrix(raw_counts[, grep(\"^IL\", colnames(raw_counts))])\nrow.names(count_data) &lt;- raw_counts$ensembl_gene_id_version\ncolnames(count_data) &lt;- row.names(sample_anno)[\n  match(colnames(count_data), sample_anno$sample_title)\n]\n\ngene_data &lt;- data.frame(\n  gene_id = raw_counts$ensembl_gene_id_version,\n  gene_name = raw_counts$mgi_symbol,\n  row.names = raw_counts$ensembl_gene_id_version\n)\n\ncol_data &lt;- data.frame(\n  sample_anno[colnames(count_data),\n              c(\"sample_title\", \"animal_id\",  \"age\", \"genotype\", \"cell_type\")],\n  workflow = \"geo\"\n)\n\ndge &lt;- DGEList(\n  counts = as.matrix(count_data), \n  samples = col_data[colnames(count_data), ], \n  genes = gene_data[row.names(count_data), ]\n)\n\ndge &lt;- calcNormFactors(dge, method = \"TMM\")\n\n\nThis is a large dataset, containing e.g. samples from two different cell types (microglia and astrocytes) and two different age groups (2 and 16 months).\nHere, we will restrict the analysis to microglia samples collected from older animals.\n\ndge &lt;- dge[, dge$samples$cell_type == \"microglia\" & dge$samples$age == \"16\"]\n\nLet’s identify which genes are significantly differentially expressed between the two genotypes!\n\nLinear modeling with limma/voom\nFirst, we use the edgeR::filterByExpr() function to identify genes with sufficiently large counts to be examined for differential expression.\n\ndesign &lt;- model.matrix(~ genotype, data = dge$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge, design = design)\n\nNext, we fit a linear model to the data using the limma/voom approach. The model only includes the genotype (with WT as the reference level) as a fixed effect.\n\nfit &lt;- voomLmFit(\n  dge[keep, row.names(design)], \n  design = design,\n  sample.weights = TRUE, \n  plot = FALSE\n)\nfit &lt;- eBayes(fit, robust=TRUE)\n\nThe following table displays the number of differentially up- and down-regulated genes after applying a false-discovery (adj.P.Val) threshold of 5%. We detect significant differences between KO and WT animals in a small number of genes\n\nsummary(decideTests(fit)[, \"KO\"])\n\n         KO\nDown     10\nNotSig 6343\nUp        2\n\n\nThe top 10 genes with the smallest p-values include well known markers of microglia activation:\n\ntopTreat(fit, coef = \"KO\")[, c(\"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")]\n\n                      gene_name      logFC      P.Value    adj.P.Val\nENSMUSG00000015568.16       Lpl -3.1524285 2.088121e-17 1.327001e-13\nENSMUSG00000023992.14     Trem2 -2.0556690 3.260855e-15 1.036137e-11\nENSMUSG00000079293.11    Clec7a -1.5285199 9.357950e-12 1.982326e-08\nENSMUSG00000029304.14      Spp1 -4.6265897 4.968572e-10 7.893819e-07\nENSMUSG00000003418.11   St8sia6 -1.6639344 7.279411e-09 8.240295e-06\nENSMUSG00000002602.16       Axl -1.2096969 7.779979e-09 8.240295e-06\nENSMUSG00000068129.5       Cst7 -2.0966828 9.881687e-08 8.971160e-05\nENSMUSG00000008845.9      Cd163  0.8165263 1.513307e-06 1.202133e-03\nENSMUSG00000039109.16     F13a1  0.8081490 1.735344e-06 1.225345e-03\nENSMUSG00000000682.7       Cd52 -0.7939756 4.846100e-06 3.079696e-03\n\n\nNext we repeat the same analysis with the output of the nf-core/rnaseq workflow."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#nf-corernaseq-results",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#nf-corernaseq-results",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "nf-core/rnaseq results",
    "text": "nf-core/rnaseq results\nWe start with the raw counts contained in the salmon.merged.gene_counts.rds file generated by the nf-core/rnaseq workflow.\nWe TMM-normalize the data, as before. (This step converts the SummarizedExperiment into a `DGEList object as well.)\n\ncount_file &lt;- file.path(work_dir, \"salmon.merged.gene_counts.rds\")\nse &lt;- readRDS(count_file)\nstopifnot(all(colnames(se) %in% row.names(sample_anno)))\ndge_nfcore &lt;- calcNormFactors(se, method = \"TMM\")\n\nNext, we add the sample metadata and fit the same linear model as before.\n\n\nCode\ndge_nfcore$samples &lt;- data.frame(\n  dge_nfcore$samples,\n  sample_anno[colnames(dge_nfcore),\n              c(\"sample_title\", \"animal_id\", \"age\", \"genotype\", \"cell_type\")],\n  workflow = \"nfcore\"\n)\nstopifnot(all(colnames(dge) %in% colnames(dge_nfcore)))\ndge_nfcore &lt;- dge_nfcore[, colnames(dge)]\n\ndesign &lt;- model.matrix(~ genotype, data = dge_nfcore$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge_nfcore, design = design)\nfit_nfcore &lt;- voomLmFit(\n  dge_nfcore[keep, row.names(design)], \n  design = design,\n  sample.weights = TRUE, \n  plot = FALSE\n)\n\n\nFirst sample weights (min/max) 0.3858285/1.6444242\n\n\nFinal sample weights (min/max) 0.3854635/1.6427464\n\n\nCode\nfit_nfcore &lt;- eBayes(fit_nfcore, robust=TRUE)\n\n\nAs with the original count data from NCBI GEO, we detect small number of differentially expressed genes (FDR &lt; 5%).\n\nsummary(decideTests(fit_nfcore)[, \"KO\"])\n\n         KO\nDown     10\nNotSig 7824\nUp        2\n\n\n\n\nCode\ncpms &lt;- local({\n  geo &lt;- cpm(dge, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"Nugent et al\")\n  \n  nfcore &lt;- cpm(dge_nfcore, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge_nfcore$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge_nfcore$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"nf-core\")\n  \n  dplyr::bind_rows(\n    dplyr::select(geo, any_of(intersect(colnames(geo), colnames(nfcore)))),\n    dplyr::select(nfcore, any_of(intersect(colnames(geo), colnames(nfcore))))\n  )\n})\n\ntt &lt;- rbind(\n  topTreat(fit, coef = \"KO\", number = Inf)[\n    , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"geo\"),\n   topTreat(fit_nfcore, coef = \"KO\", number = Inf)[\n     , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"nfcore\")\n) %&gt;%\n  dplyr::mutate(adj.P.Val = signif(adj.P.Val, 2)) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"gene_name\"), \n    names_from = \"dataset\", \n    values_from = \"adj.P.Val\") %&gt;%\n  dplyr::arrange(nfcore) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"gene_id\")\n\n\n\nNormalized expression\nFirst, we examine the correlation between the normalized log-transformed gene expression estimates returned from the two workflows. We focus on those genes that passed the filterByExpr thresholds above, e.g. those genes deemed sufficiently highly expressed to be assessed for differential expression.\n\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nsum_stats &lt;- cpms %&gt;%\n  dplyr::filter(gene_id %in% common_genes) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"sample_name\"),\n    values_from = \"cpm\",\n    names_from = \"dataset\") %&gt;%\n  dplyr::group_by(gene_id) %&gt;%\n  dplyr::summarise(\n    r = cor(log1p(`Nugent et al`), log1p(`nf-core`)),\n    mean_nugent = mean(`Nugent et al`),\n    mean_nfcore = mean(`nf-core`))\n\np &lt;- ggplot(data = sum_stats, aes(x = r)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(limits = c(0, 1.02), breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"Pearson correlation coefficient (R)\", \n       y = \"Number of genes\",\n       title = \"Correlation between normalized log2 counts\") +\n  theme_linedraw(14)\nprint(p)\n\nWarning: Removed 35 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nThe correlation between normalized log2 expression estimates is reasonably high, e.g. 80% of all genes showing a Pearson correlation coefficient &gt; 0.82.\nThe relatively low correlation might reflect the low RNA input of this experiment, e.g. only 6314 of the genes genes were detected with &gt; 10 UMI-corrected normalized counts per million reads.\n\np &lt;- ggplot(data = sum_stats, aes(x = mean_nugent + 1)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(trans = scales::log10_trans(),\n                     labels = scales::comma_format()) +\n  labs(x = \"Mean normalized counts per million\", \n       y = \"Number of genes\",\n       title = \"Average expression\",\n       subtitle = \"Nugent et al\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\nNext, we will examine the results of the differential expression analysis.\n\n\nDifferential expression results\nAnalyses based on either preprocessing pipeline yield similar numbers of differentially expressed genes.\n\n\nCode\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nresults &lt;- cbind(\n  decideTests(fit)[common_genes, \"KO\"], \n  decideTests(fit_nfcore)[common_genes, \"KO\"]\n)\ncolnames(results) &lt;- c(\"Nugent et al\", \"nf-core\")\nclass(results) &lt;- \"TestResults\"\nsummary(results)\n\n\n   Nugent et al nf-core\n-1           10       9\n0          6302    6303\n1             2       2\n\n\nBut are these the same genes in both sets of results?\nWe can visualize the overlap between the sets of significant genes in a Venn diagram (FDR &lt; 5%). The majority of differentially expressed genes is detected with both quantitation approaches (for both up- and down-regulated genes.)\n\nlimma::vennDiagram(results, include = c(\"up\", \"down\"),\n                   counts.col=c(\"red\", \"blue\"), mar = rep(0,4))\n\n\n\n\nFor example, the following plots show the normalized expression of the most significantly differentially expressed genes (known markers of active microglia).\n\n\nCode\nfor (gene in topTreat(fit, coef = \"KO\", number = 6)[[\"gene_id\"]]) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a hard FDR threshold can inflate the number of apparent differences, e.g. when a gene is close to the significance threshold (see below).\n\np_cor &lt;- cor(\n  fit$coefficients[common_genes, \"KO\"], \n  fit_nfcore$coefficients[common_genes, \"KO\"])\n\nThe log2 fold estimates for the Hom vs WT comparison are well correlated across the two analysis workflows (Pearson correlation coefficient R = 0.88 ).\n\nsmoothScatter(\n  fit$coefficients[common_genes, \"KO\"], \n  fit_nfcore$coefficients[common_genes, \"KO\"],\n  ylab = \"nf-core (log2FC)\",\n  xlab = \"Nugent et al (log2FC)\",\n  main = \"Homozygous APP vs WT (effect size)\"\n)\ntext(x = 1, y = -4, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\nas are the t-statistics across all examined genes:\n\np_cor &lt;- cor(\n  fit$t[common_genes, \"KO\"], \n  fit_nfcore$t[common_genes, \"KO\"])\nsmoothScatter(\n  fit$t[common_genes, \"KO\"], \n  fit_nfcore$t[common_genes, \"KO\"],\n  ylab = \"nf-core (t-statistic)\",\n  xlab = \"Nugent et al (t-statistic)\",\n  main = \"Homozygous APP vs WT (t-statistic)\")\ntext(x = 3, y = -15, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause this comparison yields only a small number of bona-fide differentially expressed genes, we don’t expect to see a high correlation between the log2 fold changes or the t-statistics between the two analyses: most of the values are very close to zero.\n\n\n\nDiscordant significance calls\n\n# genes detected in Nugent et al, but not significant with nf-core\ngenes &lt;- row.names(results)[which(abs(results[, 1]) == 1 & results[, 2] == 0)]\n\nAt FDR &lt; 5% 2 genes were reported as significantly differentially expressed with the original Nugent et al count matrix but not with the output of the nf-core/rnaseq workflow.\nAs side-by-side comparison of the FDR (adj.P.Val) for these genes confirms that the one of them (Cd52) displays significance close to the 5% threshold in the nf-core/rnaseq output as well. The second gene (Slamf8) also displays the same trend in both datasets, but is detected at lower levels (e.g. lower normalized CPMs) in the nf-core/rnaseq output.\n\nprint(tt[genes, ])\n\n                     gene_name    geo nfcore\nENSMUSG00000000682.7      Cd52 0.0031  0.089\nENSMUSG00000053318.7    Slamf8 0.0190  0.270\n\n\n\n\nExamples\nFinally, we plot the normalized gene expression estimates for the 2 discordant genes.\n\n\nCode\nfor (gene in genes) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#conclusions",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#conclusions",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Conclusions",
    "text": "Conclusions\n\nDifferential expression analyses of raw counts obtained with the nc-core/rnaseq workflow yields results that are highly concordant with those obtained with the raw counts the authors deposited in NCBI GEO.\nWith appropriate parameters the nf-core/rnaseq workflow can be applied to QuantSeq FWD 3’tag RNA-seq data that includes unique molecular identifiers."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#reproducibility",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#reproducibility",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date (UTC) lib source\n AnnotationDbi        * 1.60.0   2022-11-01 [1] Bioconductor\n askpass                1.1      2019-01-13 [1] CRAN (R 4.2.0)\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n Biobase              * 2.58.0   2022-11-01 [1] Bioconductor\n BiocGenerics         * 0.44.0   2022-11-01 [1] Bioconductor\n Biostrings             2.66.0   2022-11-01 [1] Bioconductor\n bit                    4.0.5    2022-11-15 [1] CRAN (R 4.2.0)\n bit64                  4.0.5    2020-08-30 [1] CRAN (R 4.2.0)\n bitops                 1.0-7    2021-04-24 [1] CRAN (R 4.2.0)\n blob                   1.2.3    2022-04-10 [1] CRAN (R 4.2.0)\n cachem                 1.0.6    2021-08-19 [1] CRAN (R 4.2.0)\n cli                    3.5.0    2022-12-20 [1] CRAN (R 4.2.0)\n colorspace             2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n crayon                 1.5.2    2022-09-29 [1] CRAN (R 4.2.0)\n credentials            1.3.2    2021-11-29 [1] CRAN (R 4.2.0)\n DBI                    1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n DelayedArray           0.24.0   2022-11-01 [1] Bioconductor\n digest                 0.6.31   2022-12-11 [1] CRAN (R 4.2.0)\n dplyr                * 1.0.10   2022-09-01 [1] CRAN (R 4.2.0)\n edgeR                * 3.40.1   2022-12-14 [1] Bioconductor\n ellipsis               0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate               0.19     2022-12-13 [1] CRAN (R 4.2.0)\n fansi                  1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n farver                 2.1.1    2022-07-06 [1] CRAN (R 4.2.0)\n fastmap                1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n generics               0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb         * 1.34.4   2022-12-01 [1] Bioconductor\n GenomeInfoDbData       1.2.9    2022-12-12 [1] Bioconductor\n GenomicRanges        * 1.50.2   2022-12-16 [1] Bioconductor\n ggplot2              * 3.4.0    2022-11-04 [1] CRAN (R 4.2.0)\n glue                   1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n gtable                 0.3.1    2022-09-01 [1] CRAN (R 4.2.0)\n here                 * 1.0.1    2020-12-13 [1] CRAN (R 4.2.0)\n htmltools              0.5.4    2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets            1.5.4    2021-09-08 [1] CRAN (R 4.2.2)\n httr                   1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n IRanges              * 2.32.0   2022-11-01 [1] Bioconductor\n jsonlite               1.8.4    2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST               1.38.0   2022-11-01 [1] Bioconductor\n KernSmooth             2.23-20  2021-05-03 [2] CRAN (R 4.2.2)\n knitr                  1.41     2022-11-18 [1] CRAN (R 4.2.0)\n labeling               0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n lattice                0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lifecycle              1.0.3    2022-10-07 [1] CRAN (R 4.2.0)\n limma                * 3.54.0   2022-11-01 [1] Bioconductor\n locfit                 1.5-9.6  2022-07-11 [1] CRAN (R 4.2.0)\n magrittr               2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n Matrix                 1.5-3    2022-11-11 [1] CRAN (R 4.2.0)\n MatrixGenerics       * 1.10.0   2022-11-01 [1] Bioconductor\n matrixStats          * 0.63.0   2022-11-18 [1] CRAN (R 4.2.0)\n memoise                2.0.1    2021-11-26 [1] CRAN (R 4.2.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.2.0)\n openssl                2.0.5    2022-12-06 [1] CRAN (R 4.2.0)\n org.Mm.eg.db         * 3.16.0   2022-12-29 [1] Bioconductor\n pillar                 1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n png                    0.1-8    2022-11-29 [1] CRAN (R 4.2.0)\n purrr                  1.0.0    2022-12-20 [1] CRAN (R 4.2.0)\n R6                     2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp                   1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n RCurl                  1.98-1.9 2022-10-03 [1] CRAN (R 4.2.0)\n rlang                  1.0.6    2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown              2.19     2022-12-15 [1] CRAN (R 4.2.0)\n rprojroot              2.0.3    2022-04-02 [1] CRAN (R 4.2.0)\n RSQLite                2.2.19   2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi             0.14     2022-08-22 [1] CRAN (R 4.2.0)\n S4Vectors            * 0.36.1   2022-12-05 [1] Bioconductor\n scales                 1.2.1    2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo            1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n statmod                1.4.37   2022-08-12 [1] CRAN (R 4.2.0)\n stringi                1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr                1.5.0    2022-12-02 [1] CRAN (R 4.2.0)\n SummarizedExperiment * 1.28.0   2022-11-01 [1] Bioconductor\n sys                    3.4.1    2022-10-18 [1] CRAN (R 4.2.0)\n tibble               * 3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr                * 1.2.1    2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect             1.2.0    2022-10-10 [1] CRAN (R 4.2.0)\n utf8                   1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n vctrs                  0.5.1    2022-11-16 [1] CRAN (R 4.2.0)\n withr                  2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun                   0.35     2022-11-16 [1] CRAN (R 4.2.0)\n XVector                0.38.0   2022-11-01 [1] Bioconductor\n yaml                   2.3.6    2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc               1.44.0   2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFull disclosure: I am a co-author of this publication.↩︎"
  },
  {
    "objectID": "posts/grav/index.html",
    "href": "posts/grav/index.html",
    "title": "Grav - a lightweight content management system",
    "section": "",
    "text": "Today I learned about Grav, a lightweight content-management system, from danwwilson. Grav is based on flat files, e.g. it does not require a database, and automatically renders pages written in markdown.\nThe optional admin plugin includes user management, 2-factor authentication and more.\nDefinitely something I will keep in mind, e.g. to distribute analysis results within an organization.\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the third of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis analysis compares the performance of the nf-core/rnaseq workflow for QuantSeq FWD 3’ tag RNAseq data without unique molecular identifiers.\nThe differential expression analysis results are highly concordant with those obtained in the original publication.\nWith the appropriate settings, the nf-core/rnaseq workflow is a valid data processing pipeline for this data type.\n\nThe first post in this series walked through the preprocesssing of QuantSeq FWD data published in a preprint by Xia et al.\nNext, we use Bioconductor/R packages to reproduce the downstream results. We perform the same differential gene expression analysis twice with either\n\nthe original counts matrix published by the authors 1\nthe output of the nf-core/rnaseq workflow\n\n\nlibrary(dplyr)\nlibrary(edgeR)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(org.Mm.eg.db)\nlibrary(readxl)\nlibrary(SummarizedExperiment)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#sample-annotations",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#sample-annotations",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Sample annotations",
    "text": "Sample annotations\nWe start by retrieving the sample annotation table, listing e.g. the sex, and genotype for each mouse, and the batch for each collected sample.\nThis information is available in the SRA Run Explorer. (I saved it as the sample_metadata.csv CSV file in case you want to follow along.)\n\nsample_sheet &lt;- file.path(work_dir, \"sample_metadata.csv\")\nsample_anno &lt;- read.csv(sample_sheet, row.names = \"Experiment\")\nhead(sample_anno[, c(\"Run\", \"Animal.ID\", \"Age\", \"age_unit\", \"Batch\", \"sex\",\n                     \"Genotype\", \"Sample.Name\")])\n\n                   Run Animal.ID Age age_unit Batch  sex Genotype Sample.Name\nSRX9142648 SRR12661924       LA1   8   months  Day1 male       WT  GSM4793335\nSRX9142649 SRR12661925       LA1   8   months  Day1 male       WT  GSM4793336\nSRX9142650 SRR12661926       LA6   8   months  Day1 male       WT  GSM4793337\nSRX9142651 SRR12661927       LA6   8   months  Day1 male       WT  GSM4793338\nSRX9142652 SRR12661928       LA9   8   months  Day2 male       WT  GSM4793339\nSRX9142653 SRR12661929       LA9   8   months  Day2 male       WT  GSM4793340\n\n\nBecause our SRA metadata doesn’t include the GEO sample title, I saved the identifier mappings in the GEO_sample_ids.csv CSV file.\n\ngeo_ids &lt;- read.csv(file.path(work_dir, \"GEO_sample_ids.csv\"))\nhead(geo_ids)\n\n  sample_name sample_id\n1  GSM4793335 DRN-18429\n2  GSM4793336 DRN-18430\n3  GSM4793337 DRN-18439\n4  GSM4793338 DRN-18440\n5  GSM4793339 DRN-18445\n6  GSM4793340 DRN-18446\n\n\n\n\nCode\ncolnames(sample_anno)&lt;- tolower(colnames(sample_anno))\ncolnames(sample_anno) &lt;- sub(\".\", \"_\", colnames(sample_anno), \n                             fixed = TRUE) \nsample_anno &lt;- sample_anno[, c(\"sample_name\", \"animal_id\", \"genotype\", \"sex\",\n                               \"batch\")]\nsample_anno$genotype &lt;- factor(sample_anno$genotype, \n                               levels = c(\"WT\", \"Het\", \"Hom\"))\nsample_anno$sample_title &lt;- geo_ids[\n  match(sample_anno$sample_name, geo_ids$sample_name), \"sample_id\"]\nhead(sample_anno)\n\n\n           sample_name animal_id genotype  sex batch sample_title\nSRX9142648  GSM4793335       LA1       WT male  Day1    DRN-18429\nSRX9142649  GSM4793336       LA1       WT male  Day1    DRN-18430\nSRX9142650  GSM4793337       LA6       WT male  Day1    DRN-18439\nSRX9142651  GSM4793338       LA6       WT male  Day1    DRN-18440\nSRX9142652  GSM4793339       LA9       WT male  Day2    DRN-18445\nSRX9142653  GSM4793340       LA9       WT male  Day2    DRN-18446\n\n\nThis experiment includes 36 samples of microglia cells obtained from 18 different 8-month old mice. Both male and female animals were included in the study.\nThe animals carry one of three different genotypes of the gene encoding the APP amyloid beta precursor protein, either\n\nthe wildtype mouse gene (WT) or\none copy (Het) or\ntwo copies (Hom)\n\nof a mutant APP gene carrying mutations associated with familial Alzheimer’s Disease.\nSamples from all three genotypes were collected on three days, and we will use this batch information to model the experiment.\nTwo separate microglia samples were obtained from each animal, and we will include this nested relationship by modeling the animal as random effect in our linear model."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#xia-et-als-original-count-data",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#xia-et-als-original-count-data",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Xia et al’s original count data",
    "text": "Xia et al’s original count data\nFirst, we retrieve the authors’ count matrix from NCBI GEO, available as a Supplementary Excel file.\n\nurl &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE158152&\",\n              \"format=file&file=GSE158152%5Fdst150%5Fprocessed%2Exlsx\")\ntemp_file &lt;- tempfile(fileext = \".xlsx\")\ndownload.file(url, destfile = temp_file)\n\nThe Excel file has three different worksheets\n\nsample_annotations\nraw_counts\nnormalized_cpm\n\n\nraw_counts &lt;- read_excel(temp_file, sheet = \"raw_counts\")\nhead(colnames(raw_counts), 10)\n\n [1] \"feature_id\" \"name\"       \"meta\"       \"source\"     \"symbol\"    \n [6] \"DRN-18429\"  \"DRN-18430\"  \"DRN-18439\"  \"DRN-18440\"  \"DRN-18445\" \n\n\nThe raw_counts excel sheet contains information about the detected genes ( feature_ID, name) and the samples are identified by their GEO title (e.g. DRN-18459, DRN-184560). We use the raw counts to populate a new DGEList object and perform Library Size Normalization with the TMM approach.\n\n\nCode\ncount_data &lt;- as.matrix(raw_counts[, grep(\"DRN-\", colnames(raw_counts))])\nrow.names(count_data) &lt;- raw_counts$feature_id\ncolnames(count_data) &lt;- row.names(sample_anno)[\n  match(colnames(count_data), sample_anno$sample_title)\n]\n\ngene_data &lt;- data.frame(\n  gene_id = raw_counts$feature_id,\n  gene_name = raw_counts$symbol,\n  row.names = raw_counts$feature_id\n)\n\ncol_data &lt;- data.frame(\n  sample_anno[colnames(count_data),\n              c(\"sample_title\", \"animal_id\", \"sex\", \"genotype\", \"batch\")],\n  workflow = \"geo\"\n)\n\ndge &lt;- DGEList(\n  counts = as.matrix(count_data), \n  samples = col_data[colnames(count_data), ], \n  genes = gene_data[row.names(count_data), ]\n)\n\ndge &lt;- calcNormFactors(dge, method = \"TMM\")\n\n\nNext, we project the samples into two dimensions by performing multi-dimensional scaling of the top 500 most variable genes. The samples cluster by genotype, with WT and Het segregating from the Hom samples.\n\nplotMDS(dge, labels = dge$samples$genotype,\n        main = \"Multi-dimensional scaling\", \n        sub = \"Based on the top 500 most variable genes\")\n\n\n\n\nLet’s identify which genes are significantly differentially expressed between the three genotypes!\n\nLinear modeling with limma/voom\nFirst, we use the edgeR::filterByExpr() function to identify genes with sufficiently large counts to be examined for differential expression. (The min.count = 25 parameter was determined by examining the mean-variance plot by the voomLmFit() function.)\n\ndesign &lt;- model.matrix(~ genotype + sex + batch, data = dge$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge, design = design, min.count = 25)\n\nNext, we fit a linear model to the data using the limma/voom approach. The model includes the following fixed effects:\n\nThe genotype coded as a factor with the WT as the reference level.\nThe sex and batch covariates, to account for systematic differences in mean gene expression.\n\nBecause the dataset included two replicate samples from each animal, we model the animal as a random effect (via the block argument of the voomLmFit() function). We then extract the coefficients, log2 fold changes and p-values via limma’s empirical Bayes approach.\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use the limma::treat() function to test the null hypothesis that genes display significant differential expression greater than 1.2-fold. This is more stringent than the conventional null hypothesis of zero change. (Please consult the limma::treat() help page for details.)\n\n\n\n\nfit &lt;- voomLmFit(\n  dge[keep, row.names(design)], \n  design = design,\n  block = dge$samples$animal_id, \n  sample.weights = TRUE, \n  plot = FALSE\n)\nfit &lt;- treat(fit, robust=TRUE)\n\nThe following table displays the number of differentially up- and down-regulated genes after applying a false-discovery (adj.P.Val) threshold of 5%. While we did not detect significant differences between Het and WT animals, the analysis revealed &gt; 450 differentially expressed genes between Hom and WT microglia.\n\nsummary(decideTests(fit))[, c(\"Het\", \"Hom\")]\n\n         Het   Hom\nDown       0    73\nNotSig 10131  9660\nUp         0   398\n\n\nThe top 10 genes with the smallest p-values include well known markers of microglia activation:\n\ntopTreat(fit, coef = \"Hom\")[, c(\"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")]\n\n                   gene_name     logFC      P.Value    adj.P.Val\nENSMUSG00000027523      Gnas 1.5904760 3.467080e-24 3.512499e-20\nENSMUSG00000022265       Ank 3.7866023 1.258687e-20 6.375881e-17\nENSMUSG00000021477      Ctsl 1.2829086 2.294556e-20 7.748717e-17\nENSMUSG00000018927      Ccl6 2.7405831 3.514945e-20 8.902477e-17\nENSMUSG00000030579    Tyrobp 1.2880854 9.748473e-19 1.975236e-15\nENSMUSG00000022415    Syngr1 1.5033949 1.367158e-18 2.308446e-15\nENSMUSG00000016256      Ctsz 1.1557870 2.878895e-18 4.166584e-15\nENSMUSG00000023992     Trem2 0.9460719 7.386805e-18 9.354465e-15\nENSMUSG00000056737      Capg 2.4537482 1.927765e-17 2.170021e-14\nENSMUSG00000030342       Cd9 1.0596747 1.328189e-16 1.328093e-13\n\n\nNext we repeat the same analysis with the output of the nf-core/rnaseq workflow."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#nf-corernaseq-results",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#nf-corernaseq-results",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "nf-core/rnaseq results",
    "text": "nf-core/rnaseq results\nWe start with the raw counts contained in the salmon.merged.gene_counts.rds file generated by the nf-core/rnaseq workflow.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe nf-core pipeline returned the versioned ENSEMBL gene identifiers (e.g.) ENSMUSG00000000001.4. Because Xia et al only provided the unversioned identifiers (e.g. ENSMUSG00000000001) we trim the numeric suffix.\n\n\n\nWe TMM-normalize the data, as before. (This step converts the SummarizedExperiment into a DGEList object as well.)\n\ncount_file &lt;- file.path(work_dir, \"salmon.merged.gene_counts.rds\")\nse &lt;- readRDS(count_file)\nrow.names(se) &lt;- sapply(\n  strsplit(row.names(se), split = \".\", fixed = TRUE), \"[[\", 1)\nstopifnot(all(colnames(se) %in% row.names(sample_anno)))\ndge_nfcore &lt;- calcNormFactors(se, method = \"TMM\")\n\nNext, we add the sample metadata and fit the same linear model as before.\n\n\nCode\ndge_nfcore$genes$gene_id &lt;- row.names(dge_nfcore)\n\ndge_nfcore$samples &lt;- data.frame(\n  dge_nfcore$samples,\n  sample_anno[colnames(dge_nfcore),\n              c(\"sample_title\", \"animal_id\", \"sex\", \"genotype\", \"batch\")],\n  workflow = \"nfcore\"\n)\nstopifnot(all(colnames(dge) %in% colnames(dge_nfcore)))\ndge_nfcore &lt;- dge_nfcore[, colnames(dge)]\n\ndesign &lt;- model.matrix(~ genotype + sex + batch, data = dge_nfcore$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge_nfcore, design = design, min.count = 25)\nfit_nfcore &lt;- voomLmFit(\n  dge_nfcore[keep, row.names(design)], \n  design = design,\n  block = dge_nfcore$samples$animal_id, \n  sample.weights = TRUE, \n  plot = FALSE\n)\n\n\nFirst sample weights (min/max) 0.5573482/2.1680260\n\n\nFirst intra-block correlation  0.02339845\n\n\nFinal sample weights (min/max) 0.5367708/2.2366196\n\n\nFinal intra-block correlation  0.02354808\n\n\nCode\nfit_nfcore &lt;- treat(fit_nfcore, robust=TRUE)\n\n\nAs with the original count data from NCBI GEO, we detect &gt; 450 differentially expressed genes between Hom and WT genotypes (FDR &lt; 5%, null hypothesis: fold change &gt; 1.2).\n\nsummary(decideTests(fit_nfcore))[, c(\"Het\", \"Hom\")]\n\n         Het   Hom\nDown       0    76\nNotSig 10428  9917\nUp         0   435"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#comparing-results-across-preprocessing-workflows",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#comparing-results-across-preprocessing-workflows",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Comparing results across preprocessing workflows",
    "text": "Comparing results across preprocessing workflows\nNext, we compare the results obtained with the two datasets. We create the cpms and tt dataframes, holding the combined absolute and differential expression results, respectively.\n\n\nCode\ncpms &lt;- local({\n  geo &lt;- cpm(dge, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"Xia et al\")\n  \n  nfcore &lt;- cpm(dge_nfcore, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge_nfcore$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge_nfcore$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"nf-core\")\n  \n  dplyr::bind_rows(\n    dplyr::select(geo, any_of(intersect(colnames(geo), colnames(nfcore)))),\n    dplyr::select(nfcore, any_of(intersect(colnames(geo), colnames(nfcore))))\n  )\n})\n\ntt &lt;- rbind(\n  topTreat(fit, coef = \"Hom\", number = Inf)[\n    , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"geo\"),\n   topTreat(fit_nfcore, coef = \"Hom\", number = Inf)[\n     , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"nfcore\")\n) %&gt;%\n  dplyr::mutate(adj.P.Val = signif(adj.P.Val, 2)) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"gene_name\"), \n    names_from = \"dataset\", \n    values_from = \"adj.P.Val\") %&gt;%\n  dplyr::arrange(nfcore) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"gene_id\")\n\n\n\nNormalized expression\nFirst, we examine the correlation between the normalized log-transformed gene expression estimates returned from the two workflows. We focus on those genes that passed the filterByExpr thresholds above, e.g. those genes deemed sufficiently highly expressed to be assessed for differential expression.\n\n\nCode\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nsum_stats &lt;- cpms %&gt;%\n  dplyr::filter(gene_id %in% common_genes) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"sample_name\"),\n    values_from = \"cpm\",\n    names_from = \"dataset\") %&gt;%\n  dplyr::group_by(gene_id) %&gt;%\n  dplyr::summarise(\n    r = cor(log1p(`Xia et al`), log1p(`nf-core`)),\n    mean_xia = mean(`Xia et al`),\n    mean_nfcore = mean(`nf-core`))\n\np &lt;- ggplot(data = sum_stats, aes(x = r)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(limits = c(0, 1.02), breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"Pearson correlation coefficient (R)\", \n       y = \"Number of genes\",\n       title = \"Correlation between normalized log2 counts\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\n\nThe correlation between normalized log2 expression estimates is very high, with 95% of all genes showing a Pearson correlation coefficient &gt; 0.94.\nMost of the 10022 examined genes were detected with &gt; 10 normalized counts per million reads.\n\np &lt;- ggplot(data = sum_stats, aes(x = mean_xia + 1)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(trans = scales::log10_trans(),\n                     labels = scales::comma_format()) +\n  labs(x = \"Mean normalized counts per million\", \n       y = \"Number of genes\",\n       title = \"Average expression\",\n       subtitle = \"Xia et al\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\nNext, we will examine the results of the differential expression analysis.\n\n\nDifferential expression results\nAnalyses based on either preprocessing pipeline yield similar numbers of differentially expressed genes.\n\n\nCode\nresults &lt;- cbind(\n  decideTests(fit)[common_genes, \"Hom\"], \n  decideTests(fit_nfcore)[common_genes, \"Hom\"]\n)\ncolnames(results) &lt;- c(\"Xia et al\", \"nf-core\")\nclass(results) &lt;- \"TestResults\"\nsummary(results)\n\n\n   Xia et al nf-core\n-1        73      75\n0       9554    9528\n1        395     419\n\n\nBut are these the same genes in both sets of results?\nWe can visualize the overlap between the sets of significant genes in a Venn diagram (FDR &lt; 5%). The vast majority of differentially expressed genes is detected with both quantitation approaches (for both up- and down-regulated genes.)\n\nlimma::vennDiagram(results, include = c(\"up\", \"down\"),\n                   counts.col=c(\"red\", \"blue\"), mar = rep(0,4))\n\n\n\n\nFor example, the following plots show the normalized expression of a few highly differentially expressed genes (known markers of active microglia).\n\n\nCode\nfor (gene in topTreat(fit, coef = \"Hom\", number = 6)[[\"gene_id\"]]) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a hard FDR threshold can inflate the number of apparent differences, e.g. when a gene is close to the significance threshold (see below).\n\np_cor &lt;- cor(\n  fit$coefficients[common_genes, \"Hom\"], \n  fit_nfcore$coefficients[common_genes, \"Hom\"])\n\nThe log2 fold estimates for the Hom vs WT comparison are highly correlated across the two analysis workflows (Pearson correlation coefficient R = 0.99 ):\n\nsmoothScatter(\n  fit$coefficients[common_genes, \"Hom\"], \n  fit_nfcore$coefficients[common_genes, \"Hom\"],\n  ylab = \"nf-core (log2FC)\",\n  xlab = \"Xia et al (log2FC)\",\n  main = \"Homozygous APP vs WT (effect size)\"\n)\ntext(x = 10, y = -2, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\nas are the t-statistics across all examined genes:\n\np_cor &lt;- cor(\n  fit$t[common_genes, \"Hom\"], \n  fit_nfcore$t[common_genes, \"Hom\"])\nsmoothScatter(\n  fit$t[common_genes, \"Hom\"], \n  fit_nfcore$t[common_genes, \"Hom\"],\n  ylab = \"nf-core (t-statistic)\",\n  xlab = \"Xia et al (t-statistic)\",\n  main = \"Homozygous APP vs WT (t-statistic)\")\ntext(x = 10, y = -2, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\n\nDiscordant significance calls\n\n# genes detected in GEO, but not significant with nf-core\ngenes &lt;- row.names(results)[which(abs(results[, 1]) == 1 & results[, 2] == 0)]\n\nAt FDR &lt; 5% 14 genes were reported as significantly differentially expressed with the original Xia et al count matrix but not with the output of the nf-core/rnaseq workflow.\nAs side-by-side comparison of the FDR (adj.P.Val) for these genes confirms that the vast majority display significant close to the 5% threshold in the nf-core/rnaseq output as well. (This is in line with the high overall correlation of the t-statistics observed above.)\n\nprint(tt[genes, ])\n\n                       gene_name    geo nfcore\nENSMUSG00000078193 RP24-228M19.1 0.0041  0.120\nENSMUSG00000027427        Polr3f 0.0490  0.052\nENSMUSG00000028394         Pole3 0.0480  0.073\nENSMUSG00000029027          Dffb 0.0360  0.082\nENSMUSG00000029649          Pomp 0.0430  0.056\nENSMUSG00000054404         Slfn5 0.0440  0.064\nENSMUSG00000050965         Prkca 0.0310  0.087\nENSMUSG00000020641         Rsad2 0.0440  0.063\nENSMUSG00000021057         Akap5 0.0260  0.074\nENSMUSG00000115230 RP24-123O20.1 0.0400  0.061\nENSMUSG00000042622          Maff 0.0500  0.051\nENSMUSG00000050410         Tcf19 0.0500  0.051\nENSMUSG00000059040         Eno1b 0.0290  0.410\nENSMUSG00000042712        Tceal9 0.0480  0.054\n\n\nFinally, we plot the normalized gene expression estimates for the 14 discordant genes.\n\n\nCode\nfor (gene in genes) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#conclusions",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#conclusions",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Conclusions",
    "text": "Conclusions\n\nDifferential expression analyses of raw counts obtained with the nc-core/rnaseq workflow yields results that are highly concordant with those obtained with the raw counts the authors deposited in NCBI GEO.\nWith appropriate parameters the nf-core/rnaseq workflow can be applied to QuantSeq FWD 3’tag RNA-seq data."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#reproducibility",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#reproducibility",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date (UTC) lib source\n AnnotationDbi        * 1.60.0   2022-11-01 [1] Bioconductor\n askpass                1.1      2019-01-13 [1] CRAN (R 4.2.0)\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n Biobase              * 2.58.0   2022-11-01 [1] Bioconductor\n BiocGenerics         * 0.44.0   2022-11-01 [1] Bioconductor\n Biostrings             2.66.0   2022-11-01 [1] Bioconductor\n bit                    4.0.5    2022-11-15 [1] CRAN (R 4.2.0)\n bit64                  4.0.5    2020-08-30 [1] CRAN (R 4.2.0)\n bitops                 1.0-7    2021-04-24 [1] CRAN (R 4.2.0)\n blob                   1.2.3    2022-04-10 [1] CRAN (R 4.2.0)\n cachem                 1.0.6    2021-08-19 [1] CRAN (R 4.2.0)\n cellranger             1.1.0    2016-07-27 [1] CRAN (R 4.2.0)\n cli                    3.5.0    2022-12-20 [1] CRAN (R 4.2.0)\n colorspace             2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n crayon                 1.5.2    2022-09-29 [1] CRAN (R 4.2.0)\n credentials            1.3.2    2021-11-29 [1] CRAN (R 4.2.0)\n DBI                    1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n DelayedArray           0.24.0   2022-11-01 [1] Bioconductor\n digest                 0.6.31   2022-12-11 [1] CRAN (R 4.2.0)\n dplyr                * 1.0.10   2022-09-01 [1] CRAN (R 4.2.0)\n edgeR                * 3.40.1   2022-12-14 [1] Bioconductor\n ellipsis               0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate               0.19     2022-12-13 [1] CRAN (R 4.2.0)\n fansi                  1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n farver                 2.1.1    2022-07-06 [1] CRAN (R 4.2.0)\n fastmap                1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n generics               0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb         * 1.34.4   2022-12-01 [1] Bioconductor\n GenomeInfoDbData       1.2.9    2022-12-12 [1] Bioconductor\n GenomicRanges        * 1.50.2   2022-12-16 [1] Bioconductor\n ggplot2              * 3.4.0    2022-11-04 [1] CRAN (R 4.2.0)\n glue                   1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n gtable                 0.3.1    2022-09-01 [1] CRAN (R 4.2.0)\n here                 * 1.0.1    2020-12-13 [1] CRAN (R 4.2.0)\n htmltools              0.5.4    2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets            1.5.4    2021-09-08 [1] CRAN (R 4.2.2)\n httr                   1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n IRanges              * 2.32.0   2022-11-01 [1] Bioconductor\n jsonlite               1.8.4    2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST               1.38.0   2022-11-01 [1] Bioconductor\n KernSmooth             2.23-20  2021-05-03 [2] CRAN (R 4.2.2)\n knitr                  1.41     2022-11-18 [1] CRAN (R 4.2.0)\n labeling               0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n lattice                0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lifecycle              1.0.3    2022-10-07 [1] CRAN (R 4.2.0)\n limma                * 3.54.0   2022-11-01 [1] Bioconductor\n locfit                 1.5-9.6  2022-07-11 [1] CRAN (R 4.2.0)\n magrittr               2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n Matrix                 1.5-3    2022-11-11 [1] CRAN (R 4.2.0)\n MatrixGenerics       * 1.10.0   2022-11-01 [1] Bioconductor\n matrixStats          * 0.63.0   2022-11-18 [1] CRAN (R 4.2.0)\n memoise                2.0.1    2021-11-26 [1] CRAN (R 4.2.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.2.0)\n openssl                2.0.5    2022-12-06 [1] CRAN (R 4.2.0)\n org.Mm.eg.db         * 3.16.0   2022-12-29 [1] Bioconductor\n pillar                 1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n png                    0.1-8    2022-11-29 [1] CRAN (R 4.2.0)\n purrr                  1.0.0    2022-12-20 [1] CRAN (R 4.2.0)\n R6                     2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp                   1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n RCurl                  1.98-1.9 2022-10-03 [1] CRAN (R 4.2.0)\n readxl               * 1.4.1    2022-08-17 [1] CRAN (R 4.2.0)\n rlang                  1.0.6    2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown              2.19     2022-12-15 [1] CRAN (R 4.2.0)\n rprojroot              2.0.3    2022-04-02 [1] CRAN (R 4.2.0)\n RSQLite                2.2.19   2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi             0.14     2022-08-22 [1] CRAN (R 4.2.0)\n S4Vectors            * 0.36.1   2022-12-05 [1] Bioconductor\n scales                 1.2.1    2022-08-20 [1] CRAN (R 4.2.0)\n sessioninfo            1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n statmod                1.4.37   2022-08-12 [1] CRAN (R 4.2.0)\n stringi                1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr                1.5.0    2022-12-02 [1] CRAN (R 4.2.0)\n SummarizedExperiment * 1.28.0   2022-11-01 [1] Bioconductor\n sys                    3.4.1    2022-10-18 [1] CRAN (R 4.2.0)\n tibble               * 3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr                * 1.2.1    2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect             1.2.0    2022-10-10 [1] CRAN (R 4.2.0)\n utf8                   1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n vctrs                  0.5.1    2022-11-16 [1] CRAN (R 4.2.0)\n withr                  2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun                   0.35     2022-11-16 [1] CRAN (R 4.2.0)\n XVector                0.38.0   2022-11-01 [1] Bioconductor\n yaml                   2.3.6    2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc               1.44.0   2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFull disclosure: I am a co-author of this publication.↩︎"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html",
    "href": "posts/geneset-sqlite-db/index.html",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "",
    "text": "The first step of any data analysis is to obtain and explore the available data, often by accessing and querying a database. There many great introductions on how to read data into an R session. But I found it harder to find tutorials on how to create and populate a new database from scratch.\nIn this document, I explore both noSQL and SQL approaches to data management. As an example use case, we store a collection of gene sets, specifically the mouse MSigDb hallmark gene sets (MH), either as unstructured documents or in relational tables.\n\n\nBioconductor offers well designed S4 Classes to store gene set collections, including e.g. in a list-like GSEABase::GeneSetCollection or a set of three tibbles within a BiocSet::BiocSet object. So why could we be interested in storing this information in a database?\n\nA database (e.g. SQLite, Postgres, etc) offers a standardized way to store, manage and access information in a language-agnostic way. E.g. some of my colleagues use python for their analyses and are comfortable retrieving gene set information from a database, but not necessarily from an R S4 object.\nGene sets capture knowledge from multiple experiments, studies and sources. If you are part of a larger organization a single source of truth, available in a central location, is very useful.\nCollaborators might not be interested / able to access information programmatically, e.g. they may prefer a web application to search, share and edit gene sets. Many tools to build web applications have built-in capabilities to interact with a database.\nAs the number of gene sets grows, sharing them in the form of one or more files might become cumbersome. A hosted database (e.g.  Postgres or MariaDB ) allows users to retrieve only the information they need.\n\nIn this tutorial, I am using the SQLite engine to explore both relational and non-relational ways to manage gene sets. SQLite can be embedded into applications, and does not require a central server, making it ideal for experimentation. (But as you move into a scenario where multiple users need to access a central, it is time to switch to a hosted database instead; my favorite is Postgres.)\n\nlibrary(BiocSet)\nlibrary(dm)\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(nodbi)\nlibrary(org.Mm.eg.db)\nlibrary(purrr)\nlibrary(RSQLite)\nlibrary(tibble)\nlibrary(tidyr)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#creating-polulating-and-querying-sql-and-nosql-databases-with-r",
    "href": "posts/geneset-sqlite-db/index.html#creating-polulating-and-querying-sql-and-nosql-databases-with-r",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "",
    "text": "The first step of any data analysis is to obtain and explore the available data, often by accessing and querying a database. There many great introductions on how to read data into an R session. But I found it harder to find tutorials on how to create and populate a new database from scratch.\nIn this document, I explore both noSQL and SQL approaches to data management. As an example use case, we store a collection of gene sets, specifically the mouse MSigDb hallmark gene sets (MH), either as unstructured documents or in relational tables.\n\n\nBioconductor offers well designed S4 Classes to store gene set collections, including e.g. in a list-like GSEABase::GeneSetCollection or a set of three tibbles within a BiocSet::BiocSet object. So why could we be interested in storing this information in a database?\n\nA database (e.g. SQLite, Postgres, etc) offers a standardized way to store, manage and access information in a language-agnostic way. E.g. some of my colleagues use python for their analyses and are comfortable retrieving gene set information from a database, but not necessarily from an R S4 object.\nGene sets capture knowledge from multiple experiments, studies and sources. If you are part of a larger organization a single source of truth, available in a central location, is very useful.\nCollaborators might not be interested / able to access information programmatically, e.g. they may prefer a web application to search, share and edit gene sets. Many tools to build web applications have built-in capabilities to interact with a database.\nAs the number of gene sets grows, sharing them in the form of one or more files might become cumbersome. A hosted database (e.g.  Postgres or MariaDB ) allows users to retrieve only the information they need.\n\nIn this tutorial, I am using the SQLite engine to explore both relational and non-relational ways to manage gene sets. SQLite can be embedded into applications, and does not require a central server, making it ideal for experimentation. (But as you move into a scenario where multiple users need to access a central, it is time to switch to a hosted database instead; my favorite is Postgres.)\n\nlibrary(BiocSet)\nlibrary(dm)\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(nodbi)\nlibrary(org.Mm.eg.db)\nlibrary(purrr)\nlibrary(RSQLite)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#the-mouse-hallmarks-msigdb-collection",
    "href": "posts/geneset-sqlite-db/index.html#the-mouse-hallmarks-msigdb-collection",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "The Mouse Hallmarks MSigDB collection",
    "text": "The Mouse Hallmarks MSigDB collection\nAt the time of writing, Mouse Molecular Signatures Database (MSigDB) contains 15918 gene sets, organized into numerous different collections. For example, the 50 hallmark gene sets (MH) summarize and represent specific well-defined biological states or processes ( Liberzon et al, Cell Systems, 2015 ).\n\n\n\n\n\n\nGene symbols\n\n\n\nEach of the 50 sets in the collection contains between 32 and 200 official gene symbols, specifying the members of the gene set.\n\n\nHere, I will use the hallmarks collection as an example but the overall approach can be applied to other gene set collection in a similar way. (You might need additional / different annotation fields, though.)\nThe mouse hallmarks collection is available in different formats, including as a JSON file. Let’s start by reading it into an R session as nested list mh.\n\njson_file &lt;- paste0(\n  \"https://raw.githubusercontent.com/tomsing1/blog/main/posts/\",\n  \"geneset-sqlite-db/mh.all.v2022.1.Mm.json\")\nmh &lt;- jsonlite::read_json(json_file, simplifyVector = TRUE)\n\nEach of the 50 elements in the JSON file corresponds to a different gene set,\n\nhead(names(mh))\n\n[1] \"HALLMARK_ADIPOGENESIS\"        \"HALLMARK_ALLOGRAFT_REJECTION\"\n[3] \"HALLMARK_ANDROGEN_RESPONSE\"   \"HALLMARK_ANGIOGENESIS\"       \n[5] \"HALLMARK_APICAL_JUNCTION\"     \"HALLMARK_APICAL_SURFACE\"     \n\n\neach gene set is a nested list with the following elements,\n\nlengths(mh[[1]])\n\n              systematicName                         pmid \n                           1                            1 \n                 exactSource                  geneSymbols \n                           1                          200 \n                   msigdbURL           externalDetailsURL \n                           1                            1 \n        filteredBySimilarity externalNamesForSimilarTerms \n                           0                            0 \n                  collection \n                           1 \n\n\nand the gene symbols that make up the set are listed in the geneSymbols vector:\n\nhead(mh[[1]]$geneSymbols)\n\n[1] \"Abca1\" \"Abcb8\" \"Acaa2\" \"Acadl\" \"Acadm\" \"Acads\""
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#nosql-storing-gene-sets-as-unstructured-documents",
    "href": "posts/geneset-sqlite-db/index.html#nosql-storing-gene-sets-as-unstructured-documents",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "noSQL: storing gene sets as unstructured documents",
    "text": "noSQL: storing gene sets as unstructured documents\nEach gene set is represented as a list - so why not store it in the same way? A noSQL database is designed to store unstructed information, e.g. data models that are not organized in tables, making them flexible and scalable. Examples of noSQL databases include e.g. Mongodb, CouchDB or AWS dynamodb.\nIn addition, traditional relational database engines - including SQLite and Postgres - can also store unstructured data in dedicated JSON fields.\nThe nodbi R package provides a unified interface to multiple noSQL implementations, including SQLite. (If you are interested in a deeper look at how to create & query a JSON field in SQLite with raw SQL, check out this gist ).\n\nCreating & populating a noSQL database with the nodbi R package\nTo experiment with its noSQL mode, we create a temporary SQLite database in memory. (For real data, you definitely want to provide a file path as the dbname instead!)\n\nsrc &lt;- nodbi::src_sqlite(dbname = \":memory:\")\n\nRight now, the names of the gene sets are only stored as the names() of the list elements, e.g. not in a field within each sub-list itself. To make sure they are included in each database record, we add them to each sub-list in a new name field.\n\nmh2 &lt;- lapply(names(mh), \\(gs) c(\"name\" = gs, mh[[gs]]))\n\n\n\n\n\n\n\nUnique identifiers\n\n\n\nThe docdb_create() function accepts either a data.frame, a JSON string or a list as its value argument.\nIf you include a field _id in your list, it will be used as the primary key for each element. If no _id field is found, then the _id field is created automatically with a call to the uuid::UUIDgenerate() function.\nIf you provide a data.frames() with row.names, they will be used to populate the _id field.\n\n\nNow we are ready to create a new SQLite table hallmarks and populate it with the 50 gene sets.\n\ndocdb_create(src, key = \"hallmarks\", value = mh2)\n\n[1] 50\n\n\nWe can retrieve the full set of records as a data.frame with the docdb_get() function. (Here we select a subset of the returned columns due to space constraints.) Because each gene set contains multiple geneSymbols, this field is a list-column.\n\ndocdb_get(src, \"hallmarks\")[1:4, c(\"name\", \"geneSymbols\", \"pmid\")]\n\n                          name  geneSymbols     pmid\n1        HALLMARK_ADIPOGENESIS Abca1, A.... 30224793\n2 HALLMARK_ALLOGRAFT_REJECTION Aars, Ab.... 30224793\n3   HALLMARK_ANDROGEN_RESPONSE Abcc4, A.... 30224793\n4        HALLMARK_ANGIOGENESIS Apoh, Ap.... 30224793\n\n\n\n\nQuerying with JSON filters\nMore commonly, users might want to retrieve one or more gene sets by name. The docdb_query() function accepts a query argument specifying the desired filter criteria (as MongoDB JSON ).\n\nresults &lt;- nodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = '{\"name\": \"HALLMARK_ADIPOGENESIS\"}')\nresults[, c(\"name\", \"geneSymbols\", \"pmid\")]\n\n                   name  geneSymbols     pmid\n1 HALLMARK_ADIPOGENESIS Abca1, A.... 30224793\n\n\nThe fields argument allows us to return only specific columns. (Specifying a field as 1 or 0 will include or exclude it, respectively.)\n\nnodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = '{\"name\": \"HALLMARK_ADIPOGENESIS\"}',\n  fields = '{\"name\": 1, \"geneSymbols\": 1}'\n)\n\n                   name  geneSymbols\n1 HALLMARK_ADIPOGENESIS Abca1, A....\n\n\nWe can also identify gene sets containing at least one of the given gene symbols:\n\nresults &lt;- nodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = paste0('{\"$or\":[',\n                 '{\"geneSymbols\": \"Abca1\"},', \n                 '{\"geneSymbols\": \"Gapdh\"}',\n                 ']}'),\n  fields = '{\"name\": 1, \"geneSymbols\": 1}'\n)\n\n\n\n\n\n\n\nUnnesting columns\n\n\n\n\n\nBecause the set contains more than one geneSymbol, we obtain a nested data.frame. We can unnest it e.g. with the tidyr R package\n\ntidyr::unnest(results, cols = c(geneSymbols))\n\n# A tibble: 798 × 2\n   name                  geneSymbols\n   &lt;chr&gt;                 &lt;chr&gt;      \n 1 HALLMARK_ADIPOGENESIS Abca1      \n 2 HALLMARK_ADIPOGENESIS Abcb8      \n 3 HALLMARK_ADIPOGENESIS Acaa2      \n 4 HALLMARK_ADIPOGENESIS Acadl      \n 5 HALLMARK_ADIPOGENESIS Acadm      \n 6 HALLMARK_ADIPOGENESIS Acads      \n 7 HALLMARK_ADIPOGENESIS Acly       \n 8 HALLMARK_ADIPOGENESIS Aco2       \n 9 HALLMARK_ADIPOGENESIS Acox1      \n10 HALLMARK_ADIPOGENESIS Adcy6      \n# … with 788 more rows\n\n\n\n\n\n\n\nQuerying using SQL\nFormulating the queries as JSON strings is tedious, though. Alternatively, SQLite also supports querying JSON columns using SQL (muddying the border between noSQL and SQL). For example, we can use SQLite’s -&gt; and -&gt;&gt; operators and the json_each() SQL function to create a query that returns the names of all gene sets that include e.g. the Abca1 gene:\n\nSELECT hallmarks.json-&gt;&gt;'name' as name\nFROM hallmarks, json_each(hallmarks.json, '$.geneSymbols')\nWHERE json_each.value LIKE '%Abca1%'\n\n\n5 records\n\n\nname\n\n\n\n\nHALLMARK_ADIPOGENESIS\n\n\nHALLMARK_BILE_ACID_METABOLISM\n\n\nHALLMARK_INFLAMMATORY_RESPONSE\n\n\nHALLMARK_PROTEIN_SECRETION\n\n\nHALLMARK_TNFA_SIGNALING_VIA_NFKB\n\n\n\n\n\nDepending on comfortable you are reading / writing SQL, this might be a nicer approach.\n\n\n\n\n\n\nLimitations\n\n\n\nSQLite’s JSON operators are somewhat limited, e.g. there is no straightforward way to ask whether a column contains one or more gene identifiers (e.g. the query we performed above using a query JSON string). Indexing a SQLite JSON column also comes with limitations.\nThe Postgres database engine supports JSON and binary JSONB fields) with indexing & additional operators like the @&gt; contains operator.\n\n\n\n\nnoSQL summary\nThis example highlights some of the advantages of a noSQL solution:\n\nRapid ingestion of data without the need for a rigid schema.\nSimple retrieval of individual object identified by their primary key.\n\nBut also some of the disadvantages:\n\nQueries that descend into the (potentially nested) objects must be carefully constructed.\nIncreasing database performance with indices is more complicated than for relational databases (see below.)\n\nNext, we will try another approach: reshaping the gene set collection into a set of tables and modeling the relationship between them.s"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#sql-storing-gene-sets-in-a-relational-database",
    "href": "posts/geneset-sqlite-db/index.html#sql-storing-gene-sets-in-a-relational-database",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "SQL: storing gene sets in a relational database",
    "text": "SQL: storing gene sets in a relational database\nR has excellent support for interacting with relational database, e.g. via the foundational ‘Common Database Interface’ (DBI) package and the numerous database-specific packages built on top of it, including the RSQLite, RPostgres and many others.\nTo take advantage of a relational database we have perform a little more work up-front. But this effort is amply repaid by simplifying subsequent queries.\n\nLearning from Bioconductor: BiocSet’s three tables\nThe BiocSet Class from the eponymous Bioconductor package represents a collection of gene sets in three tibbles. Let’s create a simple BiocSet with two gene sets for illustration:\n\nset_names &lt;- purrr::map_chr(mh2[1:2], \"name\")\ngene_ids &lt;- purrr::map(mh2[1:2], \"geneSymbols\")\nes &lt;- BiocSet(setNames(gene_ids, set_names))\n\nThe first two tibbles represent genes (called elements) and sets, respectively:\n\nes_element: one row per gene\n\n\nhead(es_element(es))\n\n# A tibble: 6 × 1\n  element\n  &lt;chr&gt;  \n1 Abca1  \n2 Abcb8  \n3 Acaa2  \n4 Acadl  \n5 Acadm  \n6 Acads  \n\n\n\nes_set: one row per gene set\n\n\nes_set(es)\n\n# A tibble: 2 × 1\n  set                         \n  &lt;chr&gt;                       \n1 HALLMARK_ADIPOGENESIS       \n2 HALLMARK_ALLOGRAFT_REJECTION\n\n\nThe third table establishes the many-to-many relationship between genes and sets, e.g. it tracks which gene is a member of each set.\n\nes_elementset: gene x set combination\n\n\n# we are showing 10 random rows\nset.seed(42)\nes_elementset(es)[sample(nrow(es_elementset(es)), size = 10), ]\n\n# A tibble: 10 × 2\n   element set                         \n   &lt;chr&gt;   &lt;chr&gt;                       \n 1 Coq5    HALLMARK_ADIPOGENESIS       \n 2 Irf7    HALLMARK_ALLOGRAFT_REJECTION\n 3 Qdpr    HALLMARK_ADIPOGENESIS       \n 4 Elovl6  HALLMARK_ADIPOGENESIS       \n 5 Cd28    HALLMARK_ALLOGRAFT_REJECTION\n 6 Ppm1b   HALLMARK_ADIPOGENESIS       \n 7 Mtarc2  HALLMARK_ADIPOGENESIS       \n 8 Zap70   HALLMARK_ALLOGRAFT_REJECTION\n 9 Ndufb7  HALLMARK_ADIPOGENESIS       \n10 Il15    HALLMARK_ALLOGRAFT_REJECTION\n\n\nEach of these tables can be augmented with additional metadata, e.g. we could add Entrez gene identifiers to the es_element (see below), or long-form descriptions for each set to the es_set tibble.\nThese three tables can easily be represented in a relational database, using the element and set columns as primary keys.\n\n\nCreating and populating a relational database\nLet’s start with a fresh SQLite database.\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nFirst, we create the geneset data.frame that lists all gene sets, and we also include their MSigDb URLs as metadata:\n\ngeneset &lt;- data.frame(\n  geneset = purrr::map_chr(mh2, \"name\"),\n  url = purrr::map_chr(mh2, \"msigdbURL\"))\nhead(geneset)\n\n                       geneset\n1        HALLMARK_ADIPOGENESIS\n2 HALLMARK_ALLOGRAFT_REJECTION\n3   HALLMARK_ANDROGEN_RESPONSE\n4        HALLMARK_ANGIOGENESIS\n5     HALLMARK_APICAL_JUNCTION\n6      HALLMARK_APICAL_SURFACE\n                                                                                 url\n1        https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ADIPOGENESIS\n2 https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ALLOGRAFT_REJECTION\n3   https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ANDROGEN_RESPONSE\n4        https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ANGIOGENESIS\n5     https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_APICAL_JUNCTION\n6      https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_APICAL_SURFACE\n\n\nNext, we identify all unique gene symbols, annotate them with their Entrez ids (using the org.Mm.eg.db Bioconductor annotation package), and store both identifier types in the element data.frame.\n\ngene_symbols &lt;- unique(unlist(purrr::map(mh2, \"geneSymbols\")))\nelement &lt;- data.frame(\n  element = gene_symbols,\n  entrezid = mapIds(org.Mm.eg.db, keys = gene_symbols, keytype = \"SYMBOL\", \n                    column = \"ENTREZID\")\n  )\nhead(element)\n\n      element entrezid\nAbca1   Abca1    11303\nAbcb8   Abcb8    74610\nAcaa2   Acaa2    52538\nAcadl   Acadl    11363\nAcadm   Acadm    11364\nAcads   Acads    11409\n\n\nFinally, we create the element_set join table, connecting gene sets to their constituent genes:\n\nelementset &lt;- purrr::map_df(mh2, \\(gs) {\n  with(gs, \n       data.frame(\n         element = geneSymbols,\n         geneset = name\n       )\n  )\n})\nhead(elementset)\n\n  element               geneset\n1   Abca1 HALLMARK_ADIPOGENESIS\n2   Abcb8 HALLMARK_ADIPOGENESIS\n3   Acaa2 HALLMARK_ADIPOGENESIS\n4   Acadl HALLMARK_ADIPOGENESIS\n5   Acadm HALLMARK_ADIPOGENESIS\n6   Acads HALLMARK_ADIPOGENESIS\n\n\nNext, we write each data.frame into a separate table in our SQLite database.\n\n\n\n\n\n\nVerifying foreign keys\n\n\n\n\n\nBy default, SQLite does not verify that foreign keys actually exist in the referenced table. To make this a requirement, we can enable checking with the following command:\n\ndbExecute(con, 'PRAGMA foreign_keys = 1;')\n\n[1] 0\n\n\n\n\n\n\ndbExecute(con, \n          \"CREATE TABLE tbl_geneset (geneset TEXT PRIMARY KEY, url TEXT)\")\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_geneset\", value = geneset, overwrite = TRUE)\n\ndbExecute(con, \n          \"CREATE TABLE tbl_element (element TEXT PRIMARY KEY, entrezid TEXT)\")\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_element\", value = element, overwrite = TRUE)\n\ndbExecute(con, paste(\n  \"CREATE TABLE tbl_elementset (\",\n  \"element TEXT,\", \n  \"geneset TEXT,\",\n  \"FOREIGN KEY(geneset) REFERENCES tbl_geneset(geneset),\",\n  \"FOREIGN KEY(element) REFERENCES tbl_element(element)\",\n  \")\")\n  )\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_elementset\", value = elementset, overwrite = TRUE)\n\n\ndbListTables(con)\n\n[1] \"tbl_element\"    \"tbl_elementset\" \"tbl_geneset\"   \n\n\n\n\nPlotting relationships\nAs we create and need to keep track of multiple tables, it is useful to visualize their contents (fields, columns) and relationships in a model diagram. The awesome dm R package, designed to bring an existing relational data model into your R session, can be used to generate diagrams like the one shown below. (dm can identify the keys in postgres and SQL server database engines automatically, but for SQLite we need to specify them ourselves with the dm_add_pk() and dm_add_fk() functions.)\n\ndm_from_con(con, learn_keys = FALSE) %&gt;%\n  dm_add_pk(tbl_element, element) %&gt;%\n  dm_add_pk(tbl_geneset, geneset) %&gt;%\n  dm_add_fk(tbl_elementset, element, tbl_element) %&gt;%\n  dm_add_fk(tbl_elementset, geneset, tbl_geneset) %&gt;%\n  dm_draw(view_type = \"all\")\n\n\n\n\nModel diagram\n\n\n\n\nQuerying the database\nGreat! Now we are ready to query our database. To make our lives easier, we will use the dplyr package to translate our R syntax into SQL. (But we could just as well use plain SQL instead.)\nFirst we define the remote tables by connecting to our brand new database:\n\ntbl_geneset &lt;- tbl(con, \"tbl_geneset\")\ntbl_element &lt;- tbl(con, \"tbl_element\")\ntbl_elementset &lt;- tbl(con, \"tbl_elementset\")\n\nLet’s return the gene symbols and entrez identifiers that make up the HALLMARK_APOPTOSIS gene set and display the first 5 (in alphabetical order of the gene symbols).\n\nresult &lt;- tbl_elementset %&gt;% \n  dplyr::filter(geneset == \"HALLMARK_ADIPOGENESIS\") %&gt;%\n  dplyr::inner_join(tbl_element, by = \"element\") %&gt;%\n  dplyr::slice_min(n = 5, order_by = element)\nresult\n\n# Source:   SQL [5 x 3]\n# Database: sqlite 3.39.4 [:memory:]\n  element geneset               entrezid\n  &lt;chr&gt;   &lt;chr&gt;                 &lt;chr&gt;   \n1 Abca1   HALLMARK_ADIPOGENESIS 11303   \n2 Abcb8   HALLMARK_ADIPOGENESIS 74610   \n3 Acaa2   HALLMARK_ADIPOGENESIS 52538   \n4 Acadl   HALLMARK_ADIPOGENESIS 11363   \n5 Acadm   HALLMARK_ADIPOGENESIS 11364   \n\n\nAnd now let’s add the gene set’s URL as well:\n\nresult %&gt;%\n  dplyr::left_join(tbl_geneset, by = \"geneset\")\n\n# Source:   SQL [5 x 4]\n# Database: sqlite 3.39.4 [:memory:]\n  element geneset               entrezid url                                    \n  &lt;chr&gt;   &lt;chr&gt;                 &lt;chr&gt;    &lt;chr&gt;                                  \n1 Abca1   HALLMARK_ADIPOGENESIS 11303    https://www.gsea-msigdb.org/gsea/msigd…\n2 Abcb8   HALLMARK_ADIPOGENESIS 74610    https://www.gsea-msigdb.org/gsea/msigd…\n3 Acaa2   HALLMARK_ADIPOGENESIS 52538    https://www.gsea-msigdb.org/gsea/msigd…\n4 Acadl   HALLMARK_ADIPOGENESIS 11363    https://www.gsea-msigdb.org/gsea/msigd…\n5 Acadm   HALLMARK_ADIPOGENESIS 11364    https://www.gsea-msigdb.org/gsea/msigd…\n\n\n\n\nPulling data into a BiocSet\nFinally, we can easily pull selected (or even all) gene sets into a Bioconductor BiocSet object for analysis in R. Importantly, the database does not require us to use R: e.g. python users can connect to the same SQLite database (e.g. using sqlalchemy ) and retrieve the information in whatever form is most useful to them.\nFor example, let’s retrieve all gene sets whose name ends in the letter N, store them in a list and create a BiocSet object.\n\ngene_set_list &lt;- with(\n  tbl_elementset %&gt;% \n    dplyr::filter(geneset %like% '%N') %&gt;%\n    collect(), \n  split(element, geneset)\n)\nes &lt;- BiocSet(gene_set_list)\n\nNext, we add gene set metadata to the es_set tibble, by joining it with the (richer) information in the database. This will add the url column.\n\nes &lt;- left_join_set(es, \n  tbl_geneset, by = c(set = \"geneset\"), \n  copy = TRUE\n)\nes_set(es)\n\n# A tibble: 8 × 2\n  set                                        url                                \n  &lt;chr&gt;                                      &lt;chr&gt;                              \n1 HALLMARK_ALLOGRAFT_REJECTION               https://www.gsea-msigdb.org/gsea/m…\n2 HALLMARK_APICAL_JUNCTION                   https://www.gsea-msigdb.org/gsea/m…\n3 HALLMARK_COAGULATION                       https://www.gsea-msigdb.org/gsea/m…\n4 HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION https://www.gsea-msigdb.org/gsea/m…\n5 HALLMARK_KRAS_SIGNALING_DN                 https://www.gsea-msigdb.org/gsea/m…\n6 HALLMARK_OXIDATIVE_PHOSPHORYLATION         https://www.gsea-msigdb.org/gsea/m…\n7 HALLMARK_PROTEIN_SECRETION                 https://www.gsea-msigdb.org/gsea/m…\n8 HALLMARK_UV_RESPONSE_DN                    https://www.gsea-msigdb.org/gsea/m…\n\n\nAnd finally, let’s also add the entrezid column from out database to the es_element table:\n\nes &lt;- left_join_element(es, \n  tbl_element, by = \"element\", \n  copy = TRUE\n)\nes_element(es)\n\n# A tibble: 1,233 × 2\n   element entrezid\n   &lt;chr&gt;   &lt;chr&gt;   \n 1 Aars    234734  \n 2 Abce1   24015   \n 3 Abi1    11308   \n 4 Ache    11423   \n 5 Acvr2a  11480   \n 6 Akt1    11651   \n 7 Apbb1   11785   \n 8 B2m     12010   \n 9 Bcat1   12035   \n10 Bcl10   12042   \n# … with 1,223 more rows\n\n\n\n\nSQL summary\n\nFor this example the effort required to transform the dataset into a set of three tables - the starting point for import into a relational database - was minimal.\nGiven the use case, e.g. management of a gene set collections, the number of times that data is added to the database is likely much smaller than the number of times it is queried. That makes it worth the effort to transform it once - and benefit from this upfront cost ever after.\nBecause we knew exactly which properties / annotations we wanted to capture in the database, defining the database tables and their relationships (e.g. the schema ) was not an obstacle, either.\nEnabling users to query the data using simple SQL or via a higher level abstraction like dplyr makes it accessible to a broader audience.\n\n\nDefining a schema is much harder when we deal with datasets that are less standardized, deeply nested, changing over time, etc."
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#references",
    "href": "posts/geneset-sqlite-db/index.html#references",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "References",
    "text": "References\nIf you are new to working with databases, then you might find these two great books useful:\n\nSQL for Data Scientists: A Beginner’s Guide for Building Datasets for Analysis by Renee M. P. Teate is a great starting place to learn SQL. It mainly focusses on accessing existing databases.\nPractical SQL: A Beginner’s Guide to Storytelling with Data by Anthony DeBarros teaches readers how to create & populate a Postgres database, and how to index and search it effectively.\n\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package          * version  date (UTC) lib source\n AnnotationDbi    * 1.60.0   2022-11-01 [1] Bioconductor\n askpass            1.1      2019-01-13 [1] CRAN (R 4.2.0)\n assertthat         0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n backports          1.4.1    2021-12-13 [1] CRAN (R 4.2.0)\n Biobase          * 2.58.0   2022-11-01 [1] Bioconductor\n BiocGenerics     * 0.44.0   2022-11-01 [1] Bioconductor\n BiocIO             1.8.0    2022-11-01 [1] Bioconductor\n BiocSet          * 1.12.0   2022-11-01 [1] Bioconductor\n Biostrings         2.66.0   2022-11-01 [1] Bioconductor\n bit                4.0.5    2022-11-15 [1] CRAN (R 4.2.0)\n bit64              4.0.5    2020-08-30 [1] CRAN (R 4.2.0)\n bitops             1.0-7    2021-04-24 [1] CRAN (R 4.2.0)\n blob               1.2.3    2022-04-10 [1] CRAN (R 4.2.0)\n cachem             1.0.6    2021-08-19 [1] CRAN (R 4.2.0)\n cli                3.5.0    2022-12-20 [1] CRAN (R 4.2.0)\n crayon             1.5.2    2022-09-29 [1] CRAN (R 4.2.0)\n credentials        1.3.2    2021-11-29 [1] CRAN (R 4.2.0)\n DBI                1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n dbplyr             2.2.1    2022-06-27 [1] CRAN (R 4.2.0)\n DiagrammeR         1.0.9    2022-03-05 [1] CRAN (R 4.2.0)\n digest             0.6.31   2022-12-11 [1] CRAN (R 4.2.0)\n dm               * 1.0.3    2022-10-12 [1] CRAN (R 4.2.0)\n dplyr            * 1.0.10   2022-09-01 [1] CRAN (R 4.2.0)\n ellipsis           0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate           0.19     2022-12-13 [1] CRAN (R 4.2.0)\n fansi              1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n fastmap            1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n generics           0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb       1.34.4   2022-12-01 [1] Bioconductor\n GenomeInfoDbData   1.2.9    2022-12-12 [1] Bioconductor\n glue               1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n highr              0.9      2021-04-16 [1] CRAN (R 4.2.0)\n htmltools          0.5.4    2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets        1.5.4    2021-09-08 [1] CRAN (R 4.2.2)\n httpuv             1.6.7    2022-12-14 [1] CRAN (R 4.2.0)\n httr               1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n igraph             1.3.5    2022-09-22 [1] CRAN (R 4.2.0)\n IRanges          * 2.32.0   2022-11-01 [1] Bioconductor\n jsonify            1.2.2    2022-11-09 [1] CRAN (R 4.2.0)\n jsonlite         * 1.8.4    2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST           1.38.0   2022-11-01 [1] Bioconductor\n knitr              1.41     2022-11-18 [1] CRAN (R 4.2.0)\n later              1.3.0    2021-08-18 [1] CRAN (R 4.2.0)\n lifecycle          1.0.3    2022-10-07 [1] CRAN (R 4.2.0)\n magrittr           2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n memoise            2.0.1    2021-11-26 [1] CRAN (R 4.2.0)\n mime               0.12     2021-09-28 [1] CRAN (R 4.2.0)\n nodbi            * 0.9.1    2022-11-20 [1] CRAN (R 4.2.0)\n ontologyIndex      2.10     2022-08-24 [1] CRAN (R 4.2.0)\n openssl            2.0.5    2022-12-06 [1] CRAN (R 4.2.0)\n org.Mm.eg.db     * 3.16.0   2022-12-29 [1] Bioconductor\n pillar             1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig          2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n plyr               1.8.8    2022-11-11 [1] CRAN (R 4.2.0)\n png                0.1-8    2022-11-29 [1] CRAN (R 4.2.0)\n promises           1.2.0.1  2021-02-11 [1] CRAN (R 4.2.0)\n purrr            * 1.0.0    2022-12-20 [1] CRAN (R 4.2.0)\n R6                 2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n RColorBrewer       1.1-3    2022-04-03 [1] CRAN (R 4.2.0)\n Rcpp               1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n RCurl              1.98-1.9 2022-10-03 [1] CRAN (R 4.2.0)\n rlang              1.0.6    2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown          2.19     2022-12-15 [1] CRAN (R 4.2.0)\n RSQLite          * 2.2.19   2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi         0.14     2022-08-22 [1] CRAN (R 4.2.0)\n S4Vectors        * 0.36.1   2022-12-05 [1] Bioconductor\n sessioninfo        1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n shiny              1.7.4    2022-12-15 [1] CRAN (R 4.2.0)\n stringi            1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr            1.5.0    2022-12-02 [1] CRAN (R 4.2.0)\n sys                3.4.1    2022-10-18 [1] CRAN (R 4.2.0)\n tibble           * 3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr            * 1.2.1    2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect         1.2.0    2022-10-10 [1] CRAN (R 4.2.0)\n utf8               1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n uuid               1.1-0    2022-04-19 [1] CRAN (R 4.2.0)\n vctrs              0.5.1    2022-11-16 [1] CRAN (R 4.2.0)\n visNetwork         2.1.2    2022-09-29 [1] CRAN (R 4.2.0)\n withr              2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun               0.35     2022-11-16 [1] CRAN (R 4.2.0)\n xtable             1.8-4    2019-04-21 [1] CRAN (R 4.2.0)\n XVector            0.38.0   2022-11-01 [1] Bioconductor\n yaml               2.3.6    2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc           1.44.0   2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the first of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis tutorial documents how to configure & execute the nf-core/rnaseq workflow for the processing of raw QuantSeq 3’ FWD RNA-seq data.\nIt highlights custom nf-core/rnaseq parameters settings and applies them to two published datasets, one with and one without UMIs."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#installing-nextflow",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#installing-nextflow",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Installing Nextflow",
    "text": "Installing Nextflow\nTo run nextflow, we create a new conda environment. (See conda installation instructions if you are not yet familiar with the conda package manager.) Then we install nextflow and its dependencies from the bioconda repository.\n&gt; conda create -n nf\n&gt; conda install -c bioconda nextflow\nOnce the installation is complete, we activate our new conda environment and verify that Nextflow is installed:\n&gt; conda activate nf\n&gt; nextflow -v\nnextflow version 22.10.4.5836\n\nNextflow configs\nNextflow separates the definition of a workflow from the platform it is executed on. For example, you can run the same workflow on your local computer, a high-performance cluster or using a cloud provider like Amazon Web Services (AWS) or Google Cloud Services (GCS).\nEach nf-core workflow defines the required software sources, which can either be installed (e.g. via conda) or be made available through a container engine such as docker, singularity, etc. For details, please consult the Nextflow documentation.\nThe desired configuration is specified in one or more configuration file(s), and passed to the nextflow command via its -profile argument. Here, we execute the workflows on the local system, using docker containers by specifying the built-in -profile docker configuration."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#analysis-of-published-studies",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#analysis-of-published-studies",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Analysis of published studies",
    "text": "Analysis of published studies\nTo validate the nf-core/rnaseq workflow for QuantSeq 3’ tag RNA-seq data, we reanalyze data from two published studies, one with and one without the use of unique molecular identifiers (UMIs).\n\n“Fibrillar Aβ causes profound microglial metabolic perturbations in a novel APP knock-in mouse model” by Xia et al, 2021\n\n\nQuantSeq 3’ mRNA-Seq Library Prep Kit FWD FWD\nThis data does not include UMIs.\nRaw data are available via\n\nGEO accession GSE158152\nSRA accession SRP213880\n\n\n\n“TREM2 regulates microglial lipid metabolism during aging in mice” by Nugent et al, 2020.\n\n\nQuantSeq 3’ mRNA-Seq Library Prep Kit FWD FWD\nThis data includes UMIs, which and can be used to filter alignments originating from PCR duplicates.\nRaw data are available via\n\nGEO accession GSE134031\nSRA accession SRP213880"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-without-umis-xia-et-al-dataset",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-without-umis-xia-et-al-dataset",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Quantseq FWD without UMIs: Xia et al dataset",
    "text": "Quantseq FWD without UMIs: Xia et al dataset\nHere, we will reanalyze RNA-seq data published in the preprint “Fibrillar Aβ causes profound microglial metabolic perturbations in a novel APP knock-in mouse model” by Xia et al, 2021.\n\nExperimental design\nThis study examines FACS-isolated microglia from 18 mice from three genetic backgrounds (N = 6 WT, N =6 heterozygous App-SAA and N = 6 homozygous App-SAA animals). Microglia were isolated from the brain (cortex & hippocampus) of each animal using FACS and gene expression changes were analyzed using 3-tag RNA-seq.\nFrom each mouse, two (technical) replicate pools of microglia were collected and processed into separate libraries with Lexogen’s QuantSeq FWD. This dataset does not include unique molecular identifiers (UMIs).\n\n\nRaw data retrieval with nf-core/fetchngs\nWe start the analysis by downloading the raw data (36 FASTQ files) and the sample metadata with the nf-core/fetchngs workflow. For datasets stored in SRA, we only need to paste the SRA Study identifier (SRP282921) into a text file for us as the input for the fetchngs workflow. The following command will download all of the FASTQ files associated with this study into the raw_data directory:\n&gt; mkdir SRP282921\n&gt; cd SRP282921\n&gt; echo SRP282921 &gt; ids.txt\n&gt; nextflow run nf-core/fetchngs \\\n      -revision 1.9 \\\n      -profile docker \\\n      --outdir raw_data \\\n      --input ids.txt \\\n      --nf_core_pipeline rnaseq \\\n      --nf_core_rnaseq_strandedness forward\nBy specifying the --nf_core_pipeline argument, the workflow creates a sample sheet that can be used as input for the nf-core/rnaseq pipeline. Because the data was generated with the QuantSeq FWD kit, we also know that the reads are from the forward strand and add this information to the sample sheet via the --nf_core_rnaseq_strandedness argument. 1\nOnce the workflow has completed, the raw_data output directory contains the following structure:\n&gt; tree raw_data\nraw_data/\n├── custom\n│   └── user-settings.mkfg\n├── fastq\n│   ├── md5\n│   │   ├── SRX9142647_SRR12661938.fastq.gz.md5\n│   │   ├── SRX9142648_SRR12661924.fastq.gz.md5\n│   │   ├── 34 additional .fastq.gz.md5 files (not shown)\n│   ├── SRX9142647_SRR12661938.fastq.gz\n│   ├── SRX9142648_SRR12661924.fastq.gz\n│   ├── 34 additional .fastq.gz files (not shown)\n├── metadata\n│   └── SRP282921.runinfo_ftp.tsv\n├── pipeline_info\n│   ├── execution_report_2023-01-12_01-49-02.html\n│   ├── execution_timeline_2023-01-12_01-49-02.html\n│   ├── execution_trace_2023-01-12_01-49-02.txt\n│   ├── pipeline_dag_2023-01-12_01-49-02.html\n│   └── software_versions.yml\n└── samplesheet\n    ├── id_mappings.csv\n    ├── multiqc_config.yml\n    ├── nf_params.json\n    ├── run.sh\n    └── samplesheet.csv\n6 directories, 90 files\n\nThe 36 FASTQ files are in the fastq subfolder, each accompanied by a MD5 checksum file in the md5 directory.\nIn the samplesheet subdirectory, we find the samplesheet.csv file that we can pass to the nf-core/rnaseq workflow.\n\nFor more details about the output oft he workflow, please check the next post in this series] and the nf-core/rnaseq output docs.\n\n\nConfiguring the nf-core/rnaseq workflow\n\nReference data\nAccording to the sample metadata in NCBI GEO the authors mapped the data to the mouse GRCm38 genome version, using Gencode annotations from release M17. To later compare our results with those obtained in the original publication, we use the same Gencode version (even though it is not the most recent).\nThe nf-core/rnaseq workflow can automatically generate all necessary indices when provided with\n\nA FASTA file with the genomic sequences, e.g. Gencode’s Primary genome assembly and\nA GFP file with matching gene annotations, e.g.  Gencode Comprehensive gene annotations\n\nBecause downloading and indexing the genome is time consuming, we will save them for later use.\n\n\nCustom arguments\nThe nf-core/rnaseq pipeline has robust default parameters that are suitable for whole transcriptome RNA-seq data. But this dataset was generated with Lexogen’s QuantSeq FWD 3’ mRNA library preparation kit, which specificially targets the 3’ end of polyadenylated transcripts. As a consequence, reads align primarily to the 3’ UTR and the final exon and coverage of the remainder of the gene body is minimal.\nTo account for these properties, we supply a number of custom arguments to the nextflow run command, which augment the workflow for this specific data type.\n\n\nExtra STAR arguments\nLexogen provides an example analysis workflow on their website, which uses the STAR aligner with several non-default parameters. Most of them parameters correspond to the ENCODE standard options listed in the STAR manual:\n\n--alignIntronMax 1000000\n--alignIntronMin 20\n--alignMatesGapMax 1000000\n--alignSJoverhangMin 8\n--alignSJDBoverhangMin 1\n--outFilterMismatchNmax 999\n--outFilterMultimapNmax 20\n--outFilterType BySJout\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nThe ENCODE project originally focussed on data from human and mouse. The parameters shown above are suitable for these genomes or those with comparable characteristics. For analysis of data from other organisms, e.g. those with shorter introns, you might need to modify them.\n\n\n\nIn addition, Lexogen also specified the\n\n--outFilterMismatchNoverLmax 0.1, decreasing the tolerance for mismatches.\n\nLexogen’s example workflow uses bbduk to trim adapters, poly(A) tails and low quality bases from the 3’ end of reads. The nf-core/rnaseq workflow uses Trim Galore to trim adapters, but does not remove poly(A) tails. Instead we use the STAR ligner to clip poly(A) tails during the alignment stage:\n\n--clip3pAdapterSeq AAAAAAAA\n\nWe will pass all of the arguments listed above to STAR via nf-core/rnaseq’s --extra_star_align_args argument (as a string, see example below).\n\n\nExtra Salmon arguments\nThe salmon algorithm takes into account transcript length when quantifying gene expression. Because QuantSeq (and other 3’ tag sequencing methods) only capture the 3’ most part of each transcript, not its full length, this feature needs to be deactivated:\n\n--noLengthCorrection\n\nThe nf-core/rnaseq workflow features a special argument, extra_salmon_quant_args to pass additional arguments to the salmon tool (see syntax below).\n\n\n\nStarting the workflow\nWe can pass the parameters we discussed above as individual arguments to the nextflow run command (prefixed with --). Alternatively, we can collect them in a JSON file and specify its name via the -params-file argument.\nFor example, the following JSON string specifies the parameters for analyzing sQuantSeq FWD data without UMIs:\n{\n    \"save_reference\": true,\n    \"fasta\": \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M17/GRCm38.primary_assembly.genome.fa.gz\",\n    \"gtf\": \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M17/gencode.vM17.primary_assembly.annotation.gtf.gz\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": false,\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nWe store the JSON string in a file called nf_params.json, and then pass it to the nextflow run command 2:\n&gt; nextflow run \\\n      nf-core/rnaseq \\\n      -r 3.10.1 \\\n      -profile docker \\\n      -params-file nf_params.json \\\n      -resume \\\n      --input raw_data/samplesheet/samplesheet.csv \\\n      --outdir SRP282921\n\n\nExamining the output files\n\n\nReusing reference files and indices\nRetrieving the reference files and indexing them is time consuming. If you are planning to map additional sample against the same genome / transcriptome in the future, you might want to reuse them. Because we set the save_reference argument to true in our workflow, the output directory contains a genome folder, featuring reference files (e.g. genome and transcriptome sequences in FASTQ formats) and indices (e.g. for STAR, salmon and rsem).\nLet’s move the genome folder to our current working directory (or any other suitable location). 3 For future runs, we can include paths to the files and indices in our nf_params.json file, e.g.\n{\n    \"save_reference\": false,\n    \"fasta\": \"genome/GRCm38.primary_assembly.genome.fa\",\n    \"gtf\": \"genome/gencode.vM17.primary_assembly.annotation.gtf\",\n    \"gene_bed\": \"genome/gencode.vM17.primary_assembly.annotation.bed\",\n    \"transcript_fasta\": \"genome/genome.transcripts.fa\",\n    \"star_index\": \"genome/index/star\",\n    \"salmon_index\": \"genome/index/salmon\",\n    \"rsem_index\": \"genome/rsem\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": false,\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nOur next run will execute much faster, because these reference files and indices are already available and don’t need to be created from scratch.\n\n\n\n\n\n\nNote\n\n\n\nIf you chose a location other than genome to store your reference data, please specify the (absolute) path in the JSON file instead."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-with-umis-nugent-et-al-dataset",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-with-umis-nugent-et-al-dataset",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Quantseq FWD with UMIs: Nugent et al dataset",
    "text": "Quantseq FWD with UMIs: Nugent et al dataset\nEspecially when only low amounts of input material (total RNA) are available, the QuantSeq FWD procotol requires multiple PCR amplification steps. Each step generates clones of the originally captured tags, but does not provide new information about transcript abundance. Inclusion of unique molecular identifiers (UMIs) in the second strand synthesis guarantees that PCR duplicates can be identified and removed in the analysis (based on both the coordinates of their alignment and the UMI sequence).\nNugent et al used the QuantSeq FWD protocol with UMIs, and we will reanalyze this dataset to illustrate how to instruct UMI-tools to extract the UMIs from the raw reads and deduplicated the STAR alignments.\n\nExperimental design\nThis study examines FACS-isolated astroctyes and microglia from female mice that were either 2- or 16 months of age. The animals are either wildtype (WT) or\nhomozygous knockouts (KO) for the Trem2 gene.\nFrom each mouse, separate microglia and astrocyte samples were collected and processed into separate libraries with Lexogen’s QuantSeq FWD kit and the QuantSeq UMI add-on module.\n\n\nRaw data retrieval with nf-core/fetchngs\nAs in the first example, we use the nf-core/fetchngs workflow to retrieve the FASTQ files from SRA.\n&gt; mkdir SRP213880\n&gt; cd SRP213880\n&gt; echo SRP213880 &gt; ids.txt\n&gt; nextflow run nf-core/fetchngs \\\n      -revision 1.9 \\\n      -profile docker \\\n      --outdir raw_data \\\n      --input ids.txt \\\n      --nf_core_pipeline rnaseq \\\n      --nf_core_rnaseq_strandedness forward\n\n\nConfiguring the nf-core/rnaseq workflow\n\nReference data\nAccording to the sample metadata in NCBI GEO the authors used the same reference data as Xia et al, Gencode release M17. We can therefore reuse the reference files and indices we generated in the first analysis (see above).\n\n\nUnique molecular identifiers\nTo process QuantSeq data with UMIs, the following parameters need to be provided.\n\n--with_umi\n--umitools_extract_method \"regex\": The type of pattern to detect, either string (default) or regex\n--umitools_bc_pattern \"^(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*\": A regular expression that instructs UMI-tools to extract the first 6 bases (the UMI) from the 5’ end of the read 4 and to discard the following 4 bases. (Lexogen adds an invariant TATA sequence motif to each UMI.)\n--umitools_grouping_method \"unique\": The method used to group similar UMIs, e.g. to correct sequencing errors. The default method is directional, but it is computationally expensive. Here, we use the simpler unique method, suppressing grouping / error correction entirely.\n\n\n\n\n\n\n\nNote\n\n\n\nThese arguments shown above are suitable for UMIs offered by Lexogen for the QuantSeq protocol. If you use custom UMIs, please consult the nf-core/rnaseq documentation for full details and additional options for details on additional parameters you can set.\n\n\n\n\n\nStarting the workflow\nNext, we add the UMI-related parameters to the JSON file we used above, so we can pass them to nextflow run via the -params-file argument. Because Nugent et al used the same reference (Gencode M17), we can reuse the same reference we generated for Xia et al’s dataset. (Here, we assume the precomputed references are in the genome folder to our current working directory. If you chose to store them elsewhere, please provide absolute paths in the JSON file instead.)\n{\n    \"save_reference\": false,\n    \"fasta\": \"genome/GRCm38.primary_assembly.genome.fa\",\n    \"gtf\": \"genome/gencode.vM17.primary_assembly.annotation.gtf\",\n    \"gene_bed\": \"genome/gencode.vM17.primary_assembly.annotation.bed\",\n    \"transcript_fasta\": \"genome/genome.transcripts.fa\",\n    \"star_index\": \"genome/index/star\",\n    \"salmon_index\": \"genome/index/salmon\",\n    \"rsem_index\": \"genome/rsem\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": true,\n    \"umitools_extract_method\": \"regex\",\n    \"umitools_grouping_method\": \"unique\",\n    \"umitools_bc_pattern\": \"^(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*\",\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nWe store the JSON string in a file called nf_params.json, and then pass it to the nextflow run command:\n&gt; nextflow run \\\n      nf-core/rnaseq \\\n      -r 3.10.1 \\\n      -profile docker \\\n      -params-file nf_params.json \\\n      -resume \\\n      --input raw_data/samplesheet/samplesheet.csv \\\n      --outdir SRP213880\nUpon successful completion, the workflow’s output is available in the SRP213880 directory.\nNext, we will take a closer look at the available results in the second post in this series]."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe strandedness column in the sample sheet CSV file must contain one of forward, reverse, unstranded or auto. The latter will prompt the nf-core/rnaseq workflow to subsample reads and map them to the transcriptome with salmon to determine the likely orientation of the reads. Because this adds additional steps to the workflow execution, it is recommended that you specify the strandedness of your library explicitly whenever possible.↩︎\nThe -resume argument allows you to resume previous executions of the same workflow. Nextflow will reuse any existing outputs and generate only those that are missing.↩︎\nNextflow can interact with different storage backends. For example, you could provide URLs of remote files (e.g. with the ftp:// prefix) or in cloud storage (e.g. with the s3:// or gs:// prefix) and they will automatically be downloaded and staged when you start the run.↩︎\nTypically, QuantSeq libraries are single-end (not paired-end) and the UMI is therefore found in the R1 read.↩︎"
  },
  {
    "objectID": "posts/pyroe-installation/index.html",
    "href": "posts/pyroe-installation/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "Alevin-fry is a highly accurate and performant method to process single-cell or single-nuclei RNA-seq data. For downstream processing, its output can be parsed into R with the fishpond::loadFry() function. For analysis using python, the pyroe module is available.\nIt can be installed either using pip or conda, and the latter will install additional dependencies (e.g. bedtools) and include the load_fry() as well.\nTo install pyroe with conda, I first followed bioconda’s instructions to add and configure the required channels:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nand then installed pyroe\nconda install pyroe\nNow I can convert alevin-fry output to one of the following formats: zarr, csvs, h5ad or loom.\npyroe convert --help\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/quarto-figure-size-and-layout/index.html",
    "href": "posts/quarto-figure-size-and-layout/index.html",
    "title": "Figure size, layout & tabsets with Quarto",
    "section": "",
    "text": "In this document, I am experimenting with various attributes that organize the layout, size and placement of figures of Quarto document. For more details, please check out the official documentation, especially the topics on figures and article layout.\n\n\n\n\n\n\nNote\n\n\n\nFor illustration, I am displaying both the code that generates a simple plot as well as the attributes that determine how it is rendered, e.g. the ::: tags interspersed with the code blocks, and the #| attributes within individual code cells. See the documentation on executable blocks for details.\n\n\nFirst, let’s generate a simple plot, so we can see the effect of different attributes on how it is rendered in subsequent code cells.\nTo start, we render the output without specifying any custom attributes, e.g. using the default settings for this Quarto website:\n\nlibrary(ggplot2)\ntheme_set(theme_linedraw(base_size = 14))\np &lt;- ggplot(mtcars, aes(x = mpg, y = drat)) + \n  geom_point(color = \"skyblue\", size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  theme(panel.grid = element_blank())\np\n\n\n\n\n\nWidth and height of individual figures\nThe fig-width and fig-height attributes specify the dimensions of the image file that is generated. The out-width attribute determines the size at which that image is displayed in the rendered HTML page.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\nFor example, the same image can be displayed at 50% of the width of the enclosing &lt;div&gt;.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"25%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\n\n\nLayout: columns and rows\nThe layout-ncol and layout-nrow attributes govern the placement of multiple figures within the same element. For example, we can place two figures next to each other, in two column.\nThe fig-align attributes specify the figure alignment within each column.\n\n\n\n\n\n\nTip\n\n\n\nThe out-width attribute is always relative to its enclosing element, e.g. here out-width: \"50%\" refers to half of the width of a column, not the page width.\n\n\n::: {layout-ncol=2}\n\n\n\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\n\n#| out-width: \"30%\"\n#| fig-align: \"right\"\n\n\n\n\np\n\n\n\n\n\n\n\n\n\np\n\n\n\n\n\n\n\n\n\n\n:::\n\n\nTabsets\nTabsets can be used to organize contents, e.g. by hiding content until the other clicks on the tab’s header.\nThe layout of the first tabset contains just one column and row.\n::: {.panel-tabset}\n\npanel 1panel 2\n\n\n\np\n\n\n\n\n\n\n\n\n\n\nThe second panel is subdivided into two columns. (Note the use of the :::: tag, nested within the ::: parent tag.)\n:::: {layout-ncol=2}\n\n\n\np\n\n\n\n\n\np\n\n\n\n\n\n\n\n::::\n\n\n\n\n\n\n:::\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html",
    "href": "posts/nextflow-core-quantseq-2-output/index.html",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the second of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis post documents the output files & folders of the nf-core/rnaseq workflow (v 3.10.1), run with default settings with the star_salmon aligner / quantitation method.\nFor additional information, e.g. on the content of the MultiQC report, please see the official nf-core/rnaseq documentation."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#reports",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#reports",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Reports",
    "text": "Reports\n\nMultiQC report\nThe MultiQC HTML report is a one-stop-shop that summarises QC metrics across the workflow. It can be found int he multiqc folder, in a subdirectory named according to the aligner & quantifier combination used (default: star_salmon).\n\n\n\nmultiqc report\n\n\n\n\nPipeline info\nThe pipeline_info folder contains html reports and text (CSV, TXT, YML) files with information about the run, including the versions of the software tools used.\n\n\n\npipeline info\n\n\n\n\nFastQC reports\nThe fastqc folder contains the output of the fastqc tool. Most of the reported metrics are included in the MultiQC report as well, but the HTML reports for individual samples are available here if needed.\n\n\n\nFastQC report\n\n\n\n\nTrim Galore reports\nThe trimgalore folder contains\n\ntrimming reports for each sample\nthe fastqc sub-folder with quality metrics for the trimmed FASTQ files\n\n\n\n\nTrim Galore\n\n\n\n\numitools\nThis folder contains the log files returned by UMI-tools\n\n\n\nUMI-tools"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#workflow-results",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#workflow-results",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Workflow results",
    "text": "Workflow results\nThe main output of the workflow is available in the star_salmon subdirectory. (This folder is named after the selected alignment & quantification strategy, e.g. star_salmon is present only if this tool combination was used.)\nIt contains multiple folders, as well as the (deduplicated) sorted BAM files for each sample.\n\nbigwig\nGenome coverage in bigWig format for each sample.\n\n\n\nBigWig files\n\n\n\n\nDESeq2 object & QC metrics\nThe workflow aggregates all counts into a DESeq2 R objects, performs QC and exploratory analyses and serializes the object as deseq2.dds.RData.\n\n\n\nDESeq2 object & QC metrics\n\n\n\n\nDupradar\nThe dupRadar Bioconductor package performs duplication rate quality control.\n\n\n\ndupRadar\n\n\n\n\nFeaturecounts\nOutput from featurecounts tool is only used to generate QC metrics. For actual quantitation of the gene-level results, the output of salmon (default) or rsem are used.\nThe metrics reported in the featurecounts folder are included in the MultiQC report.\n\n\n\nfeatureCounts\n\n\n\n\nSTAR log files\nThis folder contains log files output by the STAR aligner.\n\n\n\nSTAR logs\n\n\n\n\nQualimap\nThe qualimap package generates QC metrics from BAM files.\n\n\n\nQualimap\n\n\n\n\nRSEQC\nThe output of the rseqc QC control package are in this directory.\n\n\n\nRSEQC"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#alignments-gene-level-counts",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#alignments-gene-level-counts",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Alignments & gene-level counts",
    "text": "Alignments & gene-level counts\nThe star_salmon folder also contains the main results of the workflow: gene-level counts and alignments (BAM files).\n\nSalmon quantitation\n\nAggregated\nThe workflow outputs salmon quantitation results aggregated across all samples. Different types of counts (e.g. raw, length-scaled, TPMs) are available - the choice for downstream analyses depends on the chosen approach. Please see tximport for details.\n\n\n\nSalmon aggrated output\n\n\n\n\nBy sample\nIn addition, the salmon outputs for individual samples are avialable in sub-folders, one for each sample.\n\n\n\nSalmon output for each sample\n\n\n\n\n\nSTAR alignments\nThe sorted (and, in the case of datasets that include UMIs, deduplicated) BAM files and their indices are available:\n\n\n\nSTAR alignments"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#genome-gene-annotations-indices",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#genome-gene-annotations-indices",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Genome, gene annotations & indices",
    "text": "Genome, gene annotations & indices\nIf the workflow was exectuted with the \"save_reference\": true parameter, then all reference files (FASTA, GTF, BED, etc) and the indices generated for STAR, salmon and rsem are returned in the genome folder within the output directory:\n\n\n\ngenome folder\n\n\nThese files can be reused for future runs, shortening the execuction time of the workflow.\nNext, we will compare how the gene-counts returned of the nf-core/rnaseq workflow compare to those posted on GEO by the authors of the two datasets we processed in the first post in this series by performing a differential expression analysis in the third and fourth posts in this series."
  },
  {
    "objectID": "posts/30days-of-streamlit/index.html",
    "href": "posts/30days-of-streamlit/index.html",
    "title": "Guess the correlation - a first streamlit app",
    "section": "",
    "text": "TL;DR\n\nI learned the basics of creating web applications with Streamlit\nBuild your intuition about correlation coefficients in my first app here!\n\nThis week, I learned about Streamlit, a python module to rapidly develop dashboards and (simple) web applications. Having used Posit’s shiny framework in the past (using R), I enjoyed diving into a solution that uses python.\nThere are numerous comparisons between different frameworks to develop dashboards with python (e.g.  this one ). Most recently, shiny for python has entered the stage as well.\nTo get started, I completed 30 days of Streamlit, short exercises that introduce key Streamlit elements.\nNext, I tried my hands at coding a simple app from scratch. To challenge myself, I implemented a simplified version of Omar Wagih’s awesome Guess The Corrlelation game. A user is presented with a scatter plot and prompted to guess the (Pearson) correlation coefficient between the x- and y-variables.\n\n\n\nMy first streamlit app\n\n\nTrue to its promise of “turning data scripts into shareable web apps in minutes” I was able to get a simple application up and running very quickly, with only a few lines of code.\nStreamlit makes it easy to add form elements, graphs or markdown-formatted text to a web application. While shiny defines which elements need to be refreshed based on user input explicitly (see with reactive epressions ), streamlit simply reruns the entire script whenever a user interacts with the application. That took some getting used to, e.g. as variables are reset in the process.\nTo store selections and variables across reruns, the Session State a field-based API, is available, and I used it extensively:\n# persistent variables\nwith st.sidebar:\n    st.subheader(\"Settings\")\n    st.session_state[\"n\"] = st.number_input(\"Number of data points\", 2, 1000, 100)\nif not \"data\" in st.session_state:\n    st.session_state[\"data\"] = dataset(st.session_state[\"n\"])\nif not \"cor\" in st.session_state:\n    st.session_state[\"cor\"] = correlation(st.session_state[\"data\"])\nif not \"guessed\" in st.session_state:\n    st.session_state[\"guessed\"] = False\nif not \"streak\" in st.session_state:\n    st.session_state[\"streak\"] = False\nif not \"streak_length\" in st.session_state:\n    st.session_state[\"streak_length\"] = 0\nif not \"coins\" in st.session_state:\n    st.session_state[\"coins\"] = 3\nI also wanted to display two alternative buttons, either offering the user the option to submit a guess (Submit!) or to refresh the chart and start over (Try again!).\nControlling the conditional flow of the app was a bit of a challenge (for a beginner like myself), but eventually I was able to accomplish it through liberal use of the experimental st.experimental_rerun() command.\nI deployed the final application in the streamlit cloud at https://correlation.streamlit.app/. (Any feedback is very welcome!)\nOverall, I was impressed how quickly I could put together a dashboard, and I am looking forward to sharing analysis results and interactive plots with my collaborators in the future. For more complex applications, I will look into shiny (R/phython), Flask or Django instead.\n\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/drat/index.html",
    "href": "posts/drat/index.html",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "",
    "text": "Today I learned how to\n\nBuild an R package into source and binary bundles for distribution.\nCreate a local drat repository.\nAdd an R package to the repository and install it from there.\nHost the repository remotely in an AWS S3 bucket.\n\nMany thanks to Dirk Eddelbuettel for creating and documenting the drat R package! (As always, any mistakes are my own.)\n\n\nThere are multiple ways for developers to share R packages publicly, e.g.\n\nSubmit them to the The Comprehensive R Archive Network (CRAN),\nContribute them to the Bioconductor project,\nPublish them via rOpenSci’s R-universe\n\nUser can then install these packages via the familiar install.packages() command.\nAlternatively, authors can share their code through version control systems like github or gitlab, and users can install them with third-party tools e.g. the remotes R package.\nBut how can you make an R package available privately, e.g. for use within an organization?\nIn this tutorial, I demonstrate how to set up your own package repository with Dirk Eddelbuettel’s drat R package, add a package, make R aware of the new repo - and host it remotely on AWS S3.\n\n\n\nDirk Eddelbuettel highlights two main advantages:\n\nA package installed from a drat repository will be supported by install.packages() and update.packages(), so the user has easy methods for keeping up-to-date.\nThe package author has better control over the package version users install, because they actively push specific releases into the repository.\n\nPlease see Dirk’s Drat FAQ’s for additional points, e.g. ‘Why could install_github be wrong?’\n\n\n\nHadley Wickham and Jenny Bryan have documented how to author, document and build R packages in their freely-available R Packages book. In this walkthrough I am using Mac OS X (v13.1), but you can find instructions to set up Windows or Linux build environments in their R build toolchain chapter.\n\n\n\nFirst, we need an R package that’s ready for distribution. Here, I am using the toy R package that you can retrieve from github, either via git clone https://github.com/tomsing1/toy or by downloading its source code as a zip file. (Feel free to follow along with another R package instead - as long as you have the source package, the following steps apply.)\nNext, we bundle the package into a single compressed file with the .tar.gz file extension. Let’s download the .zip file linked above into the ~/Downloads folder and use the R CMD build command to create a source bundle 1:\n\ncd ~/Downloads\ncurl -s -L -O https://github.com/tomsing1/toy/archive/refs/heads/main.zip\nunzip -o -q main.zip\nrm main.zip\nR CMD build --force toy-main\n\n* checking for file ‘toy-main/DESCRIPTION’ ... OK\n* preparing ‘toy’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘toy_0.1.0.tar.gz’\n\n\nWe now have the toy_0.1.0.tar.gz file, ready to be inserted into a new (or existing) drat repository.\n\n\n\nTo create a new repository, we start by installing the drat R package itself (if it’s not available on your system already) with the following R commands:\n\nif (!requireNamespace(\"drat\", quietly = TRUE)) {\n  install.packages(\"drat\")\n}\nlibrary(drat)\n\nYou can specify the path of your drat repository either by setting the dratRepo option 2:\n\noptions(dratRepo = \"~/drat-tutorial\")\ngetOption(\"dratRepo\")\n\n[1] \"~/drat-tutorial\"\n\n\nor by providing it as an argument to the drat::insertPackage() function (see below).\nLet’s create a new drat repository in our home directory 3, and populate it with a minimal index.html file (to avoid HTTP 404 Not Found errors later).\n\ndir.create(\"~/drat-tutorial\", showWarnings = FALSE)\nwriteLines(\n  text = \"&lt;!doctype html&gt;&lt;title&gt;My awesome drat repository!&lt;/title&gt;\",\n  con = \"~/drat-tutorial/index.html\"\n)\n\nNow we are ready to insert the toy package bundle into the repository with drat’s insertPackage() command 4:\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tar.gz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains the following files:\n\n\n\ndrat repository\n\n\n\n\n\nWhen you prompt your R installation to install or update R packages, it searches repositories specified in the repos option. On my system, only the default repository is set in a fresh R session 5:\n\ngetOption(\"repos\")\n\n                         CRAN \n\"https://cloud.r-project.org\" \n\n\nIf I try to install our example toy R package, I don’t succeed:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'toy' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nbecause R is not aware of our new repository, yet.\n\n\n\n\n\n\nInstalling from source\n\n\n\nAt this point, we must add the type=\"source\" argument, because we have only added the source bundle to the repository. We will add a compiled version in a moment - read on!\n\n\nTo test our local repository, we add its path to the list of known repositories.\n\ndrat::addRepo(\"LocalRepo\", \"file://Users/sandmann/drat-tutorial\")\ngetOption(\"repos\")\n\n                                 CRAN                             LocalRepo \n        \"https://cloud.r-project.org\" \"file://Users/sandmann/drat-tutorial\" \n\n\n\n\n\n\n\n\nSpecifying file:// paths\n\n\n\nBy default, drat’s addRepo() command assumes that repositories are hosted on github-pages. Because we want to access a repo via the filesystem (either locally or on a network drive), we need to explicitly add the file:/ prefix - and use the absolute file path (e.g. returned by path.expand(\"~/drat-tutorial\")) to specify its location.\nIn this case, concatenating file:/ with /Users/sandmann/drat-tutorial produces the final file://Users/sandmann/drat-tutorial location (note the double forward slashes).\n\n\nNow, we can install it with the usual install.packages() command 6:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\nGreat! We have successfully installed our toy R package from our brand new repository. Now it is time to make it available to other users as well.\n\n\n\nWindows and Mac users who install packages from CRAN or any user installing files from the Posit Public Package Manager (PPPM) will usually receive a binary package. CRAN accepts package bundles and creates the platform-specific binary file for distribution. To offer the same service to users of our drat repository, we need to compile the binary package ourselves.\nHere, I create the Mac OS binary package from the bundle we obtained above by executing the following command on my Mac OS operating system:\n\ncd ~/Downloads\nR CMD INSTALL --build toy_0.1.0.tar.gz\n\n* installing to library ‘/Users/sandmann/Library/R/x86_64/4.2/library’\n* installing *source* package ‘toy’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* creating tarball\npackaged installation of ‘toy’ as ‘toy_0.1.0.tgz’\n* DONE (toy)\n\n\nThis command will first install the package into my default R library, and then create the binary toy_0.1.0.tgz file.\nNext, we add it to our local drat repository (note the .tgz file suffix).\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tgz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains a new subdirectory (bin) with the binary files for Mac OS X:\n\n\n\ndrat repository\n\n\nAt long last, now we can omit the type=\"source\" argument from calls to install.packages():\n\ninstall.packages(\"toy\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/wc/9tswmr4s74s0x90wqh2007300000gp/T//Rtmpg6tHJA/downloaded_packages\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/drat/index.html#tldr",
    "href": "posts/drat/index.html#tldr",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "",
    "text": "Today I learned how to\n\nBuild an R package into source and binary bundles for distribution.\nCreate a local drat repository.\nAdd an R package to the repository and install it from there.\nHost the repository remotely in an AWS S3 bucket.\n\nMany thanks to Dirk Eddelbuettel for creating and documenting the drat R package! (As always, any mistakes are my own.)\n\n\nThere are multiple ways for developers to share R packages publicly, e.g.\n\nSubmit them to the The Comprehensive R Archive Network (CRAN),\nContribute them to the Bioconductor project,\nPublish them via rOpenSci’s R-universe\n\nUser can then install these packages via the familiar install.packages() command.\nAlternatively, authors can share their code through version control systems like github or gitlab, and users can install them with third-party tools e.g. the remotes R package.\nBut how can you make an R package available privately, e.g. for use within an organization?\nIn this tutorial, I demonstrate how to set up your own package repository with Dirk Eddelbuettel’s drat R package, add a package, make R aware of the new repo - and host it remotely on AWS S3.\n\n\n\nDirk Eddelbuettel highlights two main advantages:\n\nA package installed from a drat repository will be supported by install.packages() and update.packages(), so the user has easy methods for keeping up-to-date.\nThe package author has better control over the package version users install, because they actively push specific releases into the repository.\n\nPlease see Dirk’s Drat FAQ’s for additional points, e.g. ‘Why could install_github be wrong?’\n\n\n\nHadley Wickham and Jenny Bryan have documented how to author, document and build R packages in their freely-available R Packages book. In this walkthrough I am using Mac OS X (v13.1), but you can find instructions to set up Windows or Linux build environments in their R build toolchain chapter.\n\n\n\nFirst, we need an R package that’s ready for distribution. Here, I am using the toy R package that you can retrieve from github, either via git clone https://github.com/tomsing1/toy or by downloading its source code as a zip file. (Feel free to follow along with another R package instead - as long as you have the source package, the following steps apply.)\nNext, we bundle the package into a single compressed file with the .tar.gz file extension. Let’s download the .zip file linked above into the ~/Downloads folder and use the R CMD build command to create a source bundle 1:\n\ncd ~/Downloads\ncurl -s -L -O https://github.com/tomsing1/toy/archive/refs/heads/main.zip\nunzip -o -q main.zip\nrm main.zip\nR CMD build --force toy-main\n\n* checking for file ‘toy-main/DESCRIPTION’ ... OK\n* preparing ‘toy’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘toy_0.1.0.tar.gz’\n\n\nWe now have the toy_0.1.0.tar.gz file, ready to be inserted into a new (or existing) drat repository.\n\n\n\nTo create a new repository, we start by installing the drat R package itself (if it’s not available on your system already) with the following R commands:\n\nif (!requireNamespace(\"drat\", quietly = TRUE)) {\n  install.packages(\"drat\")\n}\nlibrary(drat)\n\nYou can specify the path of your drat repository either by setting the dratRepo option 2:\n\noptions(dratRepo = \"~/drat-tutorial\")\ngetOption(\"dratRepo\")\n\n[1] \"~/drat-tutorial\"\n\n\nor by providing it as an argument to the drat::insertPackage() function (see below).\nLet’s create a new drat repository in our home directory 3, and populate it with a minimal index.html file (to avoid HTTP 404 Not Found errors later).\n\ndir.create(\"~/drat-tutorial\", showWarnings = FALSE)\nwriteLines(\n  text = \"&lt;!doctype html&gt;&lt;title&gt;My awesome drat repository!&lt;/title&gt;\",\n  con = \"~/drat-tutorial/index.html\"\n)\n\nNow we are ready to insert the toy package bundle into the repository with drat’s insertPackage() command 4:\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tar.gz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains the following files:\n\n\n\ndrat repository\n\n\n\n\n\nWhen you prompt your R installation to install or update R packages, it searches repositories specified in the repos option. On my system, only the default repository is set in a fresh R session 5:\n\ngetOption(\"repos\")\n\n                         CRAN \n\"https://cloud.r-project.org\" \n\n\nIf I try to install our example toy R package, I don’t succeed:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'toy' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nbecause R is not aware of our new repository, yet.\n\n\n\n\n\n\nInstalling from source\n\n\n\nAt this point, we must add the type=\"source\" argument, because we have only added the source bundle to the repository. We will add a compiled version in a moment - read on!\n\n\nTo test our local repository, we add its path to the list of known repositories.\n\ndrat::addRepo(\"LocalRepo\", \"file://Users/sandmann/drat-tutorial\")\ngetOption(\"repos\")\n\n                                 CRAN                             LocalRepo \n        \"https://cloud.r-project.org\" \"file://Users/sandmann/drat-tutorial\" \n\n\n\n\n\n\n\n\nSpecifying file:// paths\n\n\n\nBy default, drat’s addRepo() command assumes that repositories are hosted on github-pages. Because we want to access a repo via the filesystem (either locally or on a network drive), we need to explicitly add the file:/ prefix - and use the absolute file path (e.g. returned by path.expand(\"~/drat-tutorial\")) to specify its location.\nIn this case, concatenating file:/ with /Users/sandmann/drat-tutorial produces the final file://Users/sandmann/drat-tutorial location (note the double forward slashes).\n\n\nNow, we can install it with the usual install.packages() command 6:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\nGreat! We have successfully installed our toy R package from our brand new repository. Now it is time to make it available to other users as well.\n\n\n\nWindows and Mac users who install packages from CRAN or any user installing files from the Posit Public Package Manager (PPPM) will usually receive a binary package. CRAN accepts package bundles and creates the platform-specific binary file for distribution. To offer the same service to users of our drat repository, we need to compile the binary package ourselves.\nHere, I create the Mac OS binary package from the bundle we obtained above by executing the following command on my Mac OS operating system:\n\ncd ~/Downloads\nR CMD INSTALL --build toy_0.1.0.tar.gz\n\n* installing to library ‘/Users/sandmann/Library/R/x86_64/4.2/library’\n* installing *source* package ‘toy’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* creating tarball\npackaged installation of ‘toy’ as ‘toy_0.1.0.tgz’\n* DONE (toy)\n\n\nThis command will first install the package into my default R library, and then create the binary toy_0.1.0.tgz file.\nNext, we add it to our local drat repository (note the .tgz file suffix).\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tgz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains a new subdirectory (bin) with the binary files for Mac OS X:\n\n\n\ndrat repository\n\n\nAt long last, now we can omit the type=\"source\" argument from calls to install.packages():\n\ninstall.packages(\"toy\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/wc/9tswmr4s74s0x90wqh2007300000gp/T//Rtmpg6tHJA/downloaded_packages"
  },
  {
    "objectID": "posts/drat/index.html#hosting-your-drat-repository-on-aws-s3",
    "href": "posts/drat/index.html#hosting-your-drat-repository-on-aws-s3",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Hosting your drat repository on AWS S3",
    "text": "Hosting your drat repository on AWS S3\ndrat repositories can be hosted in any location\n\nthat you can write files to and\nthat can serve files via http\n\nBut unless you placed your drat repository into a network drive that is accessible by multiple users, it is currently only useful to yourself.\n\n\n\n\n\n\nSharing repository over a local network\n\n\n\n\n\nIf you chose a network drive as the location of your drat repository, then other user can benefit from it right - as long as they can read from the shared directory. As before, the absolute path must be prefixed with the file:/ prefix. For example, a repository that is available on the user’s systems at /nfs/groups/groupABC/R/drat would be added to the list of R repositories via drat::addRepo(\"workgroup\", \"file://nfs/groups/groupABC/R/drat\").\n\n\n\nThe drat documentation illustrates how you can use git and github pages to make your repository publicly available.\nHere, we are interested in hosting a repository privately instead, e.g. in a location that is only accessible from within our own organization:\n\nIf you already have access to a private server that serves files to your users (e.g. via HTTP), then you can simply copy your repository there.\nIf your organization uses Amazon Web Services (AWS), you can also use an S3 bucket to host your repository and take advantage of the access controls set by your organization.\n\n\n\n\n\n\n\nPublic repositories in S3 buckets\n\n\n\n\n\nAlthough this use case focuses on hosting private repositories, you can of course also make repositories in S3 buckets publicly available. Alas, data storage in S3 buckets incurs cost, while other options (e.g. github-pages, CRAN, Bioconductor, etc) are free, so this might not be your preferred option.\n\n\n\nWe will assume that you have write access to an S3 bucket that is configured to serve static files via HTTP. (For a brief outline of the necessary steps, please see the appendix ). Here, I am using a bucket called drat-tutorial - but you should create / access your own bucket to follow along.\n\n\n\n\n\n\nWarning\n\n\n\nAWS S3 buckets can be configured to either be visible publicly, or access can be restricted to specific IP addresses, security groups or other AWS resources. Please make sure you have configured your bucket in a way that suits your needs.\nS3 buckets do not support the HTTPS protocol. If you require an encrypted file transfer, you might need a different solution.\n\n\nTo share our repository, we must first copy its folder to the S3 bucket, either via the AWS Console or (more conveniently) with the aws command line interface7. (If you are adventurous, you can also mount an S3 bucket as a filey system with goofys).\nAssuming you have set the necessary AWS credentials, the following aws s3 sync command copies our repository to the repo folder within drat-tutorial bucket that I created in the us-west-1 AWS region.\n\naws s3 sync ~/drat-tutorial s3://drat-tutorial/repo\n\nWe can use the aws s3 ls command to confirm the upload:\n\naws s3 ls s3://drat-tutorial/repo/\n\n                           PRE bin/\n                           PRE src/\n2023-01-21 20:28:30         58 index.html\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we make changes to our local repository, e.g. after adding new packages or package versions, we have to rerun the aws s3 sync command to copy the new files to the S3 bucket.\n\n\nNow that the files are in place, we can add our remote repository to the the list of R repositories in our R session. First, we remove the LocalRepo repository that we had added earlier, which points to the folder on our local filesystem.\n\noptions(repos = getOption(\"repos\")[\n  setdiff(names(getOption(\"repos\")), \"LocalRepo\")\n])\n\nThe we add the remote repository instead, by pointing to the URL of the S3 bucket 8.\n\ndrat::addRepo(\"S3repo\", \"http://drat-tutorial.s3.us-west-1.amazonaws.com/repo/\")\ngetOption(\"repos\")\n\n                                                   CRAN \n                          \"https://cloud.r-project.org\" \n                                                 S3repo \n\"http://drat-tutorial.s3.us-west-1.amazonaws.com/repo/\" \n\n\nLet’s try to install the toy package from our S3 drat repository:\n\ninstall.packages(\"toy\")\n\nInstalling package into '/Users/sandmann/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/wc/9tswmr4s74s0x90wqh2007300000gp/T//Rtmpg6tHJA/downloaded_packages\n\n\nSuccess! R has successfully connected to the remote repository and installed the (binary) R package."
  },
  {
    "objectID": "posts/drat/index.html#conclusions",
    "href": "posts/drat/index.html#conclusions",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe drat R package makes it extremely simple to create a CRAN-like repository.\nThe static files can be served via HTTP, making it straightforward to host the repository e.g. in an AWS S3 bucket with a restrictive access policy."
  },
  {
    "objectID": "posts/drat/index.html#appendix",
    "href": "posts/drat/index.html#appendix",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Appendix",
    "text": "Appendix\n\nCreating and configuring an S3 bucket to host static files\nThe following steps briefly outline how to create and configure an S3 bucket to act as a static web server via the AWS web interface (e.g. the AWS Console). For more details, please read the AWS S3 documentation and / or consult your local AWS expert.\n\n\n\n\n\n\nWarning\n\n\n\nStoring files on AWS S3 is not free. In this tutorial, we only upload a limited number of small files, but please don’t forget to purge them from your AWS account afterward.\n\n\n\nCreate a new bucket (skip if you already have one)\n\n\nMake sure you create the bucket in the region that works best for your organization (e.g. us-west-1 if you want to host your files in California).\nYou do not need to enable public access, stick to the defaults for your organization.\n\n\n\nCreate an S3 bucket\n\n\n\n\nNext, navigate to your bucket’s properties,\n\nscroll all the way to the bottom of the page and enable Static website hosting.\n\n(Typically) specify index.html as the Index document.\n\nUnder the Permissions tab, add a bucket policy that makes your content available within your organization\n\n\n\n\n\n\nWarning\n\n\n\nThese settings determine who can access your files. Proceed with caution to avoid inadvertently exposing your data to the world!\n\n\nFor example, the following policy grants read access to all files in the s3://drat-tutorial/ bucket to requests originating (only) from the 192.0.2.0 IP address. (Your own configuration will be different, of course.)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": [\n        \"s3:GetObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::drat-tutorial/*\"\n      ],\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"192.0.2.0/32\"\n        }\n      }\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/drat/index.html#footnotes",
    "href": "posts/drat/index.html#footnotes",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternatively, you can also create the bundle from within R using the devtools::build() command.↩︎\nYou might want to add this option to your .Rprofile file.↩︎\nOf course, you can place it anywhere you like, including e.g. network drives, as long as you can write to the directory. If you are using Windows, please remember to use backward instead of forward slashes in your paths.↩︎\nIn this tutorial, I use the :: notation to highlight which package a function originates from. Because we attached the package with the library(drat) command before, the drat:: prefix could be omitted.↩︎\nIn this tutorial, I use the :: notation to highlight in which package functions originate from. Because we attached the package with the library(drat) command before, the drat:: prefix could be omitted.↩︎\nIf you use Bioconductor, the BiocManager::repositories() specifies additional repositories that host its annotation and software packages.↩︎\nInstallation instructions.↩︎\nYou can look up the URL for your bucket in the AWS S3 console: ↩︎"
  },
  {
    "objectID": "posts/rslist-r-package/index.html",
    "href": "posts/rslist-r-package/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Whenever I deal with nested lists in R - e.g. after reading JSON documents - my code starts to resemble a jumbled mess of lapply calls. (Or, on a better day, a horrible collection of purrr::map calls).\nLuckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it’s super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won’t regret it!\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-returning/index.html",
    "href": "posts/postgres-returning/index.html",
    "title": "Simultaneously inserting records into two tables with Postgres CTEs",
    "section": "",
    "text": "Today I learned how to\n\nUse Common Table Expressions (CTEs) to simultaneously insert data into two Postgres tables and\nUse the RETURNING SQL command to retrieve automatically created fields inside the same statement.\n\nGene expression data hosted at the NCBI’s Short Read Archive (SRA) or at the European Nucleotide Archive (ENA) are a great resource. Both repositories represent information for different entities that make up a project, e.g. study, sample, experiment, run and analysis information.\n\n\n\nRelationships between entities (source: ENA)\n\n\nFor example, ENA project PRJNA818657 is an RNA-seq study with data for 25 samples. For each sample, a single sequencing library (= experiment) was prepared and sequenced in two separate runs.\nIn other words, e.g.  sample SAMN26870486 produced experiment SRX14564817, which was then analyzed in run SRR18430942 and run SRR18430943.\nOne way to capture this information in a relational database is to set up three tables - one for each entity - and then use ENA’s unique sample-, experiment- and run-identifiers as natural primary keys.\nBut what if I don’t have suitable natural keys, or simply prefer to use surrogate keys?\nToday, I learned how to\n\nINSERT a new record into a Postgres database,\nautomatically generate a primary key,\nreturn the key and\ninclude it in a subsequent INSERT statement\n\n\n\nI am using a Postgres database called test, running on the local host and connect to it with the DBI R package, via the RPostgres::Postgres() driver.\nThen I pass the returned PqConnection object to the following SQL code cells in this Quarto document.\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  dbname = \"test\", \n  host = \"localhost\")\n\n\n\n\nInitially, the database is empty, so let’s create two tables:\n\nexperiment: sample-level information\nrun: run-level information\n\nEach table will include\n\nAn auto-generated primary key (experiment_id and run_id, respectively)\nA field to record the record’s ENA accession\nA time-stamp\n\nand the run table will reference its parent experiment via the experiment_id foreign key.\n\nCREATE TABLE IF NOT EXISTS experiment (\n  experiment_id SERIAL PRIMARY KEY,\n  accession text UNIQUE,\n  timestamp timestamp default current_timestamp not null\n)\n\n\nCREATE TABLE IF NOT EXISTS run (\n    run_id SERIAL PRIMARY KEY,\n    accession text UNIQUE,\n    timestamp timestamp default current_timestamp not null,\n    experiment_id integer,\n    FOREIGN KEY(experiment_id) REFERENCES experiment(experiment_id)\n)\n\n\n\n\nNext, we use a single SQL statement to insert both the experiment and its related runs:\n\nWITH\nexp AS (\n  INSERT INTO experiment (accession) \n  VALUES ('SRX14564817') \n  RETURNING experiment_id\n),\ndata(accession) AS (\n  VALUES\n  ('SRR18430942'),\n  ('SRR18430943')\n)\nINSERT INTO run (experiment_id, accession)\nSELECT e.experiment_id, d.accession\nFROM exp e, data d\n\nWe verify that the experiment has been accessioned into the experiment table, and the same identifier has then be inserted into the run table as well\n\nSELECT e.experiment_id, e.accession AS experiment_accession, \n       r.run_id, r.accession AS run_accession\nFROM experiment e\nINNER JOIN run r ON e.experiment_id = r.experiment_id\n\n\n2 records\n\n\nexperiment_id\nexperiment_accession\nrun_id\nrun_accession\n\n\n\n\n1\nSRX14564817\n1\nSRR18430942\n\n\n1\nSRX14564817\n2\nSRR18430943\n\n\n\n\n\nLet’s examine the individual parts of this query:\n\nThe WITH command creates a Common Table Expression (CTE), e.g. \n\na temporary named result set, derived from a simple query and defined within the execution scope of a SELECT, INSERT, UPDATE, or DELETE statement.\n\nIn this example, the exp temporary result is generated by the first INSERT statement, which updates the experiment table. It returns the automatically generated experiment_id via the RETURNING command. Let’s add another accession to the experiment table and examine the returned exp table:\n\n\n  WITH\n  exp AS (\n    INSERT INTO experiment (accession) \n    VALUES ('another accession') \n    RETURNING experiment_id\n  )\n  SELECT * FROM exp\n\n\n1 records\n\n\nexperiment_id\n\n\n\n\n2\n\n\n\n\n\nAs expected, the experiment_id has been incremented for the next experiment.\n\nNext, we provide the two run accessions by passing them as VALUES to the data table.\n\n\n  WITH\n  data(accession) AS (\n    VALUES\n    ('SRR18430942'),\n    ('SRR18430943')\n  )\n  SELECT * FROM data\n\n\n2 records\n\n\naccession\n\n\n\n\nSRR18430942\n\n\nSRR18430943\n\n\n\n\n\n\nFinally, the second INSERT statement adds the two runs to the run table, by retrieving the temporary values from both the exp and data result sets.\n\nBecause the CTE is a single SQL statement, it runs within a single transaction, e.g. it is committed only at the successful completion of the whole statement.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-returning/index.html#tldr",
    "href": "posts/postgres-returning/index.html#tldr",
    "title": "Simultaneously inserting records into two tables with Postgres CTEs",
    "section": "",
    "text": "Today I learned how to\n\nUse Common Table Expressions (CTEs) to simultaneously insert data into two Postgres tables and\nUse the RETURNING SQL command to retrieve automatically created fields inside the same statement.\n\nGene expression data hosted at the NCBI’s Short Read Archive (SRA) or at the European Nucleotide Archive (ENA) are a great resource. Both repositories represent information for different entities that make up a project, e.g. study, sample, experiment, run and analysis information.\n\n\n\nRelationships between entities (source: ENA)\n\n\nFor example, ENA project PRJNA818657 is an RNA-seq study with data for 25 samples. For each sample, a single sequencing library (= experiment) was prepared and sequenced in two separate runs.\nIn other words, e.g.  sample SAMN26870486 produced experiment SRX14564817, which was then analyzed in run SRR18430942 and run SRR18430943.\nOne way to capture this information in a relational database is to set up three tables - one for each entity - and then use ENA’s unique sample-, experiment- and run-identifiers as natural primary keys.\nBut what if I don’t have suitable natural keys, or simply prefer to use surrogate keys?\nToday, I learned how to\n\nINSERT a new record into a Postgres database,\nautomatically generate a primary key,\nreturn the key and\ninclude it in a subsequent INSERT statement\n\n\n\nI am using a Postgres database called test, running on the local host and connect to it with the DBI R package, via the RPostgres::Postgres() driver.\nThen I pass the returned PqConnection object to the following SQL code cells in this Quarto document.\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  dbname = \"test\", \n  host = \"localhost\")\n\n\n\n\nInitially, the database is empty, so let’s create two tables:\n\nexperiment: sample-level information\nrun: run-level information\n\nEach table will include\n\nAn auto-generated primary key (experiment_id and run_id, respectively)\nA field to record the record’s ENA accession\nA time-stamp\n\nand the run table will reference its parent experiment via the experiment_id foreign key.\n\nCREATE TABLE IF NOT EXISTS experiment (\n  experiment_id SERIAL PRIMARY KEY,\n  accession text UNIQUE,\n  timestamp timestamp default current_timestamp not null\n)\n\n\nCREATE TABLE IF NOT EXISTS run (\n    run_id SERIAL PRIMARY KEY,\n    accession text UNIQUE,\n    timestamp timestamp default current_timestamp not null,\n    experiment_id integer,\n    FOREIGN KEY(experiment_id) REFERENCES experiment(experiment_id)\n)\n\n\n\n\nNext, we use a single SQL statement to insert both the experiment and its related runs:\n\nWITH\nexp AS (\n  INSERT INTO experiment (accession) \n  VALUES ('SRX14564817') \n  RETURNING experiment_id\n),\ndata(accession) AS (\n  VALUES\n  ('SRR18430942'),\n  ('SRR18430943')\n)\nINSERT INTO run (experiment_id, accession)\nSELECT e.experiment_id, d.accession\nFROM exp e, data d\n\nWe verify that the experiment has been accessioned into the experiment table, and the same identifier has then be inserted into the run table as well\n\nSELECT e.experiment_id, e.accession AS experiment_accession, \n       r.run_id, r.accession AS run_accession\nFROM experiment e\nINNER JOIN run r ON e.experiment_id = r.experiment_id\n\n\n2 records\n\n\nexperiment_id\nexperiment_accession\nrun_id\nrun_accession\n\n\n\n\n1\nSRX14564817\n1\nSRR18430942\n\n\n1\nSRX14564817\n2\nSRR18430943\n\n\n\n\n\nLet’s examine the individual parts of this query:\n\nThe WITH command creates a Common Table Expression (CTE), e.g. \n\na temporary named result set, derived from a simple query and defined within the execution scope of a SELECT, INSERT, UPDATE, or DELETE statement.\n\nIn this example, the exp temporary result is generated by the first INSERT statement, which updates the experiment table. It returns the automatically generated experiment_id via the RETURNING command. Let’s add another accession to the experiment table and examine the returned exp table:\n\n\n  WITH\n  exp AS (\n    INSERT INTO experiment (accession) \n    VALUES ('another accession') \n    RETURNING experiment_id\n  )\n  SELECT * FROM exp\n\n\n1 records\n\n\nexperiment_id\n\n\n\n\n2\n\n\n\n\n\nAs expected, the experiment_id has been incremented for the next experiment.\n\nNext, we provide the two run accessions by passing them as VALUES to the data table.\n\n\n  WITH\n  data(accession) AS (\n    VALUES\n    ('SRR18430942'),\n    ('SRR18430943')\n  )\n  SELECT * FROM data\n\n\n2 records\n\n\naccession\n\n\n\n\nSRR18430942\n\n\nSRR18430943\n\n\n\n\n\n\nFinally, the second INSERT statement adds the two runs to the run table, by retrieving the temporary values from both the exp and data result sets.\n\nBecause the CTE is a single SQL statement, it runs within a single transaction, e.g. it is committed only at the successful completion of the whole statement."
  },
  {
    "objectID": "posts/quarto-css/index.html",
    "href": "posts/quarto-css/index.html",
    "title": "Customizing my Quarto website",
    "section": "",
    "text": "Today I gave my blog a facelift, switching to a different theme and also including some custom CSS. Christian Gebhard’s tutorial was super helpful. Many thanks, Christian!\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog.\n\nThis is Thomas Sandmann’s personal blog, created with Quarto. I am planning to share e.g. “Things I learned today” (TIL) and other pieces of news around Computational Biology and Data Science.\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/custom-badges/index.html",
    "href": "posts/custom-badges/index.html",
    "title": "Creating custom badges for your README",
    "section": "",
    "text": "Today I learned how to create custom badges with shields.io, and how to add them to the README.md file on github.\n\nPredefined badges\nMany open source software packages display key pieces of information as badges (aka shields) in their github README, indicating e.g. code coverage, unit test results, version numbers, license, etc.\nThe shields.io website provides many different ready-to-use badges, covering topics such as test results, code coverage, social media logos, activity, and many more.\n     \nBadges can show up to date information. For example, this badge shows the last commit to the github repository for this blog: . They can be returned either in svg (recommended) or png formats, from the img.shields.io and raster.shields.io servers, respectively.\n\n\nCustom badges\nIn addition to predefined outputs, you can also generate your own, entirely custom badges. They can be static like this one  or dynamically retrieve information from a JSON endpoint of your choice.\n\n\nAdding badges to a README.md file\nTo embed badges into your README.md, simply wrap its URL in markdown and surround it with the badges: start and badges: end tags:\n&lt;!-- badges: start --&gt;\n![](https://img.shields.io/github/last-commit/tomsing1/blog)\n&lt;!-- badges: end --&gt;\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/r-update-with-rig/index.html",
    "href": "posts/r-update-with-rig/index.html",
    "title": "Updating R the easy way: using rig command line tool",
    "section": "",
    "text": "Today it was time to update the R installation on my Mac OS X system, from R 4.2.1 to 4.2.2. Luckily, with Gábor Csárdi’s rig command line tool that was a breeze.\nI had previously installed rig with brew\nbrew tap r-lib/rig\nbrew install --cask rig\nso I first checked if there were any updates available for rig itself:\nbrew upgrade --cask rig\nThis command updated rig from version 0.5.0 to 0.5.2.\nThen I listed the R versions currently installed on my system:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.1)\nAt this point, I was using R release 4.2.1. Next, I updated to the latest release\nrig install\n\n[INFO] Downloading https://cloud.r-project.org/bin/macosx/base/R-4.2.2.pkg -&gt; /tmp/rig/x86_64-R-4.2.2.pkg\n[INFO] Running installer\n[INFO] &gt; installer: Package name is R 4.2.2 for macOS\n[INFO] &gt; installer: Installing at base path /\n[INFO] &gt; installer: The install was successful.\n[INFO] Forgetting installed versions\n[INFO] Fixing permissions\n[INFO] Adding R-* quick links (if needed)\n[INFO] Setting default CRAN mirror\n[INFO] Installing pak for R 4.2 (if not installed yet)\nOnce the rig install command had completed, my system had updated itself to R version 4.2.2:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.2)\nNow a new R session starts with R 4.2.2\n&gt;R\n\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nThank you, Gábor!\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-blast-tutorial/index.html",
    "href": "posts/nextflow-blast-tutorial/index.html",
    "title": "Learning nextflow: blasting multiple sequences",
    "section": "",
    "text": "Nextflow is both a reactive workflow framework and a domain-specific language (DSL). It is gaining lots of tracking in bioinformatics thanks in large part to the nf-core open source community that develops and publishes reusable workflows for many use cases.\nTo start learning nextflow, I worked through Andrew Severin’s excellent Creating a NextFlow workflow tutorial. (The tutorial follows the older DSL1 specification of nextflow, but only a few small modifications were needed to run it under DSL2.)\nThe DSL2 code I wrote is here and these are notes I took while working through the tutorial:\n\nTo make a variable a pipeline parameter prepend it with params., then specify them in the command line:\nmain.nf:\n#! /usr/bin/env nextflow\nparams.query=\"file.fasta\"\nprintln \"Querying file $params.query\"\nshell command:\nnextflow run main.nf --query other_file.fasta\nThe -log argument directs logging to the specified file.\nnextflow -log nextflo.log run main.nf \nTo clean up intermediate files automatically upon workflow completion, use the cleanup parameter within a profile.\nprofiles {\n  standard {\n      cleanup = true\n  }\n  debug {\n      cleanup = false\n  }\n}\n\nBy convention the standard profile is implicitly used when no other\nprofile is specified by the user.\nCleaning up intermediate files precludes the use of -resume.\n\nThe nextflow.config file sets the global parameters, e.g.\n\nprocess\nmanifest\nexecutor\nprofiles\ndocker\nsingularity\ntimeline\nreport\netc\n\nContents of the work folder for a nextflow task:\n\n.command.begin is the begin script if you have one\n.command.err is useful when it crashes.\n.command.run is the full nextflow pipeline that was run, this is helpful when trouble shooting a nextflow error rather than the script error.\n.command.sh shows what was run.\n.exitcode will have the exit code in it.\n\nDisplaying help messages\nmain.nf\ndef helpMessage() {\nlog.info \"\"\"\n      Usage:\n      The typical command for running the pipeline is as follows:\n      nextflow run main.nf --query QUERY.fasta --dbDir \"blastDatabaseDirectory\" --dbName \"blastPrefixName\"\n\n      Mandatory arguments:\n       --query                        Query fasta file of sequences you wish to BLAST\n       --dbDir                        BLAST database directory (full path required)\n       [...]\n\"\"\"\n}\n\n// Show help message\nif (params.help) {\n    helpMessage()\n    exit 0\n}\nshell command:\nnextflow run main.nf --help\nThe publishDir directive accepts arguments like mode and pattern to fine tune its behavior, e.g.\noutput:\nfile(\"${label}/short_summary.specific.*.txt\")\npublishDir \"${params.outdir}/BUSCOResults/${label}/\", mode: 'copy', pattern: \"${label}/short_summary.specific.*.txt\"\nDSL2 allows piping, e.g.\nworkflow {\n  res = Channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true) |\n      runBlast\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\nAdd a timeline report to the output with\ntimeline {\n    enabled = true\n    file = \"$params.outdir/timeline.html\"\n}\n(in nextflow.config).\nAdd a detailed execution report with\nreport {\nenabled = true\nfile = \"$params.outdir/report.html\"\n}\n(in nextflow.config).\nInclude a profile-specific configuration file\nnextflow.config\nprofiles {\n    slurm { includeConfig './configs/slurm.config' }\n}\nconfigs/slurm.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 24:00:00'\n}\nand use it via nextflow run main.nf -profile slurm\nSimilarly, refer to a test profile, specified in a separate file:\nnextflow.config\ntest { includeConfig './configs/test.config' }\nAdding a manifest to nextflow.config\nmanifest {\n    name = 'isugifNF/tutorial'\n    author = 'Andrew Severin'\n    homePage = 'www.bioinformaticsworkbook.org'\n    description = 'nextflow bash'\n    mainScript = 'main.nf'\n    version = '1.0.0'\n}\nUsing a label for a process allows granular control of a process’ configuration\nmain.nf\nprocess runBlast { \n    label 'blast'\n}\nnextflow.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 02:00:00'\n    withLabel: blast { module = 'blast-plus' }\n}\n\nThe label has to be placed before the input section.\n\nLoading a module specifically for a process\nprocess runBlast {\n\n    module = 'blast-plus'\n    publishDir \"${params.outdir}/blastout\"\n\n    input:\n    path queryFile from queryFile_ch\n    .\n    .\n    . // these three dots mean I didn't paste the whole process.\n}\nEnabling docker in the nextflow.config\ndocker { docker.enabled = true }\n\nThe docker container can be specified in the process, e.g.\n\ncontainer = 'ncbi/blast'\nor\ncontainer = `quay.io/biocontainers/blast/2.2.31--pl526he19e7b1_5`\n\nWe can include additional options to pass to the container as well:\n\ncontainerOptions = \"--bind $launchDir/$params.outdir/config:/augustus/config\"\nprojectDir refers to the directory where the main workflow script is located. (It used to be called baseDir.)\nRefering to local directories from within a docker container: create a channel\n\nWorking in containers, we need a way to pass the database file location directly into the runBlast process without the need of the local path.\n\nRepeating a process over each element of a channel with each: input repeaters\nTurning a queue channel into a value channel, which can be used multiple times.\n\nA value channel is implicitly created by a process when it is invoked with a simple value.\nA value channel is also implicitly created as output for a process whose inputs are all value channels.\nA queue channel can be converted into a value channel by returning a single value, using e.g. first, last, collect, count, min, max, reduce, sum, etc. For example: the runBlast process receives three inputs in the following example:\n\nthe queryFile_ch queue channel, with multiple sequences.\nthe dbDir_ch value channel, created by calling .first(), which is reused for all elements of queryFile_ch\nthe dbName_ch value channel, which is also reused for all elements of queryFile_ch\n\n\nworkflow {\n  channel.fromPath(params.dbDir).first()\n  .set { dbDir_ch }\n\n  channel.from(params.dbName).first()\n  .set { dbName_ch }\n\n  queryFile_ch = channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true)\n     res = runBlast(queryFile_ch, dbDir_ch, dbName_ch)\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\n\n\nAdditional resources\n\nSoftware Carpentry course\nNextflow cheat sheet\nAwesome nextflow\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/CarpentryCon2018/index.html",
    "href": "posts/CarpentryCon2018/index.html",
    "title": "Greg Wilson: Late Night Thoughts on Listening to Ike Quebec (2018)",
    "section": "",
    "text": "In 2018 Greg Wilson presented the Keynote “Late Night Thoughts on Listening to Ike Quebec” at the 2018 CarpentryCon meeting, after he made the decision to move on from the Software Carpentry project he had founded in 2010. His talk is available on youtube, and this post contains my notes.\nGreg has help numerous positions in his professional life, and tackled many large projects. While there are many books providing advice on how start endeavors, he realized that there was little guidance on how to best exit them. Reflecting on his personalexperience with transitioning out of roles -\nincluding leaving Academia thrice - led Greg to formulate and share the following ten rules:\n\nBe sure you mean it.\n\nLetting go is hard.\nDon’t be in-and-out.\n\nDo it when others think it’s time.\n\nYou will be the last person to realize that it’s time to move on. They are probably right.\n\nTell people the what, when, and why.\n\nCommunicate that the succession plan is.\n\nDon’t pick your successor by yourself.\n\nAvoid picking somebody like yourself, avoid forming an old boys club.\nDiversity matters.\nChange is good.\nThis choice belongs to the community that will work with your successor.\n\nTrain people before you go.\n\nIf at all possible, arrange for some overlap.\nTake a vacation to identify information that needs to be transferred.\n\nWhen you leave, leave.\n\nContinuing in another role confuses people.\nLeave a clear space for your successor.\nPeople will contact the person, not the role.\n\nHave some fun before you go.\n\nSome back burner project, something you never got around to - if not now, when?\n\nReflect on what you learned.\n\nDon’t beat yourself up about things you didn’t get right.\nWrite down what you did well.\n\nRemember the good things, too.\n\nGive yourself some credit (even if you are Canadian).\n\nDo something next.\n\nHave a plan what to do next (not just a job).\nYou probably don’t deal well with being idle.\n\n\n\nAcademia conditions us to believe that only novel things are valuable. But many tools we use and teach are old - and useful! Measure yourself by usefulness, not novelty.\nGreg believes that the biggest impact of the Carpentries will be that They taught participants how to teach, build curricula, build communities, etc.\nNothing worth having comes without some kind of fight. You have to be on the right committee to get the decision you want.\n\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/dtrackr/index.html",
    "href": "posts/dtrackr/index.html",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "",
    "text": "Today I learned about Robert Challen’s dtrackr R package. It extends functionality from the tidyverse to track and visualize the data wrangling operations that have been applied to a dataset.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/dtrackr/index.html#tldr",
    "href": "posts/dtrackr/index.html#tldr",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "",
    "text": "Today I learned about Robert Challen’s dtrackr R package. It extends functionality from the tidyverse to track and visualize the data wrangling operations that have been applied to a dataset."
  },
  {
    "objectID": "posts/dtrackr/index.html#motivation",
    "href": "posts/dtrackr/index.html#motivation",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Motivation",
    "text": "Motivation\nPublications involving cohorts of human subjects often include flow charts describing which participants were screened, included in a specific study arm or excluded from analysis. In fact, reporting guidelines such as CONSORT, STROBE or STARD include visualizations that communcate how the participants flowed through the study.\nIf a dataset is processed with tidyverse functions, e.g. from the dplyr or tidyr R packages, methods from the dtrackr package add metadata to each step - and automatically generate a flow chart."
  },
  {
    "objectID": "posts/dtrackr/index.html#installation",
    "href": "posts/dtrackr/index.html#installation",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Installation",
    "text": "Installation\nThe dtrackr package is available from CRAN\n\nif (!requireNamespace(\"dtrackr\", quietly = TRUE)) {\n  install.packages(\"dtrackr\")\n}\nsuppressPackageStartupMessages(library(\"dplyr\"))\nsuppressPackageStartupMessages(library(\"dtrackr\"))\nlibrary(\"glue\")\nlibrary(\"GenomicDataCommons\", \n        include.only = c(\"cases\", \"results\", \"ids\", \"gdc_clinical\"))\n\nIt contains several very useful vignettes, including an example of processing clinical trial according to CONSORT guidelines."
  },
  {
    "objectID": "posts/dtrackr/index.html#retrieving-metadata-from-the-cancer-genome-atlas",
    "href": "posts/dtrackr/index.html#retrieving-metadata-from-the-cancer-genome-atlas",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Retrieving metadata from The Cancer Genome Atlas",
    "text": "Retrieving metadata from The Cancer Genome Atlas\nThe GenomicDataCommons Bioconductor Package provides an interface to search and retrieve data and metadata from the NIH Genomic Data Commons (GDC), including information from The Cancer Genome Atlas (TCGA) an international collaboration that collected molecular and clinical data on tens of thousands of human tumor samples.\nHere, we retrieve metadata on the subjects and samples available as part of TCGA, and then use dtrackr to select a (hypothetical) subset of samples for analysis.\nWe start by retrieving data on 500 cases\n\ncase_ids = cases() %&gt;% \n  results(size=500L) %&gt;% \n  ids()\nclindat = gdc_clinical(case_ids)\nnames(clindat)\n\n[1] \"demographic\" \"diagnoses\"   \"exposures\"   \"main\"       \n\n\nand obtain four data.frames: demographic, diagnoses, exposures and main with complementary pieces of metadata for each participant. Of note, the diagnoses data.frame can contain multiple rows for the same case_id, e.g. when both primary tumor and a metastasis samples were collected from the same patient.\nNext, we will wrangle it into shape and track our process with dtrackr."
  },
  {
    "objectID": "posts/dtrackr/index.html#default-options",
    "href": "posts/dtrackr/index.html#default-options",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Default options",
    "text": "Default options\nFirst, we set a few default options:\n\nold = options(\n  dtrackr.strata_glue=\"{tolower(.value)}\",\n  dtrackr.strata_sep=\", \",\n  dtrackr.default_message = \"{.count} records\",\ndtrackr.default_headline = \"{paste(.strata, ' ')}\"\n)\n\n\nclindat$demographic %&gt;%\n  comment(\"Demographic\") %&gt;%\n  track() %&gt;% \n  inner_join(\n    dplyr::select(clindat$main, case_id, disease_type),\n    by = \"case_id\", \n    .headline = \"Added disease type\",\n    .messages = c(\"{.count.lhs} records from Demographic table\",\n                  \"joined with {.count.rhs} records from Main table:\",\n                  \"{.count.out} in linked set\")\n  ) %&gt;%\n  include_any(\n    disease_type == \"Adenomas and Adenocarcinomas\" ~ \"{.included} Adenomas/ Adenocarcinomas\",\n    disease_type == \"Ductal and Lobular Neoplasms\" ~ \"{.included} Ductal and Lobular Neoplasms \",\n     disease_type == \"Gliomas\" ~ \"{.included} Gliomas\",\n    .headline = \"Included disease types\") %&gt;%\n  exclude_all(\n    age_at_index&lt;35 ~ \"{.excluded} subjects under 35\",\n    age_at_index&gt;75 ~ \"{.excluded} subjects over 75\",\n    race!=\"white\" ~ \"{.excluded} non-white subjects\",\n    .headline = \"Exclusions:\"\n  ) %&gt;%\n  group_by(disease_type, .messages=\"\") %&gt;%\n  count_subgroup(ethnicity) %&gt;%\n  status(\n    percent_male = sprintf(\"%1.2f%%\", mean(gender==\"male\") * 100),\n    .messages = c(\"male: {percent_male}\")                    \n  ) %&gt;%\n  ungroup(.messages = \"{.count} in final data set\") %&gt;%\n  flowchart()\n\n\n\n\n\n\n\n\n%0\n\n\n\n8:s-&gt;11\n\n\n\n\n\n9:s-&gt;11\n\n\n\n\n\n10:s-&gt;11\n\n\n\n\n\n5:s-&gt;8\n\n\n\n\n\n6:s-&gt;9\n\n\n\n\n\n7:s-&gt;10\n\n\n\n\n\n3:s-&gt;5\n\n\n\n\n\n3:s-&gt;6\n\n\n\n\n\n3:s-&gt;7\n\n\n\n\n\n3:e-&gt;4\n\n\n\n\n\n2:s-&gt;3\n\n\n\n\n\n1:s-&gt;2\n\n\n\n\n\n11\n\n  \n242 in final data set\n\n\n\n8\n\nadenomas and adenocarcinomas  \nmale: 64.71%\n\n\n\n9\n\nductal and lobular neoplasms  \nmale: 1.31%\n\n\n\n10\n\ngliomas  \nmale: 47.37%\n\n\n\n5\n\nadenomas and adenocarcinomas  \nhispanic or latino: 1 items\nnot hispanic or latino: 45 items\nUnknown: 5 items\n\n\n\n6\n\nductal and lobular neoplasms  \nhispanic or latino: 11 items\nnot hispanic or latino: 125 items\nnot reported: 17 items\n\n\n\n7\n\ngliomas  \nhispanic or latino: 2 items\nnot hispanic or latino: 32 items\nUnknown: 4 items\n\n\n\n3\n\nIncluded disease types\ninclusions:\n111 Adenomas/ Adenocarcinomas\n292 Ductal and Lobular Neoplasms \n41 Gliomas\n\n\n\n4\n\nExclusions:\n8 subjects under 35\n27 subjects over 75\n178 non-white subjects\n\n\n\n2\n\nAdded disease type\n500 records from Demographic table\njoined with 500 records from Main table:\n500 in linked set\n\n\n\n1\n\n  \nDemographic\n\n\n\n\n\n\nFinally, we restore the default options:\n\noptions(old)"
  },
  {
    "objectID": "posts/dtrackr/index.html#reproducibility",
    "href": "posts/dtrackr/index.html#reproducibility",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Information\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21)\n os       macOS Ventura 13.4\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-06-26\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package            * version   date (UTC) lib source\n BiocGenerics         0.46.0    2023-04-25 [1] Bioconductor\n bitops               1.0-7     2021-04-24 [1] CRAN (R 4.3.0)\n cli                  3.6.1     2023-03-23 [1] CRAN (R 4.3.0)\n crayon               1.5.2     2022-09-29 [1] CRAN (R 4.3.0)\n curl                 5.0.1     2023-06-07 [1] CRAN (R 4.3.0)\n digest               0.6.32    2023-06-26 [1] CRAN (R 4.3.0)\n dplyr              * 1.1.2     2023-04-20 [1] CRAN (R 4.3.0)\n dtrackr            * 0.4.0     2023-03-24 [1] CRAN (R 4.3.0)\n evaluate             0.21      2023-05-05 [1] CRAN (R 4.3.0)\n fansi                1.0.4     2023-01-22 [1] CRAN (R 4.3.0)\n fastmap              1.1.1     2023-02-24 [1] CRAN (R 4.3.0)\n generics             0.1.3     2022-07-05 [1] CRAN (R 4.3.0)\n GenomeInfoDb         1.36.1    2023-06-21 [1] Bioconductor\n GenomeInfoDbData     1.2.10    2023-04-30 [1] Bioconductor\n GenomicDataCommons * 1.24.2    2023-05-25 [1] Bioconductor\n GenomicRanges        1.52.0    2023-04-25 [1] Bioconductor\n glue               * 1.6.2     2022-02-24 [1] CRAN (R 4.3.0)\n hms                  1.1.3     2023-03-21 [1] CRAN (R 4.3.0)\n htmltools            0.5.5     2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets          1.6.2     2023-03-17 [1] CRAN (R 4.3.0)\n httr                 1.4.6     2023-05-08 [1] CRAN (R 4.3.0)\n IRanges              2.34.1    2023-06-22 [1] Bioconductor\n jsonlite             1.8.5     2023-06-05 [1] CRAN (R 4.3.0)\n knitr                1.43      2023-05-25 [1] CRAN (R 4.3.0)\n lifecycle            1.0.3     2022-10-07 [1] CRAN (R 4.3.0)\n magrittr             2.0.3     2022-03-30 [1] CRAN (R 4.3.0)\n pillar               1.9.0     2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig            2.0.3     2019-09-22 [1] CRAN (R 4.3.0)\n purrr                1.0.1     2023-01-10 [1] CRAN (R 4.3.0)\n R6                   2.5.1     2021-08-19 [1] CRAN (R 4.3.0)\n rappdirs             0.3.3     2021-01-31 [1] CRAN (R 4.3.0)\n Rcpp                 1.0.10    2023-01-22 [1] CRAN (R 4.3.0)\n RCurl                1.98-1.12 2023-03-27 [1] CRAN (R 4.3.0)\n readr                2.1.4     2023-02-10 [1] CRAN (R 4.3.0)\n rlang                1.1.1     2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown            2.22      2023-06-01 [1] CRAN (R 4.3.0)\n rstudioapi           0.14      2022-08-22 [1] CRAN (R 4.3.0)\n S4Vectors            0.38.1    2023-05-11 [1] Bioconductor\n sessioninfo          1.2.2     2021-12-06 [1] CRAN (R 4.3.0)\n stringi              1.7.12    2023-01-11 [1] CRAN (R 4.3.0)\n stringr              1.5.0     2022-12-02 [1] CRAN (R 4.3.0)\n tibble               3.2.1     2023-03-20 [1] CRAN (R 4.3.0)\n tidyr                1.3.0     2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect           1.2.0     2022-10-10 [1] CRAN (R 4.3.0)\n tzdb                 0.4.0     2023-05-12 [1] CRAN (R 4.3.0)\n utf8                 1.2.3     2023-01-31 [1] CRAN (R 4.3.0)\n V8                   4.3.0     2023-04-08 [1] CRAN (R 4.3.0)\n vctrs                0.6.3     2023-06-14 [1] CRAN (R 4.3.0)\n withr                2.5.0     2022-03-03 [1] CRAN (R 4.3.0)\n xfun                 0.39      2023-04-20 [1] CRAN (R 4.3.0)\n xml2                 1.3.4     2023-04-27 [1] CRAN (R 4.3.0)\n XVector              0.40.0    2023-04-25 [1] Bioconductor\n yaml                 2.3.7     2023-01-23 [1] CRAN (R 4.3.0)\n zlibbioc             1.46.0    2023-04-25 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.3/library\n [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/python-hints/index.html",
    "href": "posts/python-hints/index.html",
    "title": "Python type hints",
    "section": "",
    "text": "Today I learned about python type hints (again…) as I was tackling the first parts of pybites’ FastAPI learning track. The following resources were great to get a quick overview:\n\nCode Better with Type Hints – Part 1\nCode Better with Type Hints – Part 2\nFastAPI’s typing introduction\nPysheet: typing\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/fujita_2022/index.html",
    "href": "posts/fujita_2022/index.html",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "",
    "text": "Today I read the preprint Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex by Masahi Fujita and co-authors, published on biorXiv (Fujita et al., n.d.). The authors generated the largest brain single-nuclei RNA-seq dataset to date (that I am aware of), collecting data on dorsolateral prefrontal cortex (DLPFC) samples of 424 individuals from the ROS/MAP cohort.\nThis large sample size enabled them to assess the effect of genetic variation (e.g. single-nucleotide variants) on gene expression - one cell type at a time. The authors created pseudo-bulk gene expression profiles for each patient for 7 cell types and 81 cell subtypes.\nBecause neurons are highly abundant in the DLPFC, the largest number of nuclei originated from neurons, and the statistical power to detect eQTLs was lower in rarer cell types (e.g. microglia). This highlights the potential of enrichment methods, e.g. by fluorescent activate nuclei sorting (FANS) approached. (See e.g. (Kamath et al. 2022), who specifically enriched dopaminergic neurons or (Sadick et al. 2022), who enriched astrocytes and oligodendrocytes.)\nFujita et al were able to identify ~ 10,000 eGenes1, about half of which were shared across cell types. For example, they identified a novel eQTL (rs128648) for the APOE gene specifically in microglia.\nHaving identified novel eQTL relationships in vivo, the authors then used bulk RNA-seq measurements from a panel of induced pluripotent stem cells that had been differentiated either into neurons (iNeurons) or astrocytes (iAstrocytes) to test whether they could also observe the variants’ effects in vitro.\nDespite a relatively small sample size, a subset of eQTLs were replicated. But the the authors also point out unexpected discrepancies in the MAPT locus where they observed variant effects in the opposite direction from what they had observed by snRNA-seq.\nGene expression was significantly heritable in most cell types (except for those from which only small numbers of nuclei had been sampled). This allowed the authors to use their snRNA-seq dataset to impute cell type specific gene expression for large GWAS studies, e.g. for Alzheimer’s Disease, ALS, Parkinson’s Disease, and schizophrenia. This TWAS analysis detected e.g. 48 novel loci associated with AD in microglia, 22 of which had not been implicated previously.\nIn summary, this work by Fujita et al is an impressive achievement, demonstrating that single-cell/single-nuclei approaches have now become sufficiently scalable to power human genetics analyses.\nThe authors have already made the raw data for their study available on the AD Knowledge Portal. Thank you for sharing your data!\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/fujita_2022/index.html#footnotes",
    "href": "posts/fujita_2022/index.html#footnotes",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGene whose expression was significantly associated with one or more genetic variants (FDR &lt; 5%)↩︎"
  },
  {
    "objectID": "posts/duckdb/index.html",
    "href": "posts/duckdb/index.html",
    "title": "Querying parquet files with duckdb",
    "section": "",
    "text": "Today I learned how to access and query CSV and parquet files with duckdb, using either the duckdb command line interface or the eponymous R package\n\n\nduckdb is a relational (table-oriented) database management system (RDMS) contained in a single executable. It excels at processing tabular datasets, e.g. from CSV or Parquet files, from local file systems or remote sources.\nApache Parquet is &gt; an open source, column-oriented data file format designed for efficient data storage and retrieval.\nHere, I am highlighting how to use duckdb to query remote parquet files without the need for retrieving the full dataset first. And that’s just one of the many functionalities offered by duckdb, truly a swiss army knife in the data science toolkit!\n\n  D-M Commons, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\n\nI installed the duckdb executable on my Mac OS system with homebrew:\nbrew install duckdb\nduckdb --version\n\n\n\nBy default, duckdb will create database in memory. Like other RMDS, it supports a core set of SQL statements and expressions. In addition, extensions provide additional functionality, e.g. connecting to Postgres databases or supporting JSON data.\nCommands can either be entered interactively, provided via the -c argument or in a text file. To access remote files, we first need to install the httpsfs` extension that allows reading remote/writing remote files 1.\nduckdb -c \"INSTALL httpfs\"\nTo get started, we read a small dataset from a CSV file hosted publicly on a webserver. For brevity, we store this URL in the environmental variable REMOTE_FILE:\nREMOTE_FILE=https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n\nduckdb -c \"SELECT species, island, sex, bill_length_mm, bill_depth_mm \\\n           FROM '$REMOTE_FILE' LIMIT 5;\" \n\n┌─────────┬───────────┬─────────┬────────────────┬───────────────┐\n│ species │  island   │   sex   │ bill_length_mm │ bill_depth_mm │\n│ varchar │  varchar  │ varchar │     double     │    double     │\n├─────────┼───────────┼─────────┼────────────────┼───────────────┤\n│ Adelie  │ Torgersen │ MALE    │           39.1 │          18.7 │\n│ Adelie  │ Torgersen │ FEMALE  │           39.5 │          17.4 │\n│ Adelie  │ Torgersen │ FEMALE  │           40.3 │          18.0 │\n│ Adelie  │ Torgersen │         │                │               │\n│ Adelie  │ Torgersen │ FEMALE  │           36.7 │          19.3 │\n└─────────┴───────────┴─────────┴────────────────┴───────────────┘\n\n\nBy default, duckdb will use a temporary, in-memory database. To open or create a persistent database, simply include a path as a command line argument, e.g. duckdb path/to/my_database.duckdb\nFor example, the following command will download the remote CSV file and import it into a duckdb database and store it in the penguins.duckdb file.\n\nduckdb \\\n  -c \"CREATE TABLE penguins AS SELECT * FROM '${REMOTE_FILE}';\" \\\n  penguins.duckdb \n\nNow, we can query the local file with duckdb or explore it interactive with the tad viewer 2\n\nduckdb \\\n  -c \"SELECT * from penguins WHERE sex = 'MALE' LIMIT 5;\" \\\n  penguins.duckdb\n\n┌─────────┬───────────┬────────────────┬───────────────┬───────────────────┬─────────────┬─────────┐\n│ species │  island   │ bill_length_mm │ bill_depth_mm │ flipper_length_mm │ body_mass_g │   sex   │\n│ varchar │  varchar  │     double     │    double     │       int64       │    int64    │ varchar │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼─────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ MALE    │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ MALE    │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ MALE    │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ MALE    │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ MALE    │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴─────────┘\n\n\n\n\n\nThe NYC Taxi & Limousine Commission has collected data on public NYC taxi and for-hire vehicle (Uber, Lyft) trips, going all the way back to 2009. The data is shared in the form of parquet files, and one parquet file is created for each month of data.\nHere, I will use the Yellow Taxi Trip records from January and February 2023 as examples. Let’s store the URLs pointing to the respective parquet files in environmental variables.\nPARQUET_FILE1=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\nPARQUET_FILE2=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\nEach parquet file stores a single table of data. To get an overview of the available information, we ask duckdb to DESCRIBE it:\n\nduckdb -c \"DESCRIBE SELECT * FROM '$PARQUET_FILE1'\";\n\n┌───────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│      column_name      │ column_type │  null   │   key   │ default │  extra  │\n│        varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├───────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ VendorID              │ BIGINT      │ YES     │         │         │         │\n│ tpep_pickup_datetime  │ TIMESTAMP   │ YES     │         │         │         │\n│ tpep_dropoff_datetime │ TIMESTAMP   │ YES     │         │         │         │\n│ passenger_count       │ DOUBLE      │ YES     │         │         │         │\n│ trip_distance         │ DOUBLE      │ YES     │         │         │         │\n│ RatecodeID            │ DOUBLE      │ YES     │         │         │         │\n│ store_and_fwd_flag    │ VARCHAR     │ YES     │         │         │         │\n│ PULocationID          │ BIGINT      │ YES     │         │         │         │\n│ DOLocationID          │ BIGINT      │ YES     │         │         │         │\n│ payment_type          │ BIGINT      │ YES     │         │         │         │\n│ fare_amount           │ DOUBLE      │ YES     │         │         │         │\n│ extra                 │ DOUBLE      │ YES     │         │         │         │\n│ mta_tax               │ DOUBLE      │ YES     │         │         │         │\n│ tip_amount            │ DOUBLE      │ YES     │         │         │         │\n│ tolls_amount          │ DOUBLE      │ YES     │         │         │         │\n│ improvement_surcharge │ DOUBLE      │ YES     │         │         │         │\n│ total_amount          │ DOUBLE      │ YES     │         │         │         │\n│ congestion_surcharge  │ DOUBLE      │ YES     │         │         │         │\n│ airport_fee           │ DOUBLE      │ YES     │         │         │         │\n├───────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 19 rows                                                           6 columns │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n\nA detailed description of the columns and their values is available in the metadata dictionary. For example, the payment_type field contains “A numeric code signifying how the passenger paid for the trip.” with the following encoding:\n\n1: Credit card\n2: Cash\n3: No charge\n4: Dispute\n5: Unknown\n6: Voided trip\n\nIn January, more than three million trips were recorded, but a query to return the total number of records executes almost instantaneously - because we don’t need to download the (very large) file first:\n\nduckdb -c \"SELECT count(*) FROM '$PARQUET_FILE1'\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      3066766 │\n└──────────────┘\n\n\nThe vast majority of trips was paid for by credit card (payment type 1), and a small subset of trips was performed free of charge (payment type 3).\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM '$PARQUET_FILE1' \\\n           GROUP BY payment_type LIMIT 5\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            0 │               71743 │\n│            1 │             2411462 │\n│            2 │              532241 │\n│            3 │               18023 │\n│            4 │               33297 │\n└──────────────┴─────────────────────┘\n\n\nWe can also query across multiple parquet files, e.g. retrieving the total number of trips for both January and February 2023:\n\nduckdb -c \"SELECT count(*) FROM \\\n           read_parquet(['$PARQUET_FILE1', '$PARQUET_FILE2'])\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      5980721 │\n└──────────────┘\n\n\nWe can also copy the output of a query into a new, local parquet file. For example, the following query will copy records for 100 trips that were performed free of charge into a new free_trips.parquet parquet file in the current working directory:\n\nduckdb -c \\\n  \"COPY (SELECT * FROM '$PARQUET_FILE1' \\\n         WHERE payment_type = 3 LIMIT 100) TO 'free_trips.parquet' \\\n  (FORMAT 'parquet');\"\n\nWe can now query the local parquet file to drill deeper into this data slice:\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM 'free_trips.parquet' \\\n           GROUP BY payment_type\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            3 │                 100 │\n└──────────────┴─────────────────────┘\n\n\n\n\n\nIn addition to using the duckdb command line interface (CLI), you can also use a library for your favorite programming language. For example, the duckdb R package provides a DBI interface that enables queries from within an R session. (The duckdb python module provides similar functionality.)\n\nif (!requireNamespace(\"duckdb\", quietly = TRUE)) {\n  install.packages(\"duckdb\")\n}\nsuppressPackageStartupMessages(library(\"duckdb\"))\nsuppressPackageStartupMessages(library(\"DBI\"))\n\n\ncon &lt;- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\ndbExecute(conn = con, \"INSTALL httpfs\")\n\n[1] 0\n\n\nFor example, we can use an in-memory duckdb instance to query the one (or more) of the remote parquet files we examined above:\n\nPARQUET_FILE1 = paste0(\"https://d37ci6vzurychx.cloudfront.net/\",\n                       \"trip-data/yellow_tripdata_2023-01.parquet\")\n\n\nsql &lt;- \"SELECT payment_type, count(payment_type) \\\n        FROM read_parquet([?]) \\\n        GROUP BY payment_type LIMIT 5\";\ndbGetQuery(con, sql, list(PARQUET_FILE1))\n\n  payment_type count(payment_type)\n1            0               71743\n2            1             2411462\n3            2              532241\n4            3               18023\n5            4               33297\n\n\nAlternatively, we can also access data (including CSV and parquet files) using dbplyr and dplyr\n\nsuppressPackageStartupMessages(library(dbplyr))\nsuppressPackageStartupMessages(library(dplyr))\n\ntbl(con, PARQUET_FILE1) |&gt;\n  group_by(payment_type) |&gt;\n  count() |&gt;\n  collect()\n\n# A tibble: 5 × 2\n# Groups:   payment_type [5]\n  payment_type       n\n         &lt;dbl&gt;   &lt;dbl&gt;\n1            0   71743\n2            1 2411462\n3            2  532241\n4            3   18023\n5            4   33297\n\n\nDon’t forget to disconnect from your duckdb database at the end of your R session!\n\ndbDisconnect(con, shutdown=TRUE)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/duckdb/index.html#tldr",
    "href": "posts/duckdb/index.html#tldr",
    "title": "Querying parquet files with duckdb",
    "section": "",
    "text": "Today I learned how to access and query CSV and parquet files with duckdb, using either the duckdb command line interface or the eponymous R package\n\n\nduckdb is a relational (table-oriented) database management system (RDMS) contained in a single executable. It excels at processing tabular datasets, e.g. from CSV or Parquet files, from local file systems or remote sources.\nApache Parquet is &gt; an open source, column-oriented data file format designed for efficient data storage and retrieval.\nHere, I am highlighting how to use duckdb to query remote parquet files without the need for retrieving the full dataset first. And that’s just one of the many functionalities offered by duckdb, truly a swiss army knife in the data science toolkit!\n\n  D-M Commons, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\n\nI installed the duckdb executable on my Mac OS system with homebrew:\nbrew install duckdb\nduckdb --version\n\n\n\nBy default, duckdb will create database in memory. Like other RMDS, it supports a core set of SQL statements and expressions. In addition, extensions provide additional functionality, e.g. connecting to Postgres databases or supporting JSON data.\nCommands can either be entered interactively, provided via the -c argument or in a text file. To access remote files, we first need to install the httpsfs` extension that allows reading remote/writing remote files 1.\nduckdb -c \"INSTALL httpfs\"\nTo get started, we read a small dataset from a CSV file hosted publicly on a webserver. For brevity, we store this URL in the environmental variable REMOTE_FILE:\nREMOTE_FILE=https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n\nduckdb -c \"SELECT species, island, sex, bill_length_mm, bill_depth_mm \\\n           FROM '$REMOTE_FILE' LIMIT 5;\" \n\n┌─────────┬───────────┬─────────┬────────────────┬───────────────┐\n│ species │  island   │   sex   │ bill_length_mm │ bill_depth_mm │\n│ varchar │  varchar  │ varchar │     double     │    double     │\n├─────────┼───────────┼─────────┼────────────────┼───────────────┤\n│ Adelie  │ Torgersen │ MALE    │           39.1 │          18.7 │\n│ Adelie  │ Torgersen │ FEMALE  │           39.5 │          17.4 │\n│ Adelie  │ Torgersen │ FEMALE  │           40.3 │          18.0 │\n│ Adelie  │ Torgersen │         │                │               │\n│ Adelie  │ Torgersen │ FEMALE  │           36.7 │          19.3 │\n└─────────┴───────────┴─────────┴────────────────┴───────────────┘\n\n\nBy default, duckdb will use a temporary, in-memory database. To open or create a persistent database, simply include a path as a command line argument, e.g. duckdb path/to/my_database.duckdb\nFor example, the following command will download the remote CSV file and import it into a duckdb database and store it in the penguins.duckdb file.\n\nduckdb \\\n  -c \"CREATE TABLE penguins AS SELECT * FROM '${REMOTE_FILE}';\" \\\n  penguins.duckdb \n\nNow, we can query the local file with duckdb or explore it interactive with the tad viewer 2\n\nduckdb \\\n  -c \"SELECT * from penguins WHERE sex = 'MALE' LIMIT 5;\" \\\n  penguins.duckdb\n\n┌─────────┬───────────┬────────────────┬───────────────┬───────────────────┬─────────────┬─────────┐\n│ species │  island   │ bill_length_mm │ bill_depth_mm │ flipper_length_mm │ body_mass_g │   sex   │\n│ varchar │  varchar  │     double     │    double     │       int64       │    int64    │ varchar │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼─────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ MALE    │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ MALE    │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ MALE    │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ MALE    │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ MALE    │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴─────────┘\n\n\n\n\n\nThe NYC Taxi & Limousine Commission has collected data on public NYC taxi and for-hire vehicle (Uber, Lyft) trips, going all the way back to 2009. The data is shared in the form of parquet files, and one parquet file is created for each month of data.\nHere, I will use the Yellow Taxi Trip records from January and February 2023 as examples. Let’s store the URLs pointing to the respective parquet files in environmental variables.\nPARQUET_FILE1=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\nPARQUET_FILE2=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\nEach parquet file stores a single table of data. To get an overview of the available information, we ask duckdb to DESCRIBE it:\n\nduckdb -c \"DESCRIBE SELECT * FROM '$PARQUET_FILE1'\";\n\n┌───────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│      column_name      │ column_type │  null   │   key   │ default │  extra  │\n│        varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├───────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ VendorID              │ BIGINT      │ YES     │         │         │         │\n│ tpep_pickup_datetime  │ TIMESTAMP   │ YES     │         │         │         │\n│ tpep_dropoff_datetime │ TIMESTAMP   │ YES     │         │         │         │\n│ passenger_count       │ DOUBLE      │ YES     │         │         │         │\n│ trip_distance         │ DOUBLE      │ YES     │         │         │         │\n│ RatecodeID            │ DOUBLE      │ YES     │         │         │         │\n│ store_and_fwd_flag    │ VARCHAR     │ YES     │         │         │         │\n│ PULocationID          │ BIGINT      │ YES     │         │         │         │\n│ DOLocationID          │ BIGINT      │ YES     │         │         │         │\n│ payment_type          │ BIGINT      │ YES     │         │         │         │\n│ fare_amount           │ DOUBLE      │ YES     │         │         │         │\n│ extra                 │ DOUBLE      │ YES     │         │         │         │\n│ mta_tax               │ DOUBLE      │ YES     │         │         │         │\n│ tip_amount            │ DOUBLE      │ YES     │         │         │         │\n│ tolls_amount          │ DOUBLE      │ YES     │         │         │         │\n│ improvement_surcharge │ DOUBLE      │ YES     │         │         │         │\n│ total_amount          │ DOUBLE      │ YES     │         │         │         │\n│ congestion_surcharge  │ DOUBLE      │ YES     │         │         │         │\n│ airport_fee           │ DOUBLE      │ YES     │         │         │         │\n├───────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 19 rows                                                           6 columns │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n\nA detailed description of the columns and their values is available in the metadata dictionary. For example, the payment_type field contains “A numeric code signifying how the passenger paid for the trip.” with the following encoding:\n\n1: Credit card\n2: Cash\n3: No charge\n4: Dispute\n5: Unknown\n6: Voided trip\n\nIn January, more than three million trips were recorded, but a query to return the total number of records executes almost instantaneously - because we don’t need to download the (very large) file first:\n\nduckdb -c \"SELECT count(*) FROM '$PARQUET_FILE1'\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      3066766 │\n└──────────────┘\n\n\nThe vast majority of trips was paid for by credit card (payment type 1), and a small subset of trips was performed free of charge (payment type 3).\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM '$PARQUET_FILE1' \\\n           GROUP BY payment_type LIMIT 5\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            0 │               71743 │\n│            1 │             2411462 │\n│            2 │              532241 │\n│            3 │               18023 │\n│            4 │               33297 │\n└──────────────┴─────────────────────┘\n\n\nWe can also query across multiple parquet files, e.g. retrieving the total number of trips for both January and February 2023:\n\nduckdb -c \"SELECT count(*) FROM \\\n           read_parquet(['$PARQUET_FILE1', '$PARQUET_FILE2'])\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      5980721 │\n└──────────────┘\n\n\nWe can also copy the output of a query into a new, local parquet file. For example, the following query will copy records for 100 trips that were performed free of charge into a new free_trips.parquet parquet file in the current working directory:\n\nduckdb -c \\\n  \"COPY (SELECT * FROM '$PARQUET_FILE1' \\\n         WHERE payment_type = 3 LIMIT 100) TO 'free_trips.parquet' \\\n  (FORMAT 'parquet');\"\n\nWe can now query the local parquet file to drill deeper into this data slice:\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM 'free_trips.parquet' \\\n           GROUP BY payment_type\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            3 │                 100 │\n└──────────────┴─────────────────────┘\n\n\n\n\n\nIn addition to using the duckdb command line interface (CLI), you can also use a library for your favorite programming language. For example, the duckdb R package provides a DBI interface that enables queries from within an R session. (The duckdb python module provides similar functionality.)\n\nif (!requireNamespace(\"duckdb\", quietly = TRUE)) {\n  install.packages(\"duckdb\")\n}\nsuppressPackageStartupMessages(library(\"duckdb\"))\nsuppressPackageStartupMessages(library(\"DBI\"))\n\n\ncon &lt;- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\ndbExecute(conn = con, \"INSTALL httpfs\")\n\n[1] 0\n\n\nFor example, we can use an in-memory duckdb instance to query the one (or more) of the remote parquet files we examined above:\n\nPARQUET_FILE1 = paste0(\"https://d37ci6vzurychx.cloudfront.net/\",\n                       \"trip-data/yellow_tripdata_2023-01.parquet\")\n\n\nsql &lt;- \"SELECT payment_type, count(payment_type) \\\n        FROM read_parquet([?]) \\\n        GROUP BY payment_type LIMIT 5\";\ndbGetQuery(con, sql, list(PARQUET_FILE1))\n\n  payment_type count(payment_type)\n1            0               71743\n2            1             2411462\n3            2              532241\n4            3               18023\n5            4               33297\n\n\nAlternatively, we can also access data (including CSV and parquet files) using dbplyr and dplyr\n\nsuppressPackageStartupMessages(library(dbplyr))\nsuppressPackageStartupMessages(library(dplyr))\n\ntbl(con, PARQUET_FILE1) |&gt;\n  group_by(payment_type) |&gt;\n  count() |&gt;\n  collect()\n\n# A tibble: 5 × 2\n# Groups:   payment_type [5]\n  payment_type       n\n         &lt;dbl&gt;   &lt;dbl&gt;\n1            0   71743\n2            1 2411462\n3            2  532241\n4            3   18023\n5            4   33297\n\n\nDon’t forget to disconnect from your duckdb database at the end of your R session!\n\ndbDisconnect(con, shutdown=TRUE)"
  },
  {
    "objectID": "posts/duckdb/index.html#footnotes",
    "href": "posts/duckdb/index.html#footnotes",
    "title": "Querying parquet files with duckdb",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdditional options to parse / import CSV files is available in duckdb’s documentation↩︎\nThe tad viewer is a free tool to view CSV, Parquet, and SQLite and DuckDb database files↩︎"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html",
    "href": "posts/postgres-full-text-search/index.html",
    "title": "Full text search in Postgres - the R way",
    "section": "",
    "text": "I have been learning how to organize, search and modify data in a Postgres database by working through Anthony DeBarros’ excellent book Practical SQL.\nBecause I currently perform most of my data analyses in R, I am using the great RPostgres, DBI and glue packages to interface with Postgres - without ever leaving my R session.\nToday I learned how to create a full text search index and how to search it with one or more search terms.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "href": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "title": "Full text search in Postgres - the R way",
    "section": "Connecting to Postgres",
    "text": "Connecting to Postgres\nFor this example, I created a toy database full_text_search in my local Postgres server. I connect to it with the DBI::dbConnect command, and by passing it the RPostgres::Postgres() driver.\n\nlibrary(DBI)\nlibrary(glue)\nlibrary(RPostgres)\nlibrary(sessioninfo)\n\n# Connect to a (prexisting) postgres database called `full_text_search`\ncon &lt;- DBI::dbConnect(\n  dbname = \"full_text_search\",\n  drv = RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432L,\n  user = \"postgres\"\n  )"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "href": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating and populating a table",
    "text": "Creating and populating a table\nBecause this is a toy example, I start with a fresh table datasets. (In case it already exists from previous experimentation, I drop the table if necessary).\nLet’s define four fields for the table:\n\nid: the unique identifier\nname: the short name of each entry\ntitle: a longer title\ndescription: a paragraph describing the entry\ncreated: a date and time the entry was added to the database\n\n\n# drop the `datasets` table if it already exists\nif (DBI::dbExistsTable(con, \"datasets\")) DBI::dbRemoveTable(con, \"datasets\")\n\n# create the empty `datasets` table\nsql &lt;- glue_sql(\"\n      CREATE TABLE IF NOT EXISTS datasets (\n      id bigserial PRIMARY KEY,\n      name text,\n      title text,\n      description text,\n      created timestamp with time zone default current_timestamp not null\n    );\", .con = con)\nres &lt;- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbReadTable(con, \"datasets\")\n\n[1] id          name        title       description created    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nInitially, our new database is empty. Let’s populate them with three entries, each describing a popular dataset shipped with R’s built-in datasets package.\n\n# some example entries\nbuildin_datasets &lt;- list(\n  mtcars = list(\n    \"name\" = \"mtcars\", \n    \"title\" = \"The built-in mtcars dataset from the datasets R package.\",\n    \"description\" = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The data was extracted from the 1974 Motor Trend US magazine, and \ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973–74 models).\")\n  ), \n  airmiles = list(\n    name = \"airmiles\",\n    title = \"The built-in airmiles dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The revenue passenger miles flown by commercial airlines in the United\nStates for each year from 1937 to 1960.\")\n  ),\n  attitude = list(\n    name = \"attitude\", \n    title = \"The built-in attitude dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"From a survey of the clerical employees of a large financial\norganization, the data are aggregated from the questionnaires of the\napproximately 35 employees for each of 30 (randomly selected) departments. \nThe numbers give the percent proportion of favourable responses to seven\nquestions in each department.\")\n  )\n)\n\nNext, we loop over each element of the list and use the glue_sql() command to unpack both the names (names(dataset)) and the values of each field for this entry. Then we update the datasets table with this new information.\nAfterward, we retrieve the name and title fields to verify the correct import:\n\nfor (dataset in buildin_datasets) {\n  sql &lt;- glue_sql(\n    \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({dataset*});\", \n    .con = con)\n  res &lt;- suppressMessages(DBI::dbSendStatement(con, sql))\n  DBI::dbClearResult(res)\n}\nDBI::dbGetQuery(con, \"SELECT name, title from datasets;\")\n\n      name                                                     title\n1   mtcars  The built-in mtcars dataset from the datasets R package.\n2 airmiles The built-in airmiles dataset from the datasets R package\n3 attitude The built-in attitude dataset from the datasets R package"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#searching",
    "href": "posts/postgres-full-text-search/index.html#searching",
    "title": "Full text search in Postgres - the R way",
    "section": "Searching!",
    "text": "Searching!\nOur goal is to enable full-text search for the description field. Let’s look up the term data. To perform full-text search, both the records to search and our query need to be tokinzed first, with the to_tsvector and to_tsquery functions, respectively.\nHere is an example of the tokens that are generated:\n\nsql &lt;- glue_sql(\n  \"SELECT to_tsvector('This is a my test phrase, and what \n                       a beautiful phrase it is.')\n   to_tsquery\", con = con)\nDBI::dbGetQuery(con, sql)\n\n                          to_tsquery\n1 'beauti':10 'phrase':6,11 'test':5\n\n\nThe following query correctly returns all records whose descriptions contain the word data:\n\n# search the description field\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name\n1  1   mtcars\n2  3 attitude\n\n\nWe can enrich the output by returning the output of the ts_headline function, highlighting the location / context of the the matched term:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n\n\nWe can also combine search terms, e.g. searching for either employee or motor terms:\n\n# using multiple search terms\nterm &lt;- \"employee | motor\"  # OR\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars                from the 1974 &lt;Motor&gt; Trend US magazine\n2  3 attitude clerical &lt;employees&gt; of a large financial organization\n\n\nSimilarly, we can narrow our search by requiring both data and employee terms to appear in the same description:\n\nterm &lt;- \"data & employee\"  # AND\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  3 attitude clerical &lt;employees&gt; of a large financial organization"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-indices",
    "href": "posts/postgres-full-text-search/index.html#creating-indices",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating indices",
    "text": "Creating indices\nIn the examples above, we performed tokenization of the search term and the description field at run time, e.g. when the query was executed. As our database grows, this will soon become too cumbersome and degrade performance.\nAdding an index to our database will maintain full-text search speed even with large datasets. We have two different options:\n\nCreate an index based on an expression.\nCreate a new field to hold the output of the to_tsvector function, and then index this new field.\n\n\nCreating an expression index\nA simple way to create a full-text index is to include the to_tsvector() expression in the definition of the index itself. Here, we add a Generalized Inverted Index (GIN) index for the description column:\n\nsql = glue_sql(\n  \"CREATE INDEX description_idx ON datasets \n  USING gin(to_tsvector('english', description));\",\n  con = con\n)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nThe same type of query we issued above will now take advantage of the description_idx:\n\n# search the description field using its index\nterm &lt;- \"questioning\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                       ts_headline\n1  3 attitude responses to seven &lt;questions&gt; in each department\n\n\nThe description fields of new records, e.g those that are added later, will automatically be added to the index. Let’s create a new record for the euro dataset, for example.\n\nnew_data = list(\n  name = \"euro\", \n  title = \"The built-in euro dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"The data set euro contains the value of 1 Euro in all currencies\nparticipating in the European monetary union (Austrian Schilling ATS, \nBelgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, \nFrench Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, \nDutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were \nfixed by the European Union on December 31, 1998. To convert old prices to \nEuro prices, divide by the respective rate and round to 2 digits.\")\n)\nsql &lt;- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({new_data*});\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nThis new record will now be included in the search results for the term data, for example:\n\n# search the description field using its index\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n\n\n\n\nAdding a tokenized field for full-text searches\nAlternatively, another option is to create a new column to hold the output of the to_tsvector() function, and then to index it for future use. Let’s create a new column search_description_text:\n\n# create a column to hold tokens for full text search\nsql &lt;- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\nDBI::dbListFields(con, \"datasets\")\n\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n\n\nNext, we tokenize the descriptions field, and store the output in our new search_description_text column:\n\nsql &lt;- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 4\n\n\nHere are the tokens generated from the description of the first record, for example:\n\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n\n\nAs before, we can add an index - but this time, we index the pre-tokenized search_description_text column instead:\n\n# create the search index\nsql &lt;- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nTime to run our search again. When we search the search_description_text field, we can omit the to_tsvector() call, because its has been tokenized already:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n\n\n🚨 But beware: because we have precalculated the tokens, any new records added to the database will not automatically be processed, nor will they be indexed!\nLet’s add a final record, the morely dataset:\n\nmore_data = list(\n  name = \"morley\", \n  title = \"The built-in morley dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"A classical data of Michelson (but not this one with Morley) on \nmeasurements done in 1879 on the speed of light. The data consists of five \nexperiments, each consisting of 20 consecutive ‘runs’. The response is the speed\nof light measurement, suitably coded (km/sec, with 299000 subtracted).\")\n)\n\nTo enter this record, we not only have to populate the name, title and description fields - but also the list of tokens derived from the description in the search_description_text column. In other words, we have to execute the to_tsvector function inside our INSERT statement:\n\nsql &lt;- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*}, search_description_text)\n   VALUES ({more_data*}, to_tsvector({more_data[['description']]}));\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nNow, our query returns both the original matches and the new record:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n4  5   morley            classical &lt;data&gt; of Michelson (but not this\n\n\n\n\nChoosing between indexing strategies\nAccording to the Postgres documentation:\n\nOne advantage of the separate-column approach over an expression index is that it is not necessary to explicitly specify the text search configuration in queries in order to make use of the index. Another advantage is that searches will be faster, since it will not be necessary to redo the to_tsvector calls to verify index matches. The expression-index approach is simpler to set up, however, and it requires less disk space since the tsvector representation is not stored explicitly.\n\nThat’s it. Thanks again to Anthony DeBarros’ for his excellent introduction to Practical SQL!"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#reproducibility",
    "href": "posts/postgres-full-text-search/index.html#reproducibility",
    "title": "Full text search in Postgres - the R way",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n askpass       1.1     2019-01-13 [1] CRAN (R 4.2.0)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.2.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.2.0)\n blob          1.2.3   2022-04-10 [1] CRAN (R 4.2.0)\n cli           3.5.0   2022-12-20 [1] CRAN (R 4.2.0)\n credentials   1.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.0)\n evaluate      0.19    2022-12-13 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n hms           1.1.2   2022-08-19 [1] CRAN (R 4.2.0)\n htmltools     0.5.4   2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.2)\n jsonlite      1.8.4   2022-12-06 [1] CRAN (R 4.2.0)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.2.0)\n lubridate     1.9.0   2022-11-06 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n openssl       2.0.5   2022-12-06 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n Rcpp          1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.19    2022-12-15 [1] CRAN (R 4.2.0)\n RPostgres   * 1.4.4   2022-05-02 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n sys           3.4.1   2022-10-18 [1] CRAN (R 4.2.0)\n timechange    0.1.1   2022-11-04 [1] CRAN (R 4.2.0)\n vctrs         0.5.1   2022-11-16 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/upset_plots/index.html",
    "href": "posts/upset_plots/index.html",
    "title": "UpSet plots: comparing differential expression across contrasts",
    "section": "",
    "text": "Today I learned how to use UpSet plots to visualize the overlap between sets of differentially expressed genes.\nI often analyze RNA-seq experiments with multiple factors, e.g. different treatments, conditions, cell lines, genotypes, time points, etc. The scientific questions typically involve not just one, but multiple comparisons between experimental groups. For example:\nTo answer these questions, I typically fit a single linear model and then extract the comparisons of interest by specifying each of them as as contrast. (Check out the vignette of the excellent designmatrices Bioconductor package for details on creating design matrices and extracting contrasts.)\nAfter applying a suitable p-value / FDR threshold, each comparison / contrast yields a list of differentially expressed genes1. When the lists are long, it is difficult to assess the degree of overlap, e.g. the number of genes that were detected in multiple contrasts.\nIf the number of comparisons is small (say &lt; 5), then a Venn diagram is an excellent way of displaying how these sets of genes overlap. But when the number of sets increases, so does the number of intersections - and Venn diagrams soon become hard to draw (and interpret).\nUpset plots can be used to clearly visualize larger numbers of sets. Here, I am using the airway Bioconductor dataset, an RNA-Seq experiment on four human airway smooth muscle cell lines treated with dexamethasone, to illustrate how to\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/upset_plots/index.html#footnotes",
    "href": "posts/upset_plots/index.html#footnotes",
    "title": "UpSet plots: comparing differential expression across contrasts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBoth Venn diagrams and upset plots operate on sets, e.g. they require that a hard threshold has been applied to the results of a differential expression analysis. That’s problematic, because p-values themselves display high variability and dichotomizing quantitative information looses information.↩︎"
  },
  {
    "objectID": "posts/aws-export-credentials/index.html",
    "href": "posts/aws-export-credentials/index.html",
    "title": "Refreshing & exporting temporary AWS credentials",
    "section": "",
    "text": "To increase security when interacting with AWS services, the AWS IAM Identity Center (formerly known as AWS SSO) generates temporary credentials for different AWS roles.\nToday I learned how to configure and refresh these credentials in the command line, as well how to export them either as environmental variables or write them to the credentials file where tools that do not interact with AWS SSO natively can access them.\n\nConfiguring an AWS SSO profile\nFirst, we need to configure a named profile for use with AWS SSO. The following AWS CLI version 2 command will interactively walk you through the necessary steps:\naws configure sso\nThe information you provide will be written to the config file, located in the ~/.aws directory on Mac OS. Here is an example:\n[profile my-dev-profile]\nsso_start_url = https://my-sso-portal.awsapps.com/start\nsso_region = us-east-1\nsso_account_id = 123456789011\nsso_role_name = readOnly\nregion = us-west-2\noutput = json\n\n\nLogging into the AWS SSO profile\nNow we can log into AWS SSO and request temporary credentials:\naws sso login --profile my-dev-profile\nThis command will try to open a web browser for you and prompt you to confirm the login. Alternatively, you can copy & paste the displayed URL and manually enter the confirmation code output by the command.\nIf the login was successful, you can now adopt the my-dev-profile when using the AWS CLI, e.g.\naws s3 ls --profile my-dev-profile\nThe AWS SSO endpoint recognizes many environmental variables that you can use to specify defaults, e.g.\n\nAWS_PROFILE: The profile to use (e.g. my-dev-profile)\nAWS_SHARED_CREDENTIALS_FILE: the location of the shared credentials files (default on Mac OS: ~/.aws/.credentials)\nAWS_CONFIG_FILE: the location of the AWS CLI configuration file (default on Mac OS: ~/.aws.config)\n\n\n\nAccessing temporary credentials\nThe AWS CLI and many of the AWS SKDs will automatically detect and use SSO credentials. But other tools might not (yet) be compatible with this authentication route. Instead, they might\n\nread credentials for a profile from the credentials file\nrely on environmental variables, e.g. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n\nTo expose the temporary credentials, Ben Kehoe has made the aws-export-credentials tool available.\n\n\nInstalling aws-export-credentials\nThe recommended way to install aws-export-credentials is via pipx because it will automatically make it available in your PATH.\n\nIf you don’t have pipx available on your system, install it first.\nNext, install aws-export-credentials by executing the following steps in your shell:\n\npipx ensurepath  # in case you haven't run this before\npipx install aws-export-credentials\naws-export-credentials --version  # verify the installation\n\n\nUpdating the credentials file\nAt the beginning of your workday - or whenever needed - run the following set of commands. (Replace the SSO profile with the one you want to adopt.)\nPROFILE=\"my-dev-profile\"\n\n# retrieve new credentials from AWS\naws sso login --profile \"${PROFILE}\"\n\n# write the temporary credentials to the ~/.aws/credentials file\naws-export-credentials \\\n  --profile \"${PROFILE}\" \\\n  --credentials-file-profile \"${PROFILE}\"\nThis will refresh the credentials (via aws sso login) and then write them to the my-dev-profile profiles in the ~/.aws/.credentials file. Now we can access them e.g. in the aws.s3 R package:\nlibrary(aws.s3)\nlibrary(aws.signature)\naws.signature::use_credentials(profile = \"my-dev-profile\")\naws.s3::bucketlist()\n\n\nExposing environmental variables\nSome tools only recognize environmental variables. Luckily, aws-export-credentials can automate this process, too:\nexport $(aws-export-credentials --profile my-dev-profile --env-export)\nwill export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables in your shell session.\n\n\nSourcing credentials with an external process\nFinally, you can also include a command that looks up credentials as a credential_process in your config file. (More information here) But that’s not a use case I have explored, yet.\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/lemur/index.html",
    "href": "posts/lemur/index.html",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "",
    "text": "This week, Constantin Ahlmann-Eltze and Wolfgang Huber published a preprint describing LEMUR, a new approach to analyzing single-cell experiments that include samples from multiple conditions, e.g. drug treatments, disease-status, etc.\nTo date, such analyses often involve two separate steps, e.g.\nIn contrast, LEMUR considers the continuous latent space the individual cells occupy, incorporating the design of the experiment, and then performs differential expression analysis in this embedding space.\nAn R package implementing LEMUR is available from github and includes an example dataset 1.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/lemur/index.html#ellwanger-et-al-comparing-trem2-wildtype-and-knock-out-mouse-microglia",
    "href": "posts/lemur/index.html#ellwanger-et-al-comparing-trem2-wildtype-and-knock-out-mouse-microglia",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Ellwanger et al: Comparing Trem2 wildtype and knock-out mouse microglia",
    "text": "Ellwanger et al: Comparing Trem2 wildtype and knock-out mouse microglia\nHere, I am exploring LEMUR by examining scRNA-seq data published by Ellwanger et al, 2021, who injected three strains of 5XFAD mice, a murine model of familial Alzheimer’s Disease, either\n\ncarrying the wild-type (WT) Trem2 gene,\ncarrying the R47H Trem2 variant, believed to be a loss-of-function variant,\nor completely lacking Trem2 expression\n\nwith either a Trem2 agonist (hT2AB) or a negative control antibody (hIgG1).\n48 hours later, the authors isolated CD45-positive 2 cells from the cortex and performed single-cell RNA-seq analysis using the 10X Genomics platform.\n\nRetrieving the data\n\nlibrary(dplyr)\nlibrary(Matrix)\nlibrary(org.Mm.eg.db)\nlibrary(patchwork)\nlibrary(purrr)\nlibrary(readr)\nlibrary(scater)\nlibrary(SingleCellExperiment)\nlibrary(tidyr)\n\nEllwanger et al made both raw and processed data available via the NCBI GEO and SRA repositories under GEO accession GSE156183.\nThey also included complete metadata for each cell, making this a great dataset for re-analysis.\nLet’s start by retrieving the\n\nprocessed counts (500 Mb) and the\ncell metadata (13 Mb)\n\nfiles from GEO and store them in a temporary directory:\n\ntemp_dir &lt;- file.path(tempdir(), \"ellwanger\")\ndir.create(temp_dir, showWarnings = FALSE, recursive = TRUE)\n\noptions(timeout = 360)\nurl_root &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=\",\n                   \"GSE156183&format=file&file=GSE156183%5F\")\n\nraw_counts &lt;- file.path(temp_dir, \"counts.mtx.gz\")\ndownload.file(\n  paste0(url_root, \"RAW%2Emtx%2Egz\"), \n  destfile = raw_counts)\n\ncell_metadata &lt;- file.path(temp_dir, \"cell_metadata.tsv.gz\")\ndownload.file(\n  paste0(url_root, \"Cell%5Fmetadata%2Etsv%2Egz\"), \n  destfile = cell_metadata)\n\nand read the sparse count matrix into our R session:\n\nm &lt;- Matrix::readMM(raw_counts)\ncell_anno &lt;- readr::read_tsv(cell_metadata, show_col_types = FALSE)\nstopifnot(nrow(cell_anno) == ncol(m))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUnfortunately, the GSE156183_Feature_metadata.tsv.gz feature (= gene) annotation file the authors deposited with GEO actually contains cell annotations. But luckily, they also deposited counts matrices in TSV format for each sample, which include the ENSEMBL gene identifier for each row.\nHere, I download the TAR archive that contains all of the TSV files, and then extract the gene identifiers from one of the files so I can add them to the experiment-wide raw count matrix.\n\nselected_sample &lt;- \"GSM4726219_RAW-R47H-male-IgG-rep2.tsv.gz\"\ntar_archive &lt;- file.path(temp_dir, \"RAW.tar\")\ndownload.file(\n  \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE156183&format=file\",\n  destfile = tar_archive)\nutils::untar(tar_archive, files = selected_sample, exdir = tempdir())\ngene_ids &lt;- readr::read_tsv(file.path(tempdir(), selected_sample), \n                            col_select = any_of(\"feature_id\"),\n                            show_col_types = FALSE) %&gt;%\n  dplyr::pull(feature_id)\nstopifnot(length(gene_ids) == nrow(m))\nrow.names(m) &lt;- gene_ids\n\n\n\n\n\n\nCreating a SingleCellExperiment object\nNow I have all the pieces of information required to create a SingleCellExperiment:\n\nthe raw counts (in the form of a sparse matrix),\nthe cell annotations (in the form of a data.frame)\nthe two UMAP dimensions used by the authors (included in the cell metadata).\n\nI choose to retain only a subset of the (many) cell-level annotation columns, add gene symbols as row annotations, extract the UMAP coordinates into a separate matrix - and store all of it in the sce object.\nNext, I am removing cells without an assigned cell type, and also add a coarser cell type annotation that collapses the different microglia states reported by the authors into a single category. Finally, I filter genes without a valid gene symbol and add an assay slot with the normalized log2 counts.\n\ncol_data &lt;- cell_anno %&gt;%\n  dplyr::select(cell_id, celltype, sample, sex, genotype, treatment, \n                starts_with(\"QC.\")\n  ) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"cell_id\")\ncolnames(m) &lt;- row.names(col_data)\n\nrow_data &lt;- data.frame(\n  symbol = AnnotationDbi::mapIds(org.Mm.eg.db, keys = gene_ids,\n                                 column = \"SYMBOL\", keytype = \"ENSEMBL\"),\n  row.names = gene_ids\n)\n\numap &lt;- cell_anno %&gt;%\n  dplyr::select(ends_with(\"CD45pos\")\n  ) %&gt;%\n  as.matrix()\nrow.names(umap) &lt;- colnames(m)\ncolnames(umap) &lt;- paste(\"UMAP\", seq.int(ncol(umap)))\n\nsce &lt;- SingleCellExperiment(\n  assays = list(counts = m),\n  rowData = row_data,\n  colData = col_data,\n  reducedDims = list(UMAP = umap)\n)\n\nsce &lt;- sce[, !is.na(sce$celltype)]\nsce$celltype_coarse &lt;- dplyr::case_when(\n    grepl(x = sce$celltype, pattern = \"Microglia\") ~ \"Microglia\",\n    TRUE ~ sce$celltype\n  )\nsce$treatment &lt;- factor(sce$treatment, levels = c(\"IgG\", \"hT2AB\"))\nsce$mg_type &lt;- factor(sub(\"Microglia.\", \"\", sce$celltype, fixed = TRUE))\nsce &lt;- sce[!is.na(rowData(sce)$symbol), ]\nsce &lt;- logNormCounts(sce)\n\nrm(list = c(\"m\", \"cell_anno\", \"gene_ids\", \"row_data\", \"col_data\"))\n\nThis SingleCellExperiment object is now ready for downstream analysis."
  },
  {
    "objectID": "posts/lemur/index.html#subsetting-the-experiment-to-samples-of-interest",
    "href": "posts/lemur/index.html#subsetting-the-experiment-to-samples-of-interest",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Subsetting the experiment to samples of interest",
    "text": "Subsetting the experiment to samples of interest\nThis study contains multiple experimental variables, e.g. each sample is annotated with one of the three genotypes, one of two treatments and the sex for each mouse.\nHere, I will focus only on the difference between TREM2 wildtype and TREM2 knock-out animals treated with the IgG control antibody. Only female knock-out animals were included in the study, so I exclude the male animals from the other strain as well.\n\nsce &lt;- sce[, which(sce$genotype %in% c(\"TREM2_CV-5XFAD\", \"Trem2_KO-5XFAD\"))]\nsce &lt;- sce[, which(sce$sex == \"female\" & sce$treatment == \"IgG\")]\nwith(colData(sce), table(genotype, treatment))\n\n                treatment\ngenotype          IgG hT2AB\n  TREM2_CV-5XFAD 3781     0\n  Trem2_KO-5XFAD 8911     0\n\n\nAfter subsetting, the experiment now contains 5 samples:\n\ncolData(sce) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::select(sample, treatment, genotype) %&gt;%\n  dplyr::distinct() %&gt;%\n  tibble::remove_rownames()\n\n              sample treatment       genotype\n1 CV-female-IgG-rep1       IgG TREM2_CV-5XFAD\n2 CV-female-IgG-rep2       IgG TREM2_CV-5XFAD\n3 KO-female-IgG-rep1       IgG Trem2_KO-5XFAD\n4 KO-female-IgG-rep2       IgG Trem2_KO-5XFAD\n5 KO-female-IgG-rep3       IgG Trem2_KO-5XFAD\n\n\nAt this point, I can reproduce e.g. a version of Figure 3E from the original paper, using the UMAP coordinates and cell type labels provided by the authors. (My version of the figure only includes cells from the selected subset of samples, not all cells captured in the study.)\n\ncolors &lt;- c(\"Microglia\" = \"darkgrey\",\n            \"T cells\" = \"skyblue\",\n            \"Macrophages\" = \"firebrick\",\n            \"MO:T\" = \"darkgreen\",\n            \"Dendritic cells\" = \"green\",\n            \"Monocytes\" = \"orange\",\n            \"B cells\" = \"navy\",\n            \"Neutrophils\" = \"darkblue\",\n            \"HCS\" = \"grey\", \n            \"Fibroblasts\" = \"yellow\")\nscater::plotReducedDim(sce, \"UMAP\", colour_by = \"celltype_coarse\") +\n  scale_color_manual(values = colors, name = \"Cell type\")\n\n\n\n\nBecause Ellwanger et al captured all cells with CD45 expression, the dataset includes other immune cell types besides microglia. Let’s remove those to focus only on the latter.\n\nsce &lt;- sce[, sce$celltype_coarse == \"Microglia\"]\nsce$mg_type &lt;- factor(\n  sce$mg_type, \n  levels = c(\"Resting\", \"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"IFN-R\", \"DAM\", \n             \"MHC-II\", \"Cyc-M\"))\n\nMost microglial states were captured in animals from both genotypes:\n\nmg_colors &lt;- c(\n  \"Cyc-M\" = \"navy\",\n  \"DAM\" = \"darkgreen\",\n  \"IFN-R\" = \"#C12131\", \n  \"MHC-II\" = \"green\",\n  \"Resting\" = \"grey50\",\n  \"t1\" = \"#FDF5EB\",   \n  \"t2\" =  \"#FFE2C0\", \n  \"t3\" = \"#FFC08E\",\n  \"t4\" = \"#FE945C\",\n  \"t5\" =  \"#EC5D2F\",\n  \"t6\" = \"#C12131\"\n)\nscater::plotReducedDim(sce, \"UMAP\", colour_by = \"mg_type\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  facet_wrap(~ sce$genotype)"
  },
  {
    "objectID": "posts/lemur/index.html#differential-expression-analysis-with-lemur",
    "href": "posts/lemur/index.html#differential-expression-analysis-with-lemur",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Differential expression analysis with lemur",
    "text": "Differential expression analysis with lemur\nNow I am ready to explore the lemur R package to ask: “which neighborhoods show for differential expression between samples from WT and knock-out animals?”\nThe following steps closely follow the examples outlined on the lemor github repository’s README - many thanks for the great documentation, Constantin! (All mistakes and misunderstandings in this post are my own, as always.)\n\nDependencies & installation\nFollowing the instructions from the lemu github repository I then installed the latest version of the glmGamPoi package, and then the lemur package itself from their github repositories.\nTo harmonize results across batches (in this case: samples), I will use harmony, so I need to install it from its github repository as well.\n\nremotes::install_github(\"const-ae/glmGamPoi\")\nremotes::install_github(\"const-ae/lemur\")\nremotes::install_github(\"immunogenomics/harmony\")\n\n\n\nSubsetting the experiment\n\nlibrary(lemur)\nn_cells &lt;- 1000L\n\nTo speed up my exploration of the LEMUR workflow, I subset the experiment to 1000 random cells from each of the two genotypes.\n\nset.seed(1L)\ngenotypes &lt;- unique(sce$genotype)\nselected_cells &lt;- as.vector(sapply(genotypes, \\(g) {\n  sample(which(sce$genotype == g), n_cells)\n}))\n\nAs expected, most microglial states described in the paper remain represented in the downsampled dataset:\n\ntable(sce$celltype[selected_cells], sce$genotype[selected_cells])\n\n                   \n                    TREM2_CV-5XFAD Trem2_KO-5XFAD\n  Microglia.Cyc-M               17             32\n  Microglia.DAM                122            127\n  Microglia.IFN-R               46             33\n  Microglia.MHC-II              16              0\n  Microglia.Resting            101            117\n  Microglia.t1                 145            133\n  Microglia.t2                 119             71\n  Microglia.t3                  84            140\n  Microglia.t4                 161             93\n  Microglia.t5                 132            209\n  Microglia.t6                  57             45\n\n\n\nscater::plotReducedDim(\n  sce[, selected_cells], \"UMAP\", colour_by = \"mg_type\") +\n  labs(title = sprintf(\"Subsampled to %s microglia\", length(selected_cells))) +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  facet_wrap(~ sce$genotype[selected_cells])\n\n\n\n\n\n\nFitting the LEMUR model\nNext, I fit the latent embedding multivariate regression (LEMUR) model with the lemur() function. Because the dataset is relatively homogeneous, e.g. it contains only microglia, I chose to consider only 25 Principal Components and used 15 dimensions for the LEMUR embedding (e.g. the default number).\n\nfit &lt;- lemur::lemur(sce[, selected_cells], design = ~ genotype,\n                    n_ambient = 25, n_embedding = 15, verbose = FALSE)\n\nBecause each sample was processed in a separate channel of the 10X Genomics microfluidics device, I am aligning the embeddings of similar cell clusters using harmony.\n\nfit &lt;- lemur::align_harmony(fit, stretching = FALSE)\nfit\n\nclass: lemur_fit \ndim: 23870 2000 \nmetadata(12): n_ambient n_embedding ... alignment_design\n  alignment_design_matrix\nassays(1): expr\nrownames(23870): ENSMUSG00000051951 ENSMUSG00000025900 ...\n  ENSMUSG00000094915 ENSMUSG00000079808\nrowData names(1): symbol\ncolnames(2000): CELL21559 CELL21072 ... CELL45651 CELL52059\ncolData names(22): celltype sample ... celltype_coarse mg_type\nreducedDimNames(2): linearFit embedding\nmainExpName: NULL\naltExpNames(0):\n\n\nThe returned lemur_fit object contains the embedding matrix, the latent space in which the differential expression analysis is performed.\n\ndim(fit$embedding)  # 15 dimensions, as specified above\n\n[1]   15 2000\n\n\nLet’s plot the first two dimensions against each other, coloring each cell by the microglial state Ellwanger et al identified through Louvain clustering. (This information has not been used by lemur):\n\n# plot dim 1 vs dim 2\nscater::plotReducedDim(\n  fit, \"embedding\", colour_by = \"mg_type\", shape_by = \"genotype\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  labs(\n    title = \"Embedding after accounting for genotype\",\n    subtitle = sprintf(\"Subsampled to %s microglia\", length(selected_cells)))\n\n\n\n\nCells group by Ellwanger et al’s subtype labels, and cells from both genotypes are intermixed. We can obtain an alternative visualization by arranging the cells in two dimensions using Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP):\n\n# run UMAP on the embedding\numap &lt;- uwot::umap(t(fit$embedding))\ncolnames(umap) &lt;- c(\"UMAP 1\", \"UMAP 2\")\nreducedDim(fit, \"UMAP\") &lt;- umap\n\n\nscater::plotReducedDim(\n  fit, \"UMAP\", colour_by = \"mg_type\", shape_by = \"genotype\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  labs(\n    title = \"Embedding after accounting for genotype (UMAP)\",\n    subtitle = sprintf(\"Subsampled to %s microglia\", length(selected_cells))) +\n  facet_wrap(~ colData(fit)$genotype)\n\n\n\n\n\n\nTesting for differential expression\nNext, the test_de function performs a differential expression analysis for locations in the embedding - by default, it will estimate it for each location an original cell was mapped to.\nThe find_de_neighborhoods function accepts the original counts and will estimate the log2 fold change for each neighborhood, based on aggregating the counts to pseudobulk measures across the cells in each neighborhood.\n\nfit &lt;- test_de(\n  fit, \n  contrast = cond(genotype = \"Trem2_KO-5XFAD\") - \n    cond(genotype = \"TREM2_CV-5XFAD\"))\nneighborhoods &lt;- find_de_neighborhoods(\n  fit, \n  counts = counts(sce)[, selected_cells],\n  group_by = vars(sample, genotype),\n  include_complement = FALSE) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::arrange(pval) %&gt;%\n  dplyr::left_join(\n    tibble::rownames_to_column(as.data.frame(rowData(fit)), \"gene_id\"), \n    by = c(name = \"gene_id\")) %&gt;%\n  dplyr::select(symbol, everything())\n\nThe neighborhoods data.frame contains differential expression statistics for each gene.\n\nhead(neighborhoods)\n\n# A tibble: 6 × 12\n  symbol name       region indices n_cells   mean     pval adj_p…¹ f_sta…²   df1\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  &lt;I&lt;lis&gt;   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 Fxyd5  ENSMUSG00… 1      &lt;int&gt;      1809 -0.251 6.09e-10 1.45e-5   118.      1\n2 Il4i1  ENSMUSG00… 1      &lt;int&gt;      1069 -0.181 2.40e- 9 2.87e-5   101.      1\n3 Lpl    ENSMUSG00… 1      &lt;int&gt;       830 -0.469 5.82e- 9 4.63e-5    90.6     1\n4 Cd74   ENSMUSG00… 1      &lt;int&gt;       611 -1.12  9.25e- 9 5.52e-5    85.7     1\n5 Axl    ENSMUSG00… 1      &lt;int&gt;      1584 -0.257 1.78e- 8 8.49e-5    79.2     1\n6 H2-Aa  ENSMUSG00… 1      &lt;int&gt;      1172 -0.247 6.81e- 8 2.71e-4    67.1     1\n# … with 2 more variables: df2 &lt;dbl&gt;, lfc &lt;dbl&gt;, and abbreviated variable names\n#   ¹​adj_pval, ²​f_statistic\n\n\nA volcano plot shows that we recovered a number of genes with differential expression in one or more neighborhoods (after accounting for multiple testing):\n\n# volcano plot\nneighborhoods %&gt;%\n  ggplot(aes(x = lfc, y = -log10(pval))) +\n    geom_point(aes(color  = adj_pval &lt; 0.1), alpha = 0.5) +\n    labs(title = \"Volcano plot of the neighborhoods\") + \n  scale_color_manual(values = c(\"TRUE\" = \"firebrick\", \"FALSE\" = \"grey\")) +\n  theme_bw() + \n  theme(panel.grid = element_blank())\n\n\n\n\nFor example, transcripts of the Lipoprotein lipase (Lpl) gene are generally expressed at lower levels in the Trem2 knock-out than in wildtype samples.\nBut there is also evidence for stronger differences in expression in microglia that have adopted specific states. The largest log2 fold changes are observed in Cyc-M, and DAM microglia, while the Resting microglia are mainly excluded from the neighborhood detected by lemur.\n\nsel_gene &lt;- row.names(sce)[which(rowData(sce)$symbol == \"Lpl\")]\n\nneighborhood_coordinates &lt;- neighborhoods %&gt;%\n  dplyr::filter(name == sel_gene) %&gt;%\n  dplyr::mutate(cell_id = purrr:::map(indices, \\(idx) colnames(fit)[idx])) %&gt;%\n  tidyr::unnest(c(indices, cell_id)) %&gt;%\n  dplyr::left_join(as_tibble(umap, rownames = \"cell_id\"), by = \"cell_id\") %&gt;%\n  dplyr::select(name, cell_id, `UMAP 1`, `UMAP 2`)\n\np1 &lt;- as_tibble(umap) %&gt;%\n  mutate(expr = assay(fit, \"DE\")[sel_gene, ]) %&gt;%\n  ggplot(aes(x = `UMAP 1`, y = `UMAP 2`)) +\n  scale_color_gradient2() +\n  geom_point(aes(color = expr)) +\n  geom_density2d(data = neighborhood_coordinates, breaks = 0.1, \n                 contour_var = \"ndensity\", color = \"black\") +\n  labs(title = rowData(sce)[sel_gene, \"symbol\"]) + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\np2 &lt;- as_tibble(umap) %&gt;%\n  dplyr::bind_cols(as.data.frame(colData(fit))) %&gt;%\n  ggplot(aes(x = `UMAP 1`, y = `UMAP 2`)) +\n  geom_point(aes(color = mg_type)) +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  geom_density2d(data = neighborhood_coordinates, breaks = 0.1, \n                 contour_var = \"ndensity\", color = \"black\") +\n  labs(title = \"Microglia states\") + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\np1 + p2\n\n\n\n\nNext, I examine each microglial subtype separately, and split cells according to whether they fall into a neighborhood with significant differential Lpl expression (DE) or not (not DE).\nMost Cyc-M and DAM microglia are located in neighborhoods with reduced Lpl expression in knock-out samples, e.g. the majority of these cells is in the DE column. The opposite is true for Resting microglia: nearly all of them are outside the significant Lpl differential expression neighborhood.\nThe transitional subtypes, t1 (most similar to resting microglia) to t6 (most similar to DAM, Cyc-M or IFN-R) fall in between, with a gradual increase along their proposed differentiation sequence.\n\nwithin_neighborhood &lt;- neighborhoods %&gt;%\n  dplyr::filter(name == sel_gene) %&gt;% \n  dplyr::pull(indices)\n\ncolData(fit)$neighborhood &lt;- ifelse(\n  seq.int(ncol(fit)) %in% within_neighborhood[[1]], \"DE\", \"not DE\")\n\ndata.frame(\n  colData(fit)\n) %&gt;%\n  ggplot(aes(x = neighborhood, fill = mg_type)) + \n  geom_bar(stat = \"count\", color = \"black\", linewidth = 0.2) +\n  facet_wrap(~ mg_type) +\n  labs(title = rowData(sce)[sel_gene, \"symbol\"], \n       x = element_blank(),\n       y = \"Cells per neighborhood\") +\n  scale_fill_manual(values = mg_colors, name = element_blank()) + \n  theme_linedraw(14) + \n  theme(panel.grid = element_blank())\n\n\n\n\nFinally, we can plot the estimated Lpl log2 fold changes for each cells annotated with the various subtype labels. This confirms the observations outlined above, providing an example of how insights from LEMUR can be combined with coarser, clustering-based insights.\n\ndata.frame(\n  expr = assay(fit, \"DE\")[sel_gene, ],\n  colData(fit)\n) %&gt;%\n  ggplot(aes(x = mg_type, y = expr, fill = mg_type)) + \n  geom_boxplot() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_fill_manual(values = mg_colors, name = element_blank()) + \n  labs(title = rowData(sce)[sel_gene, \"symbol\"],\n       y = \"Genotype effect (log2 FC)\", \n       x = element_blank()) +\n  theme_linedraw(14) + \n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/lemur/index.html#conclusions",
    "href": "posts/lemur/index.html#conclusions",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Conclusions",
    "text": "Conclusions\n\nThis dataset starts off with a relatively homogeneous cell population, e.g. it is designed to examine subtle differences within a single cell type (microglia.)\nRemoving Trem2 activity is known to shift the composition of the microglial subsets, e.g. depleting DAM microglia and increasing the frequency of Resting in the knock-out versus the wildtype samples. This adds an additional challenge to the task of identifying differential expression.\nThe lemur() package nevertheless successfully identified genes that track the differential expression of the microglial sub-states reported by Ellwanger et al. (Even with only a subsample of the data.)\nI am looking forward to exploring LEMUR in future datasets, e.g. those examining the effects of drug perturbation in single-nuclei RNA-seq datasets sampling a large variety of CNS cell types.\n\n\n\nReproducibility\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-03-12\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date (UTC) lib source\n AnnotationDbi        * 1.60.0   2022-11-01 [1] Bioconductor\n askpass                1.1      2019-01-13 [1] CRAN (R 4.2.0)\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n beachmat               2.14.0   2022-11-01 [1] Bioconductor\n beeswarm               0.4.0    2021-06-01 [1] CRAN (R 4.2.0)\n Biobase              * 2.58.0   2022-11-01 [1] Bioconductor\n BiocGenerics         * 0.44.0   2022-11-01 [1] Bioconductor\n BiocNeighbors          1.16.0   2022-11-01 [1] Bioconductor\n BiocParallel           1.32.4   2022-12-01 [1] Bioconductor\n BiocSingular           1.14.0   2022-11-01 [1] Bioconductor\n Biostrings             2.66.0   2022-11-01 [1] Bioconductor\n bit                    4.0.5    2022-11-15 [1] CRAN (R 4.2.0)\n bit64                  4.0.5    2020-08-30 [1] CRAN (R 4.2.0)\n bitops                 1.0-7    2021-04-24 [1] CRAN (R 4.2.0)\n blob                   1.2.3    2022-04-10 [1] CRAN (R 4.2.0)\n cachem                 1.0.6    2021-08-19 [1] CRAN (R 4.2.0)\n cli                    3.5.0    2022-12-20 [1] CRAN (R 4.2.0)\n codetools              0.2-18   2020-11-04 [2] CRAN (R 4.2.2)\n colorspace             2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n cowplot                1.1.1    2020-12-30 [1] CRAN (R 4.2.0)\n crayon                 1.5.2    2022-09-29 [1] CRAN (R 4.2.0)\n credentials            1.3.2    2021-11-29 [1] CRAN (R 4.2.0)\n DBI                    1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n DelayedArray           0.24.0   2022-11-01 [1] Bioconductor\n DelayedMatrixStats     1.20.0   2022-11-01 [1] Bioconductor\n digest                 0.6.31   2022-12-11 [1] CRAN (R 4.2.0)\n dplyr                * 1.0.10   2022-09-01 [1] CRAN (R 4.2.0)\n ellipsis               0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate               0.19     2022-12-13 [1] CRAN (R 4.2.0)\n expm                   0.999-7  2023-01-09 [1] CRAN (R 4.2.0)\n fansi                  1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n farver                 2.1.1    2022-07-06 [1] CRAN (R 4.2.0)\n fastmap                1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n FNN                    1.1.3.1  2022-05-23 [1] CRAN (R 4.2.0)\n generics               0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb         * 1.34.4   2022-12-01 [1] Bioconductor\n GenomeInfoDbData       1.2.9    2022-12-12 [1] Bioconductor\n GenomicRanges        * 1.50.2   2022-12-16 [1] Bioconductor\n ggbeeswarm             0.7.1    2022-12-16 [1] CRAN (R 4.2.0)\n ggplot2              * 3.4.0    2022-11-04 [1] CRAN (R 4.2.0)\n ggrepel                0.9.2    2022-11-06 [1] CRAN (R 4.2.0)\n glmGamPoi              1.11.7   2023-03-11 [1] Github (const-ae/glmGamPoi@78c5cff)\n glue                   1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n gridExtra              2.3      2017-09-09 [1] CRAN (R 4.2.0)\n gtable                 0.3.1    2022-09-01 [1] CRAN (R 4.2.0)\n harmony                0.1.1    2023-03-11 [1] Github (immunogenomics/harmony@63ebd73)\n here                   1.0.1    2020-12-13 [1] CRAN (R 4.2.0)\n hms                    1.1.2    2022-08-19 [1] CRAN (R 4.2.0)\n htmltools              0.5.4    2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets            1.5.4    2021-09-08 [1] CRAN (R 4.2.2)\n httr                   1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n IRanges              * 2.32.0   2022-11-01 [1] Bioconductor\n irlba                  2.3.5.1  2022-10-03 [1] CRAN (R 4.2.0)\n isoband                0.2.6    2022-10-06 [1] CRAN (R 4.2.0)\n jsonlite               1.8.4    2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST               1.38.0   2022-11-01 [1] Bioconductor\n knitr                  1.41     2022-11-18 [1] CRAN (R 4.2.0)\n labeling               0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n lattice                0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lemur                * 0.0.9    2023-03-11 [1] Github (const-ae/lemur@efdb6b3)\n lifecycle              1.0.3    2022-10-07 [1] CRAN (R 4.2.0)\n magrittr               2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n MASS                   7.3-58.1 2022-08-03 [2] CRAN (R 4.2.2)\n Matrix               * 1.5-3    2022-11-11 [1] CRAN (R 4.2.0)\n MatrixGenerics       * 1.10.0   2022-11-01 [1] Bioconductor\n matrixStats          * 0.63.0   2022-11-18 [1] CRAN (R 4.2.0)\n memoise                2.0.1    2021-11-26 [1] CRAN (R 4.2.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.2.0)\n openssl                2.0.5    2022-12-06 [1] CRAN (R 4.2.0)\n org.Mm.eg.db         * 3.16.0   2022-12-29 [1] Bioconductor\n patchwork            * 1.1.2    2022-08-19 [1] CRAN (R 4.2.0)\n pillar                 1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n png                    0.1-8    2022-11-29 [1] CRAN (R 4.2.0)\n purrr                * 1.0.0    2022-12-20 [1] CRAN (R 4.2.0)\n R6                     2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp                   1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n RCurl                  1.98-1.9 2022-10-03 [1] CRAN (R 4.2.0)\n readr                * 2.1.3    2022-10-01 [1] CRAN (R 4.2.0)\n rlang                  1.0.6    2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown              2.20     2023-01-19 [1] RSPM (R 4.2.2)\n rprojroot              2.0.3    2022-04-02 [1] CRAN (R 4.2.0)\n RSQLite                2.2.19   2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi             0.14     2022-08-22 [1] CRAN (R 4.2.0)\n rsvd                   1.0.5    2021-04-16 [1] CRAN (R 4.2.0)\n S4Vectors            * 0.36.1   2022-12-05 [1] Bioconductor\n ScaledMatrix           1.6.0    2022-11-01 [1] Bioconductor\n scales                 1.2.1    2022-08-20 [1] CRAN (R 4.2.0)\n scater               * 1.26.1   2022-11-13 [1] Bioconductor\n scuttle              * 1.8.3    2022-12-14 [1] Bioconductor\n sessioninfo            1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n SingleCellExperiment * 1.20.0   2022-11-01 [1] Bioconductor\n sparseMatrixStats      1.10.0   2022-11-01 [1] Bioconductor\n stringi                1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr                1.5.0    2022-12-02 [1] CRAN (R 4.2.0)\n SummarizedExperiment * 1.28.0   2022-11-01 [1] Bioconductor\n sys                    3.4.1    2022-10-18 [1] CRAN (R 4.2.0)\n tibble                 3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr                * 1.2.1    2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect             1.2.0    2022-10-10 [1] CRAN (R 4.2.0)\n tzdb                   0.3.0    2022-03-28 [1] CRAN (R 4.2.0)\n utf8                   1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n uwot                   0.1.14   2022-08-22 [1] CRAN (R 4.2.0)\n vctrs                  0.5.1    2022-11-16 [1] CRAN (R 4.2.0)\n vipor                  0.4.5    2017-03-22 [1] CRAN (R 4.2.0)\n viridis                0.6.2    2021-10-13 [1] CRAN (R 4.2.0)\n viridisLite            0.4.1    2022-08-22 [1] CRAN (R 4.2.0)\n withr                  2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun                   0.36     2022-12-21 [1] RSPM (R 4.2.2)\n XVector                0.38.0   2022-11-01 [1] Bioconductor\n yaml                   2.3.6    2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc               1.44.0   2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/lemur/index.html#footnotes",
    "href": "posts/lemur/index.html#footnotes",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nscRNA-seq data from glioblastoma slices cultured in vitro with either pamobinostat or a vehicle control, characterized in a terrific paper by Zhao et al, 2021↩︎\nCD45 is a cell surface antigen that is expressed on most hematopoietic lineage cells, including microglia.↩︎"
  },
  {
    "objectID": "posts/conda-speedup/index.html",
    "href": "posts/conda-speedup/index.html",
    "title": "2022 normconf: lightning talks",
    "section": "",
    "text": "I am really looking forward to the virtual #normconf conference on 2022-12-15. In addition to a great program the meeting also features numerous ~ 5 minute lightning talks\nThe full list of lightning talks is available here but here are my favorites:\n\nJacquelin Nolis: Alaska challenged my preconceived notions of storing sunset data\nJenny Bryan: How to name files like a normie\nChelsea Parlett: Why Are You The Way That You Are: Sklearn Quirks\nZachary Chetchavat: Hotkeys for Spreadsheets Cookbook, Practical Solutions from CTRL-Arrow, to F4\nShoili Pal: Data Science Intake Forms\nAmanda Fioritto: Qualify: The SQL Filtering Pattern You Never Knew You Needed\nJuulia Suvilehto: Trying to convince academics to use git\nAnuvabh Dutt: Config files for fast and reproducible ML experiments\nJane Adams: How to make six figures in an hour (slides)\nBryan Bischof: Toss that (model) in an endpoint\nSophia Yang: PyScript: Run Python in your HTML\nVictor Geislinger: Staying Alive: Persistent SSH Sessions w/ tmux\nTom Baldwin: Putting Git’s commit hash in version, two ways\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\nJul 31, 2023\n\n\nCustomizing my Quarto website\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nJul 24, 2023\n\n\nGuess the correlation - a first streamlit app\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nJul 21, 2023\n\n\nGrav - a lightweight content management system\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nJul 21, 2023\n\n\nGreg Wilson: Late Night Thoughts on Listening to Ike Quebec (2018)\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nJun 26, 2023\n\n\nDocumenting data wrangling with the dtrackr R package\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nMay 6, 2023\n\n\nQuerying parquet files with duckdb\n\n\nThomas Sandmann\n\n\n6 min\n\n\n\n\nMar 12, 2023\n\n\nLemur: analyzing multi-condition single-cell data\n\n\nThomas Sandmann\n\n\n14 min\n\n\n\n\n\nFeb 25, 2023\n\n\nSimultaneously inserting records into two tables with Postgres CTEs\n\n\nThomas Sandmann\n\n\n4 min\n\n\n\n\nJan 21, 2023\n\n\nDistributing R packages with a drat repository hosted on AWS S3\n\n\nThomas Sandmann\n\n\n12 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output\n\n\nThomas Sandmann\n\n\n4 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (3): Validating published results (no UMIs)\n\n\nThomas Sandmann\n\n\n14 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (4): Validating published results (with UMIs)\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\nJan 2, 2023\n\n\nSQL and noSQL approaches to creating & querying databases (using R)\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\n\nDec 27, 2022\n\n\nInteractive GSEA results: visualizations with reactable & plotly\n\n\nThomas Sandmann\n\n\n27 min\n\n\n\n\nDec 24, 2022\n\n\nUpSet plots: comparing differential expression across contrasts\n\n\nThomas Sandmann\n\n\n8 min\n\n\n\n\nDec 22, 2022\n\n\nFigure size, layout & tabsets with Quarto\n\n\nThomas Sandmann\n\n\n2 min\n\n\n\n\nDec 12, 2022\n\n\nFull text search in Postgres - the R way\n\n\nThomas Sandmann\n\n\n11 min\n\n\n\n\nDec 11, 2022\n\n\nUpdating R the easy way: using rig command line tool\n\n\nThomas Sandmann\n\n\n2 min\n\n\n\n\nDec 10, 2022\n\n\n2022 normconf: lightning talks\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nDec 8, 2022\n\n\nThe rlist R package\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 17, 2022\n\n\nCreating custom badges for your README\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 15, 2022\n\n\nLearning nextflow: blasting multiple sequences\n\n\nThomas Sandmann\n\n\n8 min\n\n\n\n\nNov 14, 2022\n\n\nPython type hints\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 13, 2022\n\n\nFujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nNov 13, 2022\n\n\nRefreshing & exporting temporary AWS credentials\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nNov 12, 2022\n\n\nInstalling pyroe with conda\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 12, 2022\n\n\nWelcome To My Blog\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Thomas Sandmann’s blog. Originally from Germany, my professional journey includes a degree in Biochemistry, a PhD in Developmental Biology from EMBL, postdoctoral research at Temasek Lifescience Laboratories and at the German Cancer research Center. Afterwards, I worked as a Computational Biologist at Genentech and Verily. In 2016 I joined Denali Therapeutics, where I am collaborating with colleagues across the organization to generate, analyze and understand genomics & genetics data.\nThis blog collects the personal lessons I am learning along the way.  (Opinions are my own and not the views of my employer.)\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  }
]