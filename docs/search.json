[
  {
    "objectID": "posts/CarpentryCon2018/index.html",
    "href": "posts/CarpentryCon2018/index.html",
    "title": "Greg Wilson: Late Night Thoughts on Listening to Ike Quebec (2018)",
    "section": "",
    "text": "In 2018 Greg Wilson presented the Keynote “Late Night Thoughts on Listening to Ike Quebec” at the 2018 CarpentryCon meeting, after he made the decision to move on from the Software Carpentry project he had founded in 2010. His talk is available on youtube, and this post contains my notes.\nGreg has help numerous positions in his professional life, and tackled many large projects. While there are many books providing advice on how start endeavors, he realized that there was little guidance on how to best exit them. Reflecting on his personalexperience with transitioning out of roles -\nincluding leaving Academia thrice - led Greg to formulate and share the following ten rules:\n\nBe sure you mean it.\n\nLetting go is hard.\nDon’t be in-and-out.\n\nDo it when others think it’s time.\n\nYou will be the last person to realize that it’s time to move on. They are probably right.\n\nTell people the what, when, and why.\n\nCommunicate that the succession plan is.\n\nDon’t pick your successor by yourself.\n\nAvoid picking somebody like yourself, avoid forming an old boys club.\nDiversity matters.\nChange is good.\nThis choice belongs to the community that will work with your successor.\n\nTrain people before you go.\n\nIf at all possible, arrange for some overlap.\nTake a vacation to identify information that needs to be transferred.\n\nWhen you leave, leave.\n\nContinuing in another role confuses people.\nLeave a clear space for your successor.\nPeople will contact the person, not the role.\n\nHave some fun before you go.\n\nSome back burner project, something you never got around to - if not now, when?\n\nReflect on what you learned.\n\nDon’t beat yourself up about things you didn’t get right.\nWrite down what you did well.\n\nRemember the good things, too.\n\nGive yourself some credit (even if you are Canadian).\n\nDo something next.\n\nHave a plan what to do next (not just a job).\nYou probably don’t deal well with being idle.\n\n\n\nAcademia conditions us to believe that only novel things are valuable. But many tools we use and teach are old - and useful! Measure yourself by usefulness, not novelty.\nGreg believes that the biggest impact of the Carpentries will be that They taught participants how to teach, build curricula, build communities, etc.\nNothing worth having comes without some kind of fight. You have to be on the right committee to get the decision you want.\n\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the third of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis analysis compares the performance of the nf-core/rnaseq workflow for QuantSeq FWD 3’ tag RNAseq data without unique molecular identifiers.\nThe differential expression analysis results are highly concordant with those obtained in the original publication.\nWith the appropriate settings, the nf-core/rnaseq workflow is a valid data processing pipeline for this data type.\n\nThe first post in this series walked through the preprocesssing of QuantSeq FWD data published in a preprint by Xia et al.\nNext, we use Bioconductor/R packages to reproduce the downstream results. We perform the same differential gene expression analysis twice with either\n\nthe original counts matrix published by the authors 1\nthe output of the nf-core/rnaseq workflow\n\n\nlibrary(dplyr)\nlibrary(edgeR)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(org.Mm.eg.db)\nlibrary(readxl)\nlibrary(SummarizedExperiment)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#sample-annotations",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#sample-annotations",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Sample annotations",
    "text": "Sample annotations\nWe start by retrieving the sample annotation table, listing e.g. the sex, and genotype for each mouse, and the batch for each collected sample.\nThis information is available in the SRA Run Explorer. (I saved it as the sample_metadata.csv CSV file in case you want to follow along.)\n\nsample_sheet &lt;- file.path(work_dir, \"sample_metadata.csv\")\nsample_anno &lt;- read.csv(sample_sheet, row.names = \"Experiment\")\nhead(sample_anno[, c(\"Run\", \"Animal.ID\", \"Age\", \"age_unit\", \"Batch\", \"sex\",\n                     \"Genotype\", \"Sample.Name\")])\n\n                   Run Animal.ID Age age_unit Batch  sex Genotype Sample.Name\nSRX9142648 SRR12661924       LA1   8   months  Day1 male       WT  GSM4793335\nSRX9142649 SRR12661925       LA1   8   months  Day1 male       WT  GSM4793336\nSRX9142650 SRR12661926       LA6   8   months  Day1 male       WT  GSM4793337\nSRX9142651 SRR12661927       LA6   8   months  Day1 male       WT  GSM4793338\nSRX9142652 SRR12661928       LA9   8   months  Day2 male       WT  GSM4793339\nSRX9142653 SRR12661929       LA9   8   months  Day2 male       WT  GSM4793340\n\n\nBecause our SRA metadata doesn’t include the GEO sample title, I saved the identifier mappings in the GEO_sample_ids.csv CSV file.\n\ngeo_ids &lt;- read.csv(file.path(work_dir, \"GEO_sample_ids.csv\"))\nhead(geo_ids)\n\n  sample_name sample_id\n1  GSM4793335 DRN-18429\n2  GSM4793336 DRN-18430\n3  GSM4793337 DRN-18439\n4  GSM4793338 DRN-18440\n5  GSM4793339 DRN-18445\n6  GSM4793340 DRN-18446\n\n\n\n\nCode\ncolnames(sample_anno)&lt;- tolower(colnames(sample_anno))\ncolnames(sample_anno) &lt;- sub(\".\", \"_\", colnames(sample_anno), \n                             fixed = TRUE) \nsample_anno &lt;- sample_anno[, c(\"sample_name\", \"animal_id\", \"genotype\", \"sex\",\n                               \"batch\")]\nsample_anno$genotype &lt;- factor(sample_anno$genotype, \n                               levels = c(\"WT\", \"Het\", \"Hom\"))\nsample_anno$sample_title &lt;- geo_ids[\n  match(sample_anno$sample_name, geo_ids$sample_name), \"sample_id\"]\nhead(sample_anno)\n\n\n           sample_name animal_id genotype  sex batch sample_title\nSRX9142648  GSM4793335       LA1       WT male  Day1    DRN-18429\nSRX9142649  GSM4793336       LA1       WT male  Day1    DRN-18430\nSRX9142650  GSM4793337       LA6       WT male  Day1    DRN-18439\nSRX9142651  GSM4793338       LA6       WT male  Day1    DRN-18440\nSRX9142652  GSM4793339       LA9       WT male  Day2    DRN-18445\nSRX9142653  GSM4793340       LA9       WT male  Day2    DRN-18446\n\n\nThis experiment includes 36 samples of microglia cells obtained from 18 different 8-month old mice. Both male and female animals were included in the study.\nThe animals carry one of three different genotypes of the gene encoding the APP amyloid beta precursor protein, either\n\nthe wildtype mouse gene (WT) or\none copy (Het) or\ntwo copies (Hom)\n\nof a mutant APP gene carrying mutations associated with familial Alzheimer’s Disease.\nSamples from all three genotypes were collected on three days, and we will use this batch information to model the experiment.\nTwo separate microglia samples were obtained from each animal, and we will include this nested relationship by modeling the animal as random effect in our linear model."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#xia-et-als-original-count-data",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#xia-et-als-original-count-data",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Xia et al’s original count data",
    "text": "Xia et al’s original count data\nFirst, we retrieve the authors’ count matrix from NCBI GEO, available as a Supplementary Excel file.\n\nurl &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE158152&\",\n              \"format=file&file=GSE158152%5Fdst150%5Fprocessed%2Exlsx\")\ntemp_file &lt;- tempfile(fileext = \".xlsx\")\ndownload.file(url, destfile = temp_file)\n\nThe Excel file has three different worksheets\n\nsample_annotations\nraw_counts\nnormalized_cpm\n\n\nraw_counts &lt;- read_excel(temp_file, sheet = \"raw_counts\")\nhead(colnames(raw_counts), 10)\n\n [1] \"feature_id\" \"name\"       \"meta\"       \"source\"     \"symbol\"    \n [6] \"DRN-18429\"  \"DRN-18430\"  \"DRN-18439\"  \"DRN-18440\"  \"DRN-18445\" \n\n\nThe raw_counts excel sheet contains information about the detected genes ( feature_ID, name) and the samples are identified by their GEO title (e.g. DRN-18459, DRN-184560). We use the raw counts to populate a new DGEList object and perform Library Size Normalization with the TMM approach.\n\n\nCode\ncount_data &lt;- as.matrix(raw_counts[, grep(\"DRN-\", colnames(raw_counts))])\nrow.names(count_data) &lt;- raw_counts$feature_id\ncolnames(count_data) &lt;- row.names(sample_anno)[\n  match(colnames(count_data), sample_anno$sample_title)\n]\n\ngene_data &lt;- data.frame(\n  gene_id = raw_counts$feature_id,\n  gene_name = raw_counts$symbol,\n  row.names = raw_counts$feature_id\n)\n\ncol_data &lt;- data.frame(\n  sample_anno[colnames(count_data),\n              c(\"sample_title\", \"animal_id\", \"sex\", \"genotype\", \"batch\")],\n  workflow = \"geo\"\n)\n\ndge &lt;- DGEList(\n  counts = as.matrix(count_data), \n  samples = col_data[colnames(count_data), ], \n  genes = gene_data[row.names(count_data), ]\n)\n\ndge &lt;- calcNormFactors(dge, method = \"TMM\")\n\n\nNext, we project the samples into two dimensions by performing multi-dimensional scaling of the top 500 most variable genes. The samples cluster by genotype, with WT and Het segregating from the Hom samples.\n\nplotMDS(dge, labels = dge$samples$genotype,\n        main = \"Multi-dimensional scaling\", \n        sub = \"Based on the top 500 most variable genes\")\n\n\n\n\nLet’s identify which genes are significantly differentially expressed between the three genotypes!\n\nLinear modeling with limma/voom\nFirst, we use the edgeR::filterByExpr() function to identify genes with sufficiently large counts to be examined for differential expression. (The min.count = 25 parameter was determined by examining the mean-variance plot by the voomLmFit() function.)\n\ndesign &lt;- model.matrix(~ genotype + sex + batch, data = dge$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge, design = design, min.count = 25)\n\nNext, we fit a linear model to the data using the limma/voom approach. The model includes the following fixed effects:\n\nThe genotype coded as a factor with the WT as the reference level.\nThe sex and batch covariates, to account for systematic differences in mean gene expression.\n\nBecause the dataset included two replicate samples from each animal, we model the animal as a random effect (via the block argument of the voomLmFit() function). We then extract the coefficients, log2 fold changes and p-values via limma’s empirical Bayes approach.\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe use the limma::treat() function to test the null hypothesis that genes display significant differential expression greater than 1.2-fold. This is more stringent than the conventional null hypothesis of zero change. (Please consult the limma::treat() help page for details.)\n\n\n\n\nfit &lt;- voomLmFit(\n  dge[keep, row.names(design)], \n  design = design,\n  block = dge$samples$animal_id, \n  sample.weights = TRUE, \n  plot = FALSE\n)\nfit &lt;- treat(fit, robust=TRUE)\n\nThe following table displays the number of differentially up- and down-regulated genes after applying a false-discovery (adj.P.Val) threshold of 5%. While we did not detect significant differences between Het and WT animals, the analysis revealed &gt; 450 differentially expressed genes between Hom and WT microglia.\n\nsummary(decideTests(fit))[, c(\"Het\", \"Hom\")]\n\n         Het   Hom\nDown       0    73\nNotSig 10131  9660\nUp         0   398\n\n\nThe top 10 genes with the smallest p-values include well known markers of microglia activation:\n\ntopTreat(fit, coef = \"Hom\")[, c(\"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")]\n\n                   gene_name     logFC      P.Value    adj.P.Val\nENSMUSG00000027523      Gnas 1.5904760 3.467080e-24 3.512499e-20\nENSMUSG00000022265       Ank 3.7866023 1.258687e-20 6.375881e-17\nENSMUSG00000021477      Ctsl 1.2829086 2.294556e-20 7.748717e-17\nENSMUSG00000018927      Ccl6 2.7405831 3.514945e-20 8.902477e-17\nENSMUSG00000030579    Tyrobp 1.2880854 9.748473e-19 1.975236e-15\nENSMUSG00000022415    Syngr1 1.5033949 1.367158e-18 2.308446e-15\nENSMUSG00000016256      Ctsz 1.1557870 2.878895e-18 4.166584e-15\nENSMUSG00000023992     Trem2 0.9460719 7.386805e-18 9.354465e-15\nENSMUSG00000056737      Capg 2.4537482 1.927765e-17 2.170021e-14\nENSMUSG00000030342       Cd9 1.0596747 1.328189e-16 1.328093e-13\n\n\nNext we repeat the same analysis with the output of the nf-core/rnaseq workflow."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#nf-corernaseq-results",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#nf-corernaseq-results",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "nf-core/rnaseq results",
    "text": "nf-core/rnaseq results\nWe start with the raw counts contained in the salmon.merged.gene_counts.rds file generated by the nf-core/rnaseq workflow.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe nf-core pipeline returned the versioned ENSEMBL gene identifiers (e.g.) ENSMUSG00000000001.4. Because Xia et al only provided the unversioned identifiers (e.g. ENSMUSG00000000001) we trim the numeric suffix.\n\n\n\nWe TMM-normalize the data, as before. (This step converts the SummarizedExperiment into a DGEList object as well.)\n\ncount_file &lt;- file.path(work_dir, \"salmon.merged.gene_counts.rds\")\nse &lt;- readRDS(count_file)\nrow.names(se) &lt;- sapply(\n  strsplit(row.names(se), split = \".\", fixed = TRUE), \"[[\", 1)\nstopifnot(all(colnames(se) %in% row.names(sample_anno)))\ndge_nfcore &lt;- calcNormFactors(se, method = \"TMM\")\n\nNext, we add the sample metadata and fit the same linear model as before.\n\n\nCode\ndge_nfcore$genes$gene_id &lt;- row.names(dge_nfcore)\n\ndge_nfcore$samples &lt;- data.frame(\n  dge_nfcore$samples,\n  sample_anno[colnames(dge_nfcore),\n              c(\"sample_title\", \"animal_id\", \"sex\", \"genotype\", \"batch\")],\n  workflow = \"nfcore\"\n)\nstopifnot(all(colnames(dge) %in% colnames(dge_nfcore)))\ndge_nfcore &lt;- dge_nfcore[, colnames(dge)]\n\ndesign &lt;- model.matrix(~ genotype + sex + batch, data = dge_nfcore$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge_nfcore, design = design, min.count = 25)\nfit_nfcore &lt;- voomLmFit(\n  dge_nfcore[keep, row.names(design)], \n  design = design,\n  block = dge_nfcore$samples$animal_id, \n  sample.weights = TRUE, \n  plot = FALSE\n)\n\n\nFirst sample weights (min/max) 0.5573601/2.1684287\n\n\nFirst intra-block correlation  0.02343718\n\n\nFinal sample weights (min/max) 0.536793/2.237000\n\n\nFinal intra-block correlation  0.02358457\n\n\nCode\nfit_nfcore &lt;- treat(fit_nfcore, robust=TRUE)\n\n\nAs with the original count data from NCBI GEO, we detect &gt; 450 differentially expressed genes between Hom and WT genotypes (FDR &lt; 5%, null hypothesis: fold change &gt; 1.2).\n\nsummary(decideTests(fit_nfcore))[, c(\"Het\", \"Hom\")]\n\n         Het   Hom\nDown       0    76\nNotSig 10428  9917\nUp         0   435"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#comparing-results-across-preprocessing-workflows",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#comparing-results-across-preprocessing-workflows",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Comparing results across preprocessing workflows",
    "text": "Comparing results across preprocessing workflows\nNext, we compare the results obtained with the two datasets. We create the cpms and tt dataframes, holding the combined absolute and differential expression results, respectively.\n\n\nCode\ncpms &lt;- local({\n  geo &lt;- cpm(dge, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"Xia et al\")\n  \n  nfcore &lt;- cpm(dge_nfcore, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge_nfcore$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge_nfcore$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"nf-core\")\n  \n  dplyr::bind_rows(\n    dplyr::select(geo, any_of(intersect(colnames(geo), colnames(nfcore)))),\n    dplyr::select(nfcore, any_of(intersect(colnames(geo), colnames(nfcore))))\n  )\n})\n\ntt &lt;- rbind(\n  topTreat(fit, coef = \"Hom\", number = Inf)[\n    , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"geo\"),\n   topTreat(fit_nfcore, coef = \"Hom\", number = Inf)[\n     , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"nfcore\")\n) %&gt;%\n  dplyr::mutate(adj.P.Val = signif(adj.P.Val, 2)) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"gene_name\"), \n    names_from = \"dataset\", \n    values_from = \"adj.P.Val\") %&gt;%\n  dplyr::arrange(nfcore) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"gene_id\")\n\n\n\nNormalized expression\nFirst, we examine the correlation between the normalized log-transformed gene expression estimates returned from the two workflows. We focus on those genes that passed the filterByExpr thresholds above, e.g. those genes deemed sufficiently highly expressed to be assessed for differential expression.\n\n\nCode\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nsum_stats &lt;- cpms %&gt;%\n  dplyr::filter(gene_id %in% common_genes) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"sample_name\"),\n    values_from = \"cpm\",\n    names_from = \"dataset\") %&gt;%\n  dplyr::group_by(gene_id) %&gt;%\n  dplyr::summarise(\n    r = cor(log1p(`Xia et al`), log1p(`nf-core`)),\n    mean_xia = mean(`Xia et al`),\n    mean_nfcore = mean(`nf-core`))\n\np &lt;- ggplot(data = sum_stats, aes(x = r)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(limits = c(0, 1.02), breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"Pearson correlation coefficient (R)\", \n       y = \"Number of genes\",\n       title = \"Correlation between normalized log2 counts\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\n\nThe correlation between normalized log2 expression estimates is very high, with 95% of all genes showing a Pearson correlation coefficient &gt; 0.94.\nMost of the 10022 examined genes were detected with &gt; 10 normalized counts per million reads.\n\np &lt;- ggplot(data = sum_stats, aes(x = mean_xia + 1)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(trans = scales::log10_trans(),\n                     labels = scales::comma_format()) +\n  labs(x = \"Mean normalized counts per million\", \n       y = \"Number of genes\",\n       title = \"Average expression\",\n       subtitle = \"Xia et al\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\nNext, we will examine the results of the differential expression analysis.\n\n\nDifferential expression results\nAnalyses based on either preprocessing pipeline yield similar numbers of differentially expressed genes.\n\n\nCode\nresults &lt;- cbind(\n  decideTests(fit)[common_genes, \"Hom\"], \n  decideTests(fit_nfcore)[common_genes, \"Hom\"]\n)\ncolnames(results) &lt;- c(\"Xia et al\", \"nf-core\")\nclass(results) &lt;- \"TestResults\"\nsummary(results)\n\n\n   Xia et al nf-core\n-1        73      75\n0       9554    9528\n1        395     419\n\n\nBut are these the same genes in both sets of results?\nWe can visualize the overlap between the sets of significant genes in a Venn diagram (FDR &lt; 5%). The vast majority of differentially expressed genes is detected with both quantitation approaches (for both up- and down-regulated genes.)\n\nlimma::vennDiagram(results, include = c(\"up\", \"down\"),\n                   counts.col=c(\"red\", \"blue\"), mar = rep(0,4))\n\n\n\n\nFor example, the following plots show the normalized expression of a few highly differentially expressed genes (known markers of active microglia).\n\n\nCode\nfor (gene in topTreat(fit, coef = \"Hom\", number = 6)[[\"gene_id\"]]) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a hard FDR threshold can inflate the number of apparent differences, e.g. when a gene is close to the significance threshold (see below).\n\np_cor &lt;- cor(\n  fit$coefficients[common_genes, \"Hom\"], \n  fit_nfcore$coefficients[common_genes, \"Hom\"])\n\nThe log2 fold estimates for the Hom vs WT comparison are highly correlated across the two analysis workflows (Pearson correlation coefficient R = 0.99 ):\n\nsmoothScatter(\n  fit$coefficients[common_genes, \"Hom\"], \n  fit_nfcore$coefficients[common_genes, \"Hom\"],\n  ylab = \"nf-core (log2FC)\",\n  xlab = \"Xia et al (log2FC)\",\n  main = \"Homozygous APP vs WT (effect size)\"\n)\ntext(x = 10, y = -2, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\nas are the t-statistics across all examined genes:\n\np_cor &lt;- cor(\n  fit$t[common_genes, \"Hom\"], \n  fit_nfcore$t[common_genes, \"Hom\"])\nsmoothScatter(\n  fit$t[common_genes, \"Hom\"], \n  fit_nfcore$t[common_genes, \"Hom\"],\n  ylab = \"nf-core (t-statistic)\",\n  xlab = \"Xia et al (t-statistic)\",\n  main = \"Homozygous APP vs WT (t-statistic)\")\ntext(x = 10, y = -2, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\n\nDiscordant significance calls\n\n# genes detected in GEO, but not significant with nf-core\ngenes &lt;- row.names(results)[which(abs(results[, 1]) == 1 & results[, 2] == 0)]\n\nAt FDR &lt; 5% 14 genes were reported as significantly differentially expressed with the original Xia et al count matrix but not with the output of the nf-core/rnaseq workflow.\nAs side-by-side comparison of the FDR (adj.P.Val) for these genes confirms that the vast majority display significant close to the 5% threshold in the nf-core/rnaseq output as well. (This is in line with the high overall correlation of the t-statistics observed above.)\n\nprint(tt[genes, ])\n\n                       gene_name    geo nfcore\nENSMUSG00000078193 RP24-228M19.1 0.0041  0.120\nENSMUSG00000027427        Polr3f 0.0490  0.052\nENSMUSG00000028394         Pole3 0.0480  0.073\nENSMUSG00000029027          Dffb 0.0360  0.082\nENSMUSG00000029649          Pomp 0.0430  0.056\nENSMUSG00000054404         Slfn5 0.0440  0.064\nENSMUSG00000050965         Prkca 0.0310  0.087\nENSMUSG00000020641         Rsad2 0.0440  0.063\nENSMUSG00000021057         Akap5 0.0260  0.074\nENSMUSG00000115230 RP24-123O20.1 0.0400  0.061\nENSMUSG00000042622          Maff 0.0500  0.051\nENSMUSG00000050410         Tcf19 0.0500  0.051\nENSMUSG00000059040         Eno1b 0.0290  0.410\nENSMUSG00000042712        Tceal9 0.0480  0.054\n\n\nFinally, we plot the normalized gene expression estimates for the 14 discordant genes.\n\n\nCode\nfor (gene in genes) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#conclusions",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#conclusions",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Conclusions",
    "text": "Conclusions\n\nDifferential expression analyses of raw counts obtained with the nc-core/rnaseq workflow yields results that are highly concordant with those obtained with the raw counts the authors deposited in NCBI GEO.\nWith appropriate parameters the nf-core/rnaseq workflow can be applied to QuantSeq FWD 3’tag RNA-seq data."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#reproducibility",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#reproducibility",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package              * version   date (UTC) lib source\n P abind                  1.4-5     2016-07-21 [?] CRAN (R 4.3.0)\n P AnnotationDbi        * 1.62.2    2023-07-02 [?] Bioconductor\n P askpass                1.1       2019-01-13 [?] CRAN (R 4.3.0)\n P Biobase              * 2.60.0    2023-05-08 [?] Bioconductor\n P BiocGenerics         * 0.46.0    2023-06-04 [?] Bioconductor\n P BiocManager            1.30.22   2023-08-08 [?] CRAN (R 4.3.0)\n P Biostrings             2.68.1    2023-05-21 [?] Bioconductor\n P bit                    4.0.5     2022-11-15 [?] CRAN (R 4.3.0)\n P bit64                  4.0.5     2020-08-30 [?] CRAN (R 4.3.0)\n P bitops                 1.0-7     2021-04-24 [?] CRAN (R 4.3.0)\n P blob                   1.2.4     2023-03-17 [?] CRAN (R 4.3.0)\n P cachem                 1.0.8     2023-05-01 [?] CRAN (R 4.3.0)\n P cellranger             1.1.0     2016-07-27 [?] CRAN (R 4.3.0)\n P cli                    3.6.1     2023-03-23 [?] CRAN (R 4.3.0)\n P colorspace             2.1-0     2023-01-23 [?] CRAN (R 4.3.0)\n P crayon                 1.5.2     2022-09-29 [?] CRAN (R 4.3.0)\n R credentials            1.3.2     &lt;NA&gt;       [?] &lt;NA&gt;\n P DBI                    1.1.3     2022-06-18 [?] CRAN (R 4.3.0)\n P DelayedArray           0.26.7    2023-07-30 [?] Bioconductor\n P digest                 0.6.33    2023-07-07 [?] CRAN (R 4.3.0)\n P dplyr                * 1.1.2     2023-04-20 [?] CRAN (R 4.3.0)\n P edgeR                * 3.42.4    2023-06-04 [?] Bioconductor\n P evaluate               0.21      2023-05-05 [?] CRAN (R 4.3.0)\n P fansi                  1.0.4     2023-01-22 [?] CRAN (R 4.3.0)\n P farver                 2.1.1     2022-07-06 [?] CRAN (R 4.3.0)\n P fastmap                1.1.1     2023-02-24 [?] CRAN (R 4.3.0)\n P generics               0.1.3     2022-07-05 [?] CRAN (R 4.3.0)\n P GenomeInfoDb         * 1.36.1    2023-07-02 [?] Bioconductor\n P GenomeInfoDbData       1.2.10    2023-08-23 [?] Bioconductor\n P GenomicRanges        * 1.52.0    2023-05-08 [?] Bioconductor\n P ggplot2              * 3.4.3     2023-08-14 [?] CRAN (R 4.3.0)\n P glue                   1.6.2     2022-02-24 [?] CRAN (R 4.3.0)\n P gtable                 0.3.4     2023-08-21 [?] CRAN (R 4.3.1)\n P here                 * 1.0.1     2020-12-13 [?] CRAN (R 4.3.0)\n P htmltools              0.5.6     2023-08-10 [?] CRAN (R 4.3.0)\n P htmlwidgets            1.6.2     2023-03-17 [?] CRAN (R 4.3.0)\n P httr                   1.4.7     2023-08-15 [?] CRAN (R 4.3.0)\n P IRanges              * 2.34.1    2023-07-02 [?] Bioconductor\n P jsonlite               1.8.7     2023-06-29 [?] CRAN (R 4.3.0)\n P KEGGREST               1.40.0    2023-05-08 [?] Bioconductor\n P KernSmooth             2.23-21   2023-05-03 [?] CRAN (R 4.3.1)\n P knitr                  1.43      2023-05-25 [?] CRAN (R 4.3.0)\n P labeling               0.4.2     2020-10-20 [?] CRAN (R 4.3.0)\n P lattice                0.21-8    2023-04-05 [?] CRAN (R 4.3.1)\n P lifecycle              1.0.3     2022-10-07 [?] CRAN (R 4.3.0)\n P limma                * 3.56.2    2023-06-04 [?] Bioconductor\n P locfit                 1.5-9.8   2023-06-11 [?] CRAN (R 4.3.0)\n P magrittr               2.0.3     2022-03-30 [?] CRAN (R 4.3.0)\n P Matrix                 1.5-4.1   2023-05-18 [?] CRAN (R 4.3.1)\n P MatrixGenerics       * 1.12.3    2023-07-30 [?] Bioconductor\n P matrixStats          * 1.0.0     2023-06-02 [?] CRAN (R 4.3.0)\n P memoise                2.0.1     2021-11-26 [?] CRAN (R 4.3.0)\n P munsell                0.5.0     2018-06-12 [?] CRAN (R 4.3.0)\n P openssl                2.1.0     2023-07-15 [?] CRAN (R 4.3.0)\n P org.Mm.eg.db         * 3.17.0    2023-08-23 [?] Bioconductor\n P pillar                 1.9.0     2023-03-22 [?] CRAN (R 4.3.0)\n P pkgconfig              2.0.3     2019-09-22 [?] CRAN (R 4.3.0)\n P png                    0.1-8     2022-11-29 [?] CRAN (R 4.3.0)\n P purrr                  1.0.2     2023-08-10 [?] CRAN (R 4.3.0)\n P R6                     2.5.1     2021-08-19 [?] CRAN (R 4.3.0)\n P Rcpp                   1.0.11    2023-07-06 [?] CRAN (R 4.3.0)\n P RCurl                  1.98-1.12 2023-03-27 [?] CRAN (R 4.3.0)\n P readxl               * 1.4.3     2023-07-06 [?] CRAN (R 4.3.0)\n   renv                   1.0.2     2023-08-15 [1] CRAN (R 4.3.0)\n P rlang                  1.1.1     2023-04-28 [?] CRAN (R 4.3.0)\n P rmarkdown              2.24      2023-08-14 [?] CRAN (R 4.3.0)\n P rprojroot              2.0.3     2022-04-02 [?] CRAN (R 4.3.0)\n P RSQLite                2.3.1     2023-04-03 [?] CRAN (R 4.3.0)\n P rstudioapi             0.15.0    2023-07-07 [?] CRAN (R 4.3.0)\n P S4Arrays               1.0.5     2023-07-30 [?] Bioconductor\n P S4Vectors            * 0.38.1    2023-05-08 [?] Bioconductor\n P scales                 1.2.1     2022-08-20 [?] CRAN (R 4.3.0)\n P sessioninfo            1.2.2     2021-12-06 [?] CRAN (R 4.3.0)\n P statmod                1.5.0     2023-01-06 [?] CRAN (R 4.3.0)\n P SummarizedExperiment * 1.30.2    2023-06-11 [?] Bioconductor\n P sys                    3.4.2     2023-05-23 [?] CRAN (R 4.3.0)\n P tibble               * 3.2.1     2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr                * 1.3.0     2023-01-24 [?] CRAN (R 4.3.0)\n P tidyselect             1.2.0     2022-10-10 [?] CRAN (R 4.3.0)\n P utf8                   1.2.3     2023-01-31 [?] CRAN (R 4.3.0)\n P vctrs                  0.6.3     2023-06-14 [?] CRAN (R 4.3.0)\n P withr                  2.5.0     2022-03-03 [?] CRAN (R 4.3.0)\n P xfun                   0.40      2023-08-09 [?] CRAN (R 4.3.0)\n P XVector                0.40.0    2023-05-08 [?] Bioconductor\n P yaml                   2.3.7     2023-01-23 [?] CRAN (R 4.3.0)\n P zlibbioc               1.46.0    2023-05-08 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-3-xia/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-3-xia/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (3): Validating published results (no UMIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFull disclosure: I am a co-author of this publication.↩︎"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog.\n\nThis is Thomas Sandmann’s personal blog, created with Quarto. I am planning to share e.g. “Things I learned today” (TIL) and other pieces of news around Computational Biology and Data Science.\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/rslist-r-package/index.html",
    "href": "posts/rslist-r-package/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Whenever I deal with nested lists in R - e.g. after reading JSON documents - my code starts to resemble a jumbled mess of lapply calls. (Or, on a better day, a horrible collection of purrr::map calls).\nLuckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it’s super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won’t regret it!\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/python-hints/index.html",
    "href": "posts/python-hints/index.html",
    "title": "Python type hints",
    "section": "",
    "text": "Today I learned about python type hints (again…) as I was tackling the first parts of pybites’ FastAPI learning track. The following resources were great to get a quick overview:\n\nCode Better with Type Hints – Part 1\nCode Better with Type Hints – Part 2\nFastAPI’s typing introduction\nPysheet: typing\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/dtrackr/index.html",
    "href": "posts/dtrackr/index.html",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "",
    "text": "Today I learned about Robert Challen’s dtrackr R package. It extends functionality from the tidyverse to track and visualize the data wrangling operations that have been applied to a dataset.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/dtrackr/index.html#tldr",
    "href": "posts/dtrackr/index.html#tldr",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "",
    "text": "Today I learned about Robert Challen’s dtrackr R package. It extends functionality from the tidyverse to track and visualize the data wrangling operations that have been applied to a dataset."
  },
  {
    "objectID": "posts/dtrackr/index.html#motivation",
    "href": "posts/dtrackr/index.html#motivation",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Motivation",
    "text": "Motivation\nPublications involving cohorts of human subjects often include flow charts describing which participants were screened, included in a specific study arm or excluded from analysis. In fact, reporting guidelines such as CONSORT, STROBE or STARD include visualizations that communcate how the participants flowed through the study.\nIf a dataset is processed with tidyverse functions, e.g. from the dplyr or tidyr R packages, methods from the dtrackr package add metadata to each step - and automatically generate a flow chart."
  },
  {
    "objectID": "posts/dtrackr/index.html#installation",
    "href": "posts/dtrackr/index.html#installation",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Installation",
    "text": "Installation\nThe dtrackr package is available from CRAN\n\nif (!requireNamespace(\"dtrackr\", quietly = TRUE)) {\n  install.packages(\"dtrackr\")\n}\nsuppressPackageStartupMessages(library(\"dplyr\"))\nsuppressPackageStartupMessages(library(\"dtrackr\"))\nlibrary(\"glue\")\nlibrary(\"GenomicDataCommons\", \n        include.only = c(\"cases\", \"results\", \"ids\", \"gdc_clinical\"))\n\nIt contains several very useful vignettes, including an example of processing clinical trial according to CONSORT guidelines."
  },
  {
    "objectID": "posts/dtrackr/index.html#retrieving-metadata-from-the-cancer-genome-atlas",
    "href": "posts/dtrackr/index.html#retrieving-metadata-from-the-cancer-genome-atlas",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Retrieving metadata from The Cancer Genome Atlas",
    "text": "Retrieving metadata from The Cancer Genome Atlas\nThe GenomicDataCommons Bioconductor Package provides an interface to search and retrieve data and metadata from the NIH Genomic Data Commons (GDC), including information from The Cancer Genome Atlas (TCGA) an international collaboration that collected molecular and clinical data on tens of thousands of human tumor samples.\nHere, we retrieve metadata on the subjects and samples available as part of TCGA, and then use dtrackr to select a (hypothetical) subset of samples for analysis.\nWe start by retrieving data on 500 cases\n\ncase_ids = cases() %&gt;% \n  results(size=500L) %&gt;% \n  ids()\nclindat = gdc_clinical(case_ids)\nnames(clindat)\n\n[1] \"demographic\" \"diagnoses\"   \"exposures\"   \"main\"       \n\n\nand obtain four data.frames: demographic, diagnoses, exposures and main with complementary pieces of metadata for each participant. Of note, the diagnoses data.frame can contain multiple rows for the same case_id, e.g. when both primary tumor and a metastasis samples were collected from the same patient.\nNext, we will wrangle it into shape and track our process with dtrackr."
  },
  {
    "objectID": "posts/dtrackr/index.html#default-options",
    "href": "posts/dtrackr/index.html#default-options",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Default options",
    "text": "Default options\nFirst, we set a few default options:\n\nold = options(\n  dtrackr.strata_glue=\"{tolower(.value)}\",\n  dtrackr.strata_sep=\", \",\n  dtrackr.default_message = \"{.count} records\",\ndtrackr.default_headline = \"{paste(.strata, ' ')}\"\n)\n\n\nclindat$demographic %&gt;%\n  comment(\"Demographic\") %&gt;%\n  track() %&gt;% \n  inner_join(\n    dplyr::select(clindat$main, case_id, disease_type),\n    by = \"case_id\", \n    .headline = \"Added disease type\",\n    .messages = c(\"{.count.lhs} records from Demographic table\",\n                  \"joined with {.count.rhs} records from Main table:\",\n                  \"{.count.out} in linked set\")\n  ) %&gt;%\n  include_any(\n    disease_type == \"Adenomas and Adenocarcinomas\" ~ \"{.included} Adenomas/ Adenocarcinomas\",\n    disease_type == \"Ductal and Lobular Neoplasms\" ~ \"{.included} Ductal and Lobular Neoplasms \",\n     disease_type == \"Gliomas\" ~ \"{.included} Gliomas\",\n    .headline = \"Included disease types\") %&gt;%\n  exclude_all(\n    age_at_index&lt;35 ~ \"{.excluded} subjects under 35\",\n    age_at_index&gt;75 ~ \"{.excluded} subjects over 75\",\n    race!=\"white\" ~ \"{.excluded} non-white subjects\",\n    .headline = \"Exclusions:\"\n  ) %&gt;%\n  group_by(disease_type, .messages=\"\") %&gt;%\n  count_subgroup(ethnicity) %&gt;%\n  status(\n    percent_male = sprintf(\"%1.2f%%\", mean(gender==\"male\") * 100),\n    .messages = c(\"male: {percent_male}\")                    \n  ) %&gt;%\n  ungroup(.messages = \"{.count} in final data set\") %&gt;%\n  flowchart()\n\n\n\n\n\n\n\n\n%0\n\n\n\n8:s-&gt;11\n\n\n\n\n\n9:s-&gt;11\n\n\n\n\n\n10:s-&gt;11\n\n\n\n\n\n5:s-&gt;8\n\n\n\n\n\n6:s-&gt;9\n\n\n\n\n\n7:s-&gt;10\n\n\n\n\n\n3:s-&gt;5\n\n\n\n\n\n3:s-&gt;6\n\n\n\n\n\n3:s-&gt;7\n\n\n\n\n\n3:e-&gt;4\n\n\n\n\n\n2:s-&gt;3\n\n\n\n\n\n1:s-&gt;2\n\n\n\n\n\n11\n\n  \n242 in final data set\n\n\n\n8\n\nadenomas and adenocarcinomas  \nmale: 64.71%\n\n\n\n9\n\nductal and lobular neoplasms  \nmale: 1.31%\n\n\n\n10\n\ngliomas  \nmale: 47.37%\n\n\n\n5\n\nadenomas and adenocarcinomas  \nhispanic or latino: 1 items\nnot hispanic or latino: 45 items\nUnknown: 5 items\n\n\n\n6\n\nductal and lobular neoplasms  \nhispanic or latino: 11 items\nnot hispanic or latino: 125 items\nnot reported: 17 items\n\n\n\n7\n\ngliomas  \nhispanic or latino: 2 items\nnot hispanic or latino: 32 items\nUnknown: 4 items\n\n\n\n3\n\nIncluded disease types\ninclusions:\n111 Adenomas/ Adenocarcinomas\n292 Ductal and Lobular Neoplasms \n41 Gliomas\n\n\n\n4\n\nExclusions:\n8 subjects under 35\n27 subjects over 75\n178 non-white subjects\n\n\n\n2\n\nAdded disease type\n500 records from Demographic table\njoined with 500 records from Main table:\n500 in linked set\n\n\n\n1\n\n  \nDemographic\n\n\n\n\n\n\nFinally, we restore the default options:\n\noptions(old)"
  },
  {
    "objectID": "posts/dtrackr/index.html#reproducibility",
    "href": "posts/dtrackr/index.html#reproducibility",
    "title": "Documenting data wrangling with the dtrackr R package",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Information\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21)\n os       macOS Ventura 13.4\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-06-26\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package            * version   date (UTC) lib source\n BiocGenerics         0.46.0    2023-04-25 [1] Bioconductor\n bitops               1.0-7     2021-04-24 [1] CRAN (R 4.3.0)\n cli                  3.6.1     2023-03-23 [1] CRAN (R 4.3.0)\n crayon               1.5.2     2022-09-29 [1] CRAN (R 4.3.0)\n curl                 5.0.1     2023-06-07 [1] CRAN (R 4.3.0)\n digest               0.6.32    2023-06-26 [1] CRAN (R 4.3.0)\n dplyr              * 1.1.2     2023-04-20 [1] CRAN (R 4.3.0)\n dtrackr            * 0.4.0     2023-03-24 [1] CRAN (R 4.3.0)\n evaluate             0.21      2023-05-05 [1] CRAN (R 4.3.0)\n fansi                1.0.4     2023-01-22 [1] CRAN (R 4.3.0)\n fastmap              1.1.1     2023-02-24 [1] CRAN (R 4.3.0)\n generics             0.1.3     2022-07-05 [1] CRAN (R 4.3.0)\n GenomeInfoDb         1.36.1    2023-06-21 [1] Bioconductor\n GenomeInfoDbData     1.2.10    2023-04-30 [1] Bioconductor\n GenomicDataCommons * 1.24.2    2023-05-25 [1] Bioconductor\n GenomicRanges        1.52.0    2023-04-25 [1] Bioconductor\n glue               * 1.6.2     2022-02-24 [1] CRAN (R 4.3.0)\n hms                  1.1.3     2023-03-21 [1] CRAN (R 4.3.0)\n htmltools            0.5.5     2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets          1.6.2     2023-03-17 [1] CRAN (R 4.3.0)\n httr                 1.4.6     2023-05-08 [1] CRAN (R 4.3.0)\n IRanges              2.34.1    2023-06-22 [1] Bioconductor\n jsonlite             1.8.5     2023-06-05 [1] CRAN (R 4.3.0)\n knitr                1.43      2023-05-25 [1] CRAN (R 4.3.0)\n lifecycle            1.0.3     2022-10-07 [1] CRAN (R 4.3.0)\n magrittr             2.0.3     2022-03-30 [1] CRAN (R 4.3.0)\n pillar               1.9.0     2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig            2.0.3     2019-09-22 [1] CRAN (R 4.3.0)\n purrr                1.0.1     2023-01-10 [1] CRAN (R 4.3.0)\n R6                   2.5.1     2021-08-19 [1] CRAN (R 4.3.0)\n rappdirs             0.3.3     2021-01-31 [1] CRAN (R 4.3.0)\n Rcpp                 1.0.10    2023-01-22 [1] CRAN (R 4.3.0)\n RCurl                1.98-1.12 2023-03-27 [1] CRAN (R 4.3.0)\n readr                2.1.4     2023-02-10 [1] CRAN (R 4.3.0)\n rlang                1.1.1     2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown            2.22      2023-06-01 [1] CRAN (R 4.3.0)\n rstudioapi           0.14      2022-08-22 [1] CRAN (R 4.3.0)\n S4Vectors            0.38.1    2023-05-11 [1] Bioconductor\n sessioninfo          1.2.2     2021-12-06 [1] CRAN (R 4.3.0)\n stringi              1.7.12    2023-01-11 [1] CRAN (R 4.3.0)\n stringr              1.5.0     2022-12-02 [1] CRAN (R 4.3.0)\n tibble               3.2.1     2023-03-20 [1] CRAN (R 4.3.0)\n tidyr                1.3.0     2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect           1.2.0     2022-10-10 [1] CRAN (R 4.3.0)\n tzdb                 0.4.0     2023-05-12 [1] CRAN (R 4.3.0)\n utf8                 1.2.3     2023-01-31 [1] CRAN (R 4.3.0)\n V8                   4.3.0     2023-04-08 [1] CRAN (R 4.3.0)\n vctrs                0.6.3     2023-06-14 [1] CRAN (R 4.3.0)\n withr                2.5.0     2022-03-03 [1] CRAN (R 4.3.0)\n xfun                 0.39      2023-04-20 [1] CRAN (R 4.3.0)\n xml2                 1.3.4     2023-04-27 [1] CRAN (R 4.3.0)\n XVector              0.40.0    2023-04-25 [1] Bioconductor\n yaml                 2.3.7     2023-01-23 [1] CRAN (R 4.3.0)\n zlibbioc             1.46.0    2023-04-25 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.3/library\n [2] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/parquet/index.html",
    "href": "posts/parquet/index.html",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "",
    "text": "Today I explored storing gene expression data in parquet files, and querying them with the arrow, duckdb or sparklyr R packages.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/parquet/index.html#tldr",
    "href": "posts/parquet/index.html#tldr",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "",
    "text": "Today I explored storing gene expression data in parquet files, and querying them with the arrow, duckdb or sparklyr R packages."
  },
  {
    "objectID": "posts/parquet/index.html#introduction",
    "href": "posts/parquet/index.html#introduction",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Introduction",
    "text": "Introduction\n\nfor (lib in c(\"arrow\", \"dplyr\", \"duckdb\", \"edgeR\", \"fs\", \"glue\", \"memoise\",\n              \"rnaseqExamples\", \"sparklyr\", \"tictoc\", \"tidyr\", \"DESeq2\", \n              \"Homo.sapiens\", \"Mus.musculus\")) {\n  suppressPackageStartupMessages(\n    library(lib, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)\n  )\n}\n\nA while back, I created the rnaseq-examples R package with serialized SummarizedExperiment objects for three published RNA-seq experiments 1. Here, I will\n\nLoad the data from all three experiments into my R session,\nExtract the raw counts and TMM-normalized gene expression measurements (counts per million) into (tidy) R data.frames,\nStore the data in separate parquet files, one for each experiment and\nQuery the parquet files with the\n\n\narrow,\nduckdb and\nsparklyr R packages.\n\nI was amazed by the speed of duckdb on my local system, and am looking forward to distributing large datasets across nodes of a Spark cluster in the future. Either way, parquet files are a highly portable and language-agnostic file format that I am happy to add to my toolkit."
  },
  {
    "objectID": "posts/parquet/index.html#loading-gene-expression-datasets",
    "href": "posts/parquet/index.html#loading-gene-expression-datasets",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Loading gene expression datasets",
    "text": "Loading gene expression datasets\nThe rnaseqExamplesR package is available from github and can be installed via\n\nremotes::install_github(\"tomsing1/rnaseq-examples\")\n\nOnce installed, we can load the three SummarizedExperiment objects it contains into our R session.\n\ndatasets &lt;- data(package = \"rnaseqExamples\")\nknitr::kable(data.frame(datasets$results[, c(\"Item\", \"Title\")]))\n\n\n\n\n\n\n\n\nItem\nTitle\n\n\n\n\nrnai\nNaguib et al: Global effects of SUPT4H1 RNAi on gene expression\n\n\nsarm1\nZhu et al: Effects of SARM1 deficiency on gene expression\n\n\ntau\nWang et al: time course of microglia from the rTg4510 mouse model\n\n\n\n\n\n\ndata(\"tau\")\n\nNext, I define the tidy() helper function to calculate sizeFactors for normalization with Robinson’s and Oshlack’s TMM method and export raw counts alongside (normalized) counts per million (CPM) in a data.frame.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nNote that I am not exporting sample annotations in this example. This information could be added to the data.frame (with a join operation) or stored in a different format, e.g. for efficient row-wise queries.\nThe helper function is wrapped in a to memoise::memoise(), which caches the outputs in memory. That way, repeated calling of the same function avoids additional computations / joins. This functionality is not really required here, but I will try to remember it for future reference!\n\n\n\n\n\n\nA helper function to coerce a SummarizedExperiment into a tibbbe\n#' Coerce a DGEList or a SummarizedExperiment into a tibble\n#' \n#' @param x Either a `DGEList` or a `SummarizedExperiment`\n#' @return A tibble\n.tidy &lt;- function(x) {\n  \n  # remove non-ensembl feature identifiers (e.g. spike-ins)\n  x &lt;- x[grepl(\"^ENS\", row.names(x)), ]\n  annotation &lt;- dplyr::case_when(\n    all(grepl(\"^ENSG\", row.names(x))) ~ \"Homo.sapiens\",\n    all(grepl(\"^ENSMUS\", row.names(x))) ~ \"Mus.musculus\"\n  )\n  stopifnot(!is.na(annotation))\n  require(annotation, character.only = TRUE)\n  \n  # extract raw counts\n  y &lt;- edgeR::calcNormFactors(x)\n  counts &lt;- y$counts %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"feature_id\") %&gt;%\n    tidyr::pivot_longer(cols = colnames(y), \n                        names_to = \"sample_id\", values_to = \"count\")\n  \n  # extract cpms\n  cpms &lt;- edgeR::cpm(y, normalized.lib.sizes = TRUE, log = FALSE) %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"feature_id\") %&gt;%\n    tidyr::pivot_longer(cols = colnames(y), \n                        names_to = \"sample_id\", values_to = \"cpm\")\n  \n  # add gene annotations & alternative identifiers\n  dplyr::inner_join(\n    counts, cpms, by = c(\"feature_id\", \"sample_id\")\n  ) %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(y$genes, \"feature_id\"),\n      by = \"feature_id\") %&gt;%\n    dplyr::rename(ensembl = \"gene_id\") %&gt;%\n    dplyr::mutate(\n      entrez = suppressMessages({\n        mapIds(get(annotation), keys = .data$ensembl, \n               column = \"ENTREZID\", keytype = \"ENSEMBL\",\n               multiVals = \"first\")\n      })\n    ) %&gt;%\n    dplyr::select(-any_of(\"spikein\"))\n}\n\ntidy &lt;- memoise(.tidy)"
  },
  {
    "objectID": "posts/parquet/index.html#write-parquet-files",
    "href": "posts/parquet/index.html#write-parquet-files",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Write parquet files",
    "text": "Write parquet files\nNow we extract the gene expression measurements from each of our SummarizedExperiment objects and then write it to a parquet file in a temporary directory on the local file system. Alternatively, I could store the files on a cloud system, e.g. AWS S3, and access them remotely.\n\n# create a temporary directory\nout_dir &lt;- file.path(tempdir(), \"parquet\")\nfs::dir_create(out_dir)\n\nfor (dataset in c(\"tau\", \"rnai\", \"sarm1\")) {\n  df &lt;- tidy(get(dataset))\n  df$study &lt;- dataset  # add a columsn with the name of the experiment\n  arrow::write_parquet(\n    x = df, \n    sink = file.path(out_dir, paste0(dataset, \".parquet\"))\n  )\n}\n\n# list the contents of the temporary directory\nfs::dir_info(out_dir) %&gt;%\n  dplyr::select(path, size) %&gt;%\n  dplyr::mutate(path = basename(path))\n\n# A tibble: 3 × 2\n  path                 size\n  &lt;chr&gt;         &lt;fs::bytes&gt;\n1 rnai.parquet        5.22M\n2 sarm1.parquet       5.29M\n3 tau.parquet         8.22M"
  },
  {
    "objectID": "posts/parquet/index.html#querying",
    "href": "posts/parquet/index.html#querying",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Querying",
    "text": "Querying\n\nArrow\nEven though we created three separate files, the arrow R package can abstract them into a single Dataset. We simply point the open_dataset() function at the directory containing the .parquet files.\n\nds &lt;- arrow::open_dataset(out_dir)\nds\n\nFileSystemDataset with 3 Parquet files\nfeature_id: string\nsample_id: string\ncount: int32\ncpm: double\nensembl: string\nsymbol: string\ngene_type: string\nentrez: string\nstudy: string\n\nSee $metadata for additional Schema metadata\n\n\nWe can use dbplr verbs to query this FileSystemDataset:\n\ntic(\"arrow\")\nds %&gt;%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %&gt;%\n  group_by(symbol, study) %&gt;%\n  tally() %&gt;%\n  collect()\n\n# A tibble: 3 × 3\n# Groups:   symbol [2]\n  symbol study     n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 GAPDH  rnai     12\n2 Gapdh  sarm1    16\n3 Gapdh  tau      32\n\ntoc()\n\narrow: 1.717 sec elapsed\n\n\nOn my system, it takes ~ 1 second to retrieve the results, and about the same amount of time is needed to retrieve the full rnai dataset:\n\ntic()\nds %&gt;%\n  filter(study == \"rnai\") %&gt;%\n  collect()\n\n# A tibble: 694,236 × 9\n   feature_id      sample_id count     cpm ensembl symbol gene_type entrez study\n * &lt;chr&gt;           &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;\n 1 ENSG00000163106 S320          0  0      ENSG00… HPGDS  protein_… 27306  rnai \n 2 ENSG00000163106 S168          2  0.0318 ENSG00… HPGDS  protein_… 27306  rnai \n 3 ENSG00000163106 S255          1  0.0111 ENSG00… HPGDS  protein_… 27306  rnai \n 4 ENSG00000163106 S190          0  0      ENSG00… HPGDS  protein_… 27306  rnai \n 5 ENSG00000163110 S912       1607 26.0    ENSG00… PDLIM5 protein_… 10611  rnai \n 6 ENSG00000163110 S832       2320 29.2    ENSG00… PDLIM5 protein_… 10611  rnai \n 7 ENSG00000163110 S783       1022 22.7    ENSG00… PDLIM5 protein_… 10611  rnai \n 8 ENSG00000163110 S623       1159 28.9    ENSG00… PDLIM5 protein_… 10611  rnai \n 9 ENSG00000163110 S622       1384 20.7    ENSG00… PDLIM5 protein_… 10611  rnai \n10 ENSG00000163110 S487       1023 32.6    ENSG00… PDLIM5 protein_… 10611  rnai \n# ℹ 694,226 more rows\n\ntoc()\n\n1.09 sec elapsed\n\n\n\n\nDuckdb\nAlternatively, we can use duckdb to query our parquet files. The duckdb R API supports both dbplyr verbs and raw SQL queries.\nFirst, we establish a connection to the duckdb backend:\n\ncon &lt;- dbConnect(duckdb::duckdb())\n\n\nUsing dbplyr\nFirst, we execute the same dbplyr query we used above, which translates it into SQL for us and passes it on to duckdb.\n\ntic(\"duckdb\")\ntbl(con, sprintf(\"read_parquet('%s/*.parquet')\", out_dir)) %&gt;%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %&gt;%\n  group_by(symbol, study) %&gt;%\n  tally() %&gt;%\n  collect()\n\n# A tibble: 3 × 3\n  symbol study     n\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 GAPDH  rnai     12\n2 Gapdh  tau      32\n3 Gapdh  sarm1    16\n\ntoc()\n\nduckdb: 0.065 sec elapsed\n\n\nOn my system, duckdb returns results more than 10x faster than arrow’s implementation (see above).\nRetrieval of the full dataset for the rnai study is also completed in less than half a second:\n\ntic()\ntbl(con, glue(\"read_parquet('{out_dir}/*.parquet')\")) %&gt;%\n  filter(study == \"rnai\") %&gt;%\n  collect()\n\n# A tibble: 694,236 × 9\n   feature_id      sample_id count   cpm ensembl   symbol gene_type entrez study\n   &lt;chr&gt;           &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;\n 1 ENSG00000000003 S912       1942  31.4 ENSG0000… TSPAN6 protein_… 7105   rnai \n 2 ENSG00000000003 S832       2942  37.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 3 ENSG00000000003 S783        905  20.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 4 ENSG00000000003 S623        960  24.0 ENSG0000… TSPAN6 protein_… 7105   rnai \n 5 ENSG00000000003 S622       2002  29.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 6 ENSG00000000003 S487        792  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 7 ENSG00000000003 S458       1549  20.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 8 ENSG00000000003 S420       1021  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 9 ENSG00000000003 S320       1432  21.5 ENSG0000… TSPAN6 protein_… 7105   rnai \n10 ENSG00000000003 S168       1773  28.2 ENSG0000… TSPAN6 protein_… 7105   rnai \n# ℹ 694,226 more rows\n\ntoc()\n\n0.244 sec elapsed\n\n\n\n\nUsing SQL\nBecause duckdb is a SQL database, we can also query the parquet files directly with raw SQL, realizing another gain in execution speed:\n\ntic()\ndbGetQuery(\n  con = con,\n  glue_sql(\n    \"SELECT symbol, study, COUNT(*) AS n \n     FROM read_parquet({paste0(out_dir, '/*.parquet')}) \n     WHERE UPPER(symbol) = 'GAPDH' \n     GROUP BY symbol, study\", \n    .con = con)\n)\n\n  symbol study  n\n1  GAPDH  rnai 12\n2  Gapdh sarm1 16\n3  Gapdh   tau 32\n\ntoc()\n\n0.024 sec elapsed\n\n\nSimilarly, reading all data for the rnai dataset into memory is faster than with arrow’s implementation as well:\n\ntic()\ndbGetQuery(\n  con = con,\n  glue_sql(\n    \"SELECT * \n     FROM read_parquet({paste0(out_dir, '/*.parquet')}) \n     WHERE study = 'rnai'\", \n    .con = con)\n) %&gt;%\n  head()\n\n       feature_id sample_id count      cpm         ensembl symbol\n1 ENSG00000000003      S912  1942 31.43893 ENSG00000000003 TSPAN6\n2 ENSG00000000003      S832  2942 37.08823 ENSG00000000003 TSPAN6\n3 ENSG00000000003      S783   905 20.10531 ENSG00000000003 TSPAN6\n4 ENSG00000000003      S623   960 23.97706 ENSG00000000003 TSPAN6\n5 ENSG00000000003      S622  2002 29.94743 ENSG00000000003 TSPAN6\n6 ENSG00000000003      S487   792 25.26281 ENSG00000000003 TSPAN6\n       gene_type entrez study\n1 protein_coding   7105  rnai\n2 protein_coding   7105  rnai\n3 protein_coding   7105  rnai\n4 protein_coding   7105  rnai\n5 protein_coding   7105  rnai\n6 protein_coding   7105  rnai\n\ntoc()\n\n0.211 sec elapsed"
  },
  {
    "objectID": "posts/parquet/index.html#spark",
    "href": "posts/parquet/index.html#spark",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Spark",
    "text": "Spark\nFinally, and mainly just for future reference, the sparklyr R package provides an R interface to leverage a Spark cluster and its distributed analysis libraries.\nFirst, we establish a connection to the Spark cluster. Here am running a local Spark cluster (e.g. a single local node) on my laptop:\n\nsc &lt;- spark_connect(master = \"local\")\n\nNext, we import the datasets into the cluster by creating a new Spark DataFrame with the name gene_expression.\n\nsdf &lt;- spark_read_parquet(sc = sc, name = \"gene_expression\",\n                          path = paste0(out_dir, '/*.parquet'))\n\n\ndbplyr\nThe tbl_spark object returned by sparklyr::spark_read_parquet() can be queried with dbplyr verbs, e.g. to translate our now familiar queries into Spark SQL statements on the fly:\n\ntic()\nsdf %&gt;%\n  filter(symbol %in% c(\"GAPDH\", \"Gapdh\")) %&gt;%\n  group_by(symbol, study) %&gt;%\n  tally() %&gt;%\n  collect()\n\n# A tibble: 3 × 3\n  symbol study     n\n  &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n1 Gapdh  sarm1    16\n2 GAPDH  rnai     12\n3 Gapdh  tau      32\n\ntoc()\n\n1.472 sec elapsed\n\n\n\ntic()\nsdf %&gt;%\n  filter(study == \"rnai\") %&gt;%\n  collect()\n\n# A tibble: 694,236 × 9\n   feature_id      sample_id count   cpm ensembl   symbol gene_type entrez study\n   &lt;chr&gt;           &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;\n 1 ENSG00000000003 S912       1942  31.4 ENSG0000… TSPAN6 protein_… 7105   rnai \n 2 ENSG00000000003 S832       2942  37.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 3 ENSG00000000003 S783        905  20.1 ENSG0000… TSPAN6 protein_… 7105   rnai \n 4 ENSG00000000003 S623        960  24.0 ENSG0000… TSPAN6 protein_… 7105   rnai \n 5 ENSG00000000003 S622       2002  29.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 6 ENSG00000000003 S487        792  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 7 ENSG00000000003 S458       1549  20.9 ENSG0000… TSPAN6 protein_… 7105   rnai \n 8 ENSG00000000003 S420       1021  25.3 ENSG0000… TSPAN6 protein_… 7105   rnai \n 9 ENSG00000000003 S320       1432  21.5 ENSG0000… TSPAN6 protein_… 7105   rnai \n10 ENSG00000000003 S168       1773  28.2 ENSG0000… TSPAN6 protein_… 7105   rnai \n# ℹ 694,226 more rows\n\ntoc()\n\n3.329 sec elapsed\n\n\n\n\nSQL\nAlternatively, we can use SQL queries to query the cluster’s gene_expression table directly:\n\ntic()\ndbGetQuery(\n  con = sc,\n  glue(\n    \"SELECT symbol, study, COUNT(*) AS n \n     FROM gene_expression \n     WHERE UPPER(symbol) = 'GAPDH' \n     GROUP BY symbol, study\")\n)\n\n  symbol study  n\n1  Gapdh sarm1 16\n2  GAPDH  rnai 12\n3  Gapdh   tau 32\n\ntoc()\n\n1.493 sec elapsed\n\n\nMy local Spark instance performs these queries more slowly than e.g. duckdb. But Spark’s real power is in deploying Machine learning models across a (potentially large) cluster, enabling parallel processing of very large datasets by distributing both data and computation across nodes.\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "posts/parquet/index.html#reproducibility",
    "href": "posts/parquet/index.html#reproducibility",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSession Information\n\n\nsessioninfo::session_info(\"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-09-01\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package                            * version    date (UTC) lib source\n P AnnotationDbi                      * 1.62.2     2023-07-02 [?] Bioconductor\n P arrow                              * 13.0.0     2023-08-30 [?] CRAN (R 4.3.0)\n P Biobase                            * 2.60.0     2023-05-08 [?] Bioconductor\n P BiocGenerics                       * 0.46.0     2023-06-04 [?] Bioconductor\n P DBI                                * 1.1.3      2022-06-18 [?] CRAN (R 4.3.0)\n P DESeq2                             * 1.40.2     2023-07-02 [?] Bioconductor\n P dplyr                              * 1.1.2      2023-04-20 [?] CRAN (R 4.3.0)\n P duckdb                             * 0.8.1-3    2023-09-01 [?] CRAN (R 4.3.1)\n P edgeR                              * 3.42.4     2023-06-04 [?] Bioconductor\n P fs                                 * 1.6.3      2023-07-20 [?] CRAN (R 4.3.0)\n P GenomeInfoDb                       * 1.36.1     2023-07-02 [?] Bioconductor\n P GenomicFeatures                    * 1.52.2     2023-08-27 [?] Bioconductor\n P GenomicRanges                      * 1.52.0     2023-05-08 [?] Bioconductor\n P glue                               * 1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n P GO.db                              * 3.17.0     2023-08-25 [?] Bioconductor\n P Homo.sapiens                       * 1.3.1      2023-09-01 [?] Bioconductor\n P IRanges                            * 2.34.1     2023-07-02 [?] Bioconductor\n P limma                              * 3.56.2     2023-06-04 [?] Bioconductor\n P MatrixGenerics                     * 1.12.3     2023-07-30 [?] Bioconductor\n P matrixStats                        * 1.0.0      2023-06-02 [?] CRAN (R 4.3.0)\n P memoise                            * 2.0.1      2021-11-26 [?] CRAN (R 4.3.0)\n P Mus.musculus                       * 1.3.1      2023-09-01 [?] Bioconductor\n P org.Hs.eg.db                       * 3.17.0     2023-08-25 [?] Bioconductor\n P org.Mm.eg.db                       * 3.17.0     2023-08-23 [?] Bioconductor\n P OrganismDbi                        * 1.42.0     2023-05-08 [?] Bioconductor\n P rnaseqExamples                     * 0.0.0.9000 2023-09-01 [?] Github (tomsing1/rnaseq-examples@ac35304)\n P S4Vectors                          * 0.38.1     2023-05-08 [?] Bioconductor\n P sparklyr                           * 1.8.2      2023-07-01 [?] CRAN (R 4.3.0)\n P SummarizedExperiment               * 1.30.2     2023-06-11 [?] Bioconductor\n P tictoc                             * 1.2        2023-04-23 [?] CRAN (R 4.3.0)\n P tidyr                              * 1.3.0      2023-01-24 [?] CRAN (R 4.3.0)\n P TxDb.Hsapiens.UCSC.hg19.knownGene  * 3.2.2      2023-09-01 [?] Bioconductor\n P TxDb.Mmusculus.UCSC.mm10.knownGene * 3.10.0     2023-09-01 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/parquet/index.html#footnotes",
    "href": "posts/parquet/index.html#footnotes",
    "title": "Adventures with parquet: Storing & quering gene expression data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you are curious, please consult the vignettes to see examples of differential expression analyses for each dataset.↩︎"
  },
  {
    "objectID": "posts/custom-badges/index.html",
    "href": "posts/custom-badges/index.html",
    "title": "Creating custom badges for your README",
    "section": "",
    "text": "Today I learned how to create custom badges with shields.io, and how to add them to the README.md file on github.\n\nPredefined badges\nMany open source software packages display key pieces of information as badges (aka shields) in their github README, indicating e.g. code coverage, unit test results, version numbers, license, etc.\nThe shields.io website provides many different ready-to-use badges, covering topics such as test results, code coverage, social media logos, activity, and many more.\n     \nBadges can show up to date information. For example, this badge shows the last commit to the github repository for this blog: . They can be returned either in svg (recommended) or png formats, from the img.shields.io and raster.shields.io servers, respectively.\n\n\nCustom badges\nIn addition to predefined outputs, you can also generate your own, entirely custom badges. They can be static like this one  or dynamically retrieve information from a JSON endpoint of your choice.\n\n\nAdding badges to a README.md file\nTo embed badges into your README.md, simply wrap its URL in markdown and surround it with the badges: start and badges: end tags:\n&lt;!-- badges: start --&gt;\n![](https://img.shields.io/github/last-commit/tomsing1/blog)\n&lt;!-- badges: end --&gt;\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html",
    "href": "posts/nextflow-core-quantseq-2-output/index.html",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the second of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis post documents the output files & folders of the nf-core/rnaseq workflow (v 3.10.1), run with default settings with the star_salmon aligner / quantitation method.\nFor additional information, e.g. on the content of the MultiQC report, please see the official nf-core/rnaseq documentation."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#reports",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#reports",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Reports",
    "text": "Reports\n\nMultiQC report\nThe MultiQC HTML report is a one-stop-shop that summarises QC metrics across the workflow. It can be found int he multiqc folder, in a subdirectory named according to the aligner & quantifier combination used (default: star_salmon).\n\n\n\nmultiqc report\n\n\n\n\nPipeline info\nThe pipeline_info folder contains html reports and text (CSV, TXT, YML) files with information about the run, including the versions of the software tools used.\n\n\n\npipeline info\n\n\n\n\nFastQC reports\nThe fastqc folder contains the output of the fastqc tool. Most of the reported metrics are included in the MultiQC report as well, but the HTML reports for individual samples are available here if needed.\n\n\n\nFastQC report\n\n\n\n\nTrim Galore reports\nThe trimgalore folder contains\n\ntrimming reports for each sample\nthe fastqc sub-folder with quality metrics for the trimmed FASTQ files\n\n\n\n\nTrim Galore\n\n\n\n\numitools\nThis folder contains the log files returned by UMI-tools\n\n\n\nUMI-tools"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#workflow-results",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#workflow-results",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Workflow results",
    "text": "Workflow results\nThe main output of the workflow is available in the star_salmon subdirectory. (This folder is named after the selected alignment & quantification strategy, e.g. star_salmon is present only if this tool combination was used.)\nIt contains multiple folders, as well as the (deduplicated) sorted BAM files for each sample.\n\nbigwig\nGenome coverage in bigWig format for each sample.\n\n\n\nBigWig files\n\n\n\n\nDESeq2 object & QC metrics\nThe workflow aggregates all counts into a DESeq2 R objects, performs QC and exploratory analyses and serializes the object as deseq2.dds.RData.\n\n\n\nDESeq2 object & QC metrics\n\n\n\n\nDupradar\nThe dupRadar Bioconductor package performs duplication rate quality control.\n\n\n\ndupRadar\n\n\n\n\nFeaturecounts\nOutput from featurecounts tool is only used to generate QC metrics. For actual quantitation of the gene-level results, the output of salmon (default) or rsem are used.\nThe metrics reported in the featurecounts folder are included in the MultiQC report.\n\n\n\nfeatureCounts\n\n\n\n\nSTAR log files\nThis folder contains log files output by the STAR aligner.\n\n\n\nSTAR logs\n\n\n\n\nQualimap\nThe qualimap package generates QC metrics from BAM files.\n\n\n\nQualimap\n\n\n\n\nRSEQC\nThe output of the rseqc QC control package are in this directory.\n\n\n\nRSEQC"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#alignments-gene-level-counts",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#alignments-gene-level-counts",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Alignments & gene-level counts",
    "text": "Alignments & gene-level counts\nThe star_salmon folder also contains the main results of the workflow: gene-level counts and alignments (BAM files).\n\nSalmon quantitation\n\nAggregated\nThe workflow outputs salmon quantitation results aggregated across all samples. Different types of counts (e.g. raw, length-scaled, TPMs) are available - the choice for downstream analyses depends on the chosen approach. Please see tximport for details.\n\n\n\nSalmon aggrated output\n\n\n\n\nBy sample\nIn addition, the salmon outputs for individual samples are avialable in sub-folders, one for each sample.\n\n\n\nSalmon output for each sample\n\n\n\n\n\nSTAR alignments\nThe sorted (and, in the case of datasets that include UMIs, deduplicated) BAM files and their indices are available:\n\n\n\nSTAR alignments"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-2-output/index.html#genome-gene-annotations-indices",
    "href": "posts/nextflow-core-quantseq-2-output/index.html#genome-gene-annotations-indices",
    "title": "QuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output",
    "section": "Genome, gene annotations & indices",
    "text": "Genome, gene annotations & indices\nIf the workflow was exectuted with the \"save_reference\": true parameter, then all reference files (FASTA, GTF, BED, etc) and the indices generated for STAR, salmon and rsem are returned in the genome folder within the output directory:\n\n\n\ngenome folder\n\n\nThese files can be reused for future runs, shortening the execuction time of the workflow.\nNext, we will compare how the gene-counts returned of the nf-core/rnaseq workflow compare to those posted on GEO by the authors of the two datasets we processed in the first post in this series by performing a differential expression analysis in the third and fourth posts in this series."
  },
  {
    "objectID": "posts/interactive-gene-set-results/index.html",
    "href": "posts/interactive-gene-set-results/index.html",
    "title": "Interactive GSEA results: visualizations with reactable & plotly",
    "section": "",
    "text": "As a Computational Biologist, I frequently analyze data from high throughput experiments, including transcriptomics, proteomics or metabolomics results. As a first step, I usually examine the behavior of individual analysis - genes, proteins or metabolites - and obtain a long list of effect sizes, p- or q-values.\nFrequently, another layer of analysis focuses on the behavior of predefined gene sets, e.g. groups of genes whose up- or down-regulation reflects the activity of a biological process, a metabolic pathway or is indicative of a cellular state.\nThere are numerous methods to perform gene set enrichment (GSEA), over-representation (ORA) or pathway analysis, with more than 140 R packages on Bioconductor alone.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/interactive-gene-set-results/index.html#sharing-analysis-results",
    "href": "posts/interactive-gene-set-results/index.html#sharing-analysis-results",
    "title": "Interactive GSEA results: visualizations with reactable & plotly",
    "section": "Sharing analysis results",
    "text": "Sharing analysis results\nRegardless of the chosen statistical approach, GSEA or ORA analyses typically produce set-level statistics, e.g. a summary of the effect size across all members of a gene set alongside a statistic, p-value, etc.\nTo share results with my collaborators, I would like to enable them to\n\nBrowse set-level results to hone in on specific pathways / processes of interest.\nVisualize the behavior of the analytes in the set.\nDrill down to a subset of analytes and export e.g. gene-level results\n\nThe pioneering ReportingTools Bioconductor package creates static web pages for gene-set enrichment results, including gene- and set-level plots and statistics. But all of the plots are generated in advance, and interactivity is limited.\nIn this blog post, I take advantage of the reactable, plotly, crosstalk and htmlwidgets R packages to create a stand-alone interactive HTML report, allowing my collaborators to explore the results without the need for a server.\nI learned a lot about these incredibly useful packages!\n\n\n\n\n\n\nNote\n\n\n\nAt the time of writing, the current release of the reactable R package (v0.4.1) is not compatible with the latest release of the htmlwidgets (v1.6.0). This issue has already been fixed in reactable’s development version, which is available from github Alternatively, you can use the previous release of htmlwidgets (v1.5.4), e.g.  by installing it with remotes::install_version(\"htmlwidgets\", version = \"1.5.4\").\n\n\n\nFeatures\nHere, I am combining several interactive elements, linked through SharedData objects via crosstalk:\n\nAt the top, an interactive volcano plot showing the effects sizes (mean trimmed log2 fold changes) and nominal p-values for each tested gene set.\nBelow, a nested reactable table displays the results for each set. When a row is expanded\n\nIt shows a volcano plot with gene-level results, as well as a linked table with the corresponding statistics.\nThe reader can hone in on specific genes by selecting points in the volcano plot, or by searching the table.\n\n\nFirst, we define a set of helper functions are, which are composed into the main gene_set_report() function.\n\n\nCode\nlibrary(Biobase)\nlibrary(crosstalk)\nlibrary(dplyr)\nlibrary(htmltools)\nlibrary(plotly)\nlibrary(reactable)\nlibrary(sparrow)\nlibrary(stringr)\nlibrary(htmlwidgets)  \nlibrary(V8)  # to create static HTML\n\n#' Retrieve gene-level statistics for a single gene set\n#' \n#' @param stats named list of data.frames with gene-level statistics, one for\n#' each gene set \n#' @gene_set_name Scalar character, the name of an element of `stats`.\n#' in `data`.\n#' @return A data.frame with results for a single gene set\n.get_gene_data &lt;- function(mg, gene_set_name, keep.cols = c(\n  \"symbol\", \"entrez_id\", \"logFC\", \"pval\", \"CI.L\", \"CI.R\", \"pval\", \"padj\")) {\n  sparrow::geneSet(mg, name = gene_set_name) %&gt;%\n    dplyr::select(tidyselect::any_of(keep.cols)) %&gt;%\n    dplyr::arrange(pval)\n}\n\n#' @importFrom htmltools tags\n.entrez_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\"http://www.ncbi.nlm.nih.gov/gene/%s\",\n                   value)\n    return(htmltools::tags$a(href = url, target = \"_blank\", \n                             as.character(value)))\n  } else {\n    return(value)\n  }\n}\n\n#' @importFrom htmltools tags\n.symbol_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\n      \"https://www.genenames.org/tools/search/#!/?query=%s\",\n      value)\n    return(\n      htmltools::tags$a(href = url, target = \"_blank\", as.character(value))\n    )\n  } else {\n    return(value)\n  }\n}\n\n#' @importFrom htmltools tags\n.msigdb_url &lt;- function(value) {\n  if(!is.na(value) & nzchar(value)) {\n    url &lt;- sprintf(\n      \"https://www.gsea-msigdb.org/gsea/msigdb/human/geneset/%s.html\",\n      value)\n    return(\n      htmltools::tags$a(href = url, target = \"_blank\", as.character(value))\n    )\n  } else {\n    return(value)\n  }\n}\n\n#' Create a reactable table with gene-level results\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param defaultColDef A list that defines the default configuration for a \n#' column, typically the output of the [reactable::colDef] function.\n#' @param columns A list of column definitions, each generated with the \n#' [reactable::colDef] function.\n#' @param theme A `reactableTheme` object, typically generated with a call to\n#' the [reactable::reactableTheme] function.\n#' @param striped Scalar flag, display stripes?\n#' @param bordered Scalar flag, display borders?\n#' @param highlight Scalar flag, highlight selected rows?\n#' @param searchable Scalar flag, add search box?\n#' @param defaultPageSize Scalar integer, the default number of rows to display.\n#' @param elementId Scalar character, an (optional) element identifier\n#' @param ... Additional arguments for the [reactable::reactable] function.\n#' @return A `reactable` object.\n#' @export\n#' @importFrom reactable colDef reactable colFormat\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    symbol = c(\"TP53\", \"KRAS\", \"PIK3CA\"),\n#'    pval = runif(3, 0, 1),\n#'    logFC = rnorm(3)\n#' )\n#' stats_table(df)\n#' }\nstats_table &lt;- function(\n    data, \n    defaultColDef = reactable::colDef(\n      align = \"center\",\n      minWidth = 100,\n      sortNALast = TRUE\n    ),\n    columns = list(\n      symbol = reactable::colDef(\n        name = \"Symbol\",\n        cell = .symbol_url\n      ),\n      entrezid = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      entrez_id = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      entrez = reactable::colDef(\n        name = \"EntrezId\",\n        cell = .entrez_url\n      ),\n      pval = reactable::colDef(\n        name = \"P-value\",\n        format = reactable::colFormat(digits = 4)),\n      padj = reactable::colDef(\n        name = \"P-value\",\n        format = reactable::colFormat(digits = 4)),\n      t = reactable::colDef(\n        name = \"t-statistic\",\n        format = reactable::colFormat(digits = 2)),\n      B = reactable::colDef(\n        name = \"log-odds\",\n        format = reactable::colFormat(digits = 2)),\n      AveExpr = reactable::colDef(\n        name = \"Mean expr\",\n        format = reactable::colFormat(digits = 2)),\n      CI.L = reactable::colDef(\n        name = \"Lower 95% CI\",\n        format = reactable::colFormat(digits = 2)),\n      CI.R = reactable::colDef(\n        name = \"Upper 95% CI\",\n        format = reactable::colFormat(digits = 2)),\n      logFC = reactable::colDef(\n        name = \"logFC\", \n        format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- \"firebrick\"\n          } else if (value &lt; 0) {\n            color &lt;- \"navy\"\n          } else {\n            color &lt;- \"lightgrey\"\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      )\n    ),\n    theme = reactable::reactableTheme(\n      stripedColor = \"#f6f8fa\",\n      highlightColor = \"#f0f5f9\",\n      cellPadding = \"8px 12px\",\n      style = list(\n        fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, \n        Arial, sans-serif\")\n    ),\n    striped = FALSE,\n    bordered = FALSE,\n    highlight = TRUE,\n    searchable = TRUE,\n    defaultPageSize = 10L,\n    elementId = NULL,\n    ...\n) {\n  reactable::reactable(\n    data = data,\n    searchable = searchable,\n    striped = striped,\n    bordered = bordered,\n    highlight = highlight,\n    selection = \"multiple\",\n    onClick = \"select\",\n    rowStyle = list(cursor = \"pointer\"),\n    theme = theme,\n    defaultPageSize = defaultPageSize,\n    defaultColDef = defaultColDef,\n    columns = columns,\n    elementId = elementId,\n    ...\n  )\n}\n\n#' Wrap stats_table() output in a div html tag\n#' \n#' @param style Scalar character, the style tag for the tag\n#' @param elementId Scalar character, the element identifier\n#' @param ... Arguments passed on to the `stats_table` function.\n#' @return A `shiny.tag` object.\n#' @importFrom htmltools div tags\n.stats_table_div &lt;- function(\n    style = paste0(\n      \"width: 50%;\",\n      \"float: right;\",\n      \"padding-top: 1rem;\"\n    ),\n    elementId = NULL,\n    ...) {\n  if (is.null(elementId)) {\n    elementId &lt;- basename(tempfile(pattern = \"id\"))\n  }    \n  htmltools::div(\n    style = style,\n    htmltools::tagList(\n      stats_table(..., elementId = elementId),\n      # download button\n      htmltools::tags$button(\n        \"\\u21E9 Download as CSV\",\n        onclick = sprintf(\"Reactable.downloadDataCSV('%s', 'gene-results.csv')\",\n                          elementId)\n      )\n    )    \n  )\n}\n\n#' Create an interactive volcano plot\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param x A `formula` defining the column of `data` mapped to the x-axis.\n#' @param y A `formula` defining the column of `data` mapped to the y-axis.\n#' @param text A `formula` defining the column of `data` mapped to the tooltip.\n#' @param title Scalar character, the title of the plot.\n#' @param xlab Scalar character, the title of the x-axis\n#' @param ylab Scalar character, the title pf the y-axis\n#' @param title.width Scalar integer, the target line width (passed on to the\n#' [stringr::str_wrap] function.\n#' @param opacity Scalar numeric between 0 and 1, the opacity of the points.\n#' @param marker A list defining the size, line and color limits of the points.\n#' @param colors Character vector of colors used to shade the points.\n#' @param highlight.color Scalar character, the color used to highlight selected\n#' points.\n#' @param webGL Scalar flag, use webGL to render the plot?\n#' @param width Scalar numeric or scalar character, width of the plot\n#' @param height Scalar numeric or scalar character, height of the plot\n#' @param ... Additional arguments passed to the [plotly::plot_ly] function.\n#' @return A `plotly` object.\n#' @importFrom plotly plot_ly add_trace config layout highlight toWebGL\n#' @importFrom grDevices colorRampPalette\n#' @export\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    symbol = letters,\n#'    pval = runif(length(letters), 0, 1),\n#'    logFC = rnorm(length(letters))\n#' )\n#' volcano_plot(df)\n#' }\nvolcano_plot &lt;- function(\n    data, \n    x = ~logFC,\n    y = ~-log10(pval),\n    text = ~symbol,\n    title = \"\",\n    xlab = \"Fold change (log2)\",\n    ylab = \"-log10(pval)\",\n    title.width = 35L,\n    opacity = 0.5,\n    marker = list(\n      color = ~logFC,\n      size = 10, \n      cmax = 3,\n      cmid = 0,\n      cmin = -3,\n      line = list(color = \"grey\", width = 1)),\n    colors = grDevices::colorRampPalette(\n      c('navy', 'lightgrey', 'firebrick'))(15),\n    highlight.color = \"red\",\n    webGL = FALSE,\n    width = NULL,\n    height = NULL,\n    ...) {\n  p &lt;- plotly::plot_ly(\n    width = width,\n    height = height\n  ) %&gt;% \n    plotly::add_trace(\n      data = data, \n      name = \"\",\n      type = 'scatter',\n      mode = 'markers',\n      x = x,\n      y = y,\n      text = text,\n      hoverinfo =\"text\",\n      opacity = opacity,\n      colors = colors,\n      marker = marker,\n      ...\n    ) %&gt;%\n    plotly::config(displaylogo = FALSE) %&gt;%\n    plotly::layout(\n        xaxis = list(title = xlab),\n        yaxis = list(title = ylab),\n        title = stringr::str_wrap(\n          stringr::str_replace_all(title, \"_\", \" \"),\n          width = title.width)\n    ) %&gt;%\n    plotly::highlight(\n      color = highlight.color,\n      on = \"plotly_selected\",\n      off = \"plotly_deselect\"\n    )\n  if (isTRUE(webGL)) p &lt;- plotly::toWebGL(p)\n  return(p)\n}\n\n#' Create an interactive volcano plot for gene-set results\n#' \n#' @param data A data.frame or a `SharedData` object.\n#' @param x A `formula` defining the column of `data` mapped to the x-axis.\n#' @param y A `formula` defining the column of `data` mapped to the y-axis.\n#' @param text A `formula` defining the column of `data` mapped to the tooltip.\n#' @param xlab Scalar character, the title of the x-axis\n#' @param text.width Scalar integer, the target line width (passed on to the\n#' [stringr::str_wrap] function.\n#' @param hovertemplate Scalar character defining the tooltip template.\n#' @param marker A list defining the size, line and color limits of the points.\n#' @param width Scalar numeric or scalar character, width of the plot\n#' @param height Scalar numeric or scalar character, height of the plot\n#' @param ... Additional arguments passed to the [volcano_plot] function.\n#' @return A `plotly` object.\n#' @importFrom grDevices colorRampPalette\n#' @importFrom stringr str_wrap str_replace_all\n#' @export\n#' @examples\n#' \\dontrun{\n#' df &lt;- data.frame(\n#'    name = paste(\"Set\", letters),\n#'    pval = runif(length(letters), 0, 1),\n#'    mean.logFC.trim = rnorm(length(letters)),\n#'    n = sample(1:100, size = length(letters))\n#' )\n#' volcano_gene_set_plot(df)\n#' }\nvolcano_gene_set_plot &lt;- function(\n    data,\n    text = ~stringr::str_wrap(\n          stringr::str_replace_all(name, \"_\", \" \"),\n          width = text.width),\n    text.width = 25,\n    x = ~mean.logFC.trim,\n    y = ~-log10(pval),\n    marker = list(\n      color = ~mean.logFC.trim,\n      size = ~n, \n      sizemode = 'area', \n      cmax = 2,\n      cmid = 0,\n      cmin = -2,\n      line = list(color = \"grey\", width = 1)\n    ), \n    hovertemplate = paste(\n            '&lt;b&gt;%{text}&lt;/b&gt;',\n            '&lt;br&gt;&lt;i&gt;logFC&lt;/i&gt;: %{x:.2f}',\n            '&lt;br&gt;&lt;i&gt;-log10(pval)&lt;/i&gt;: %{y:.2f}',\n            '&lt;br&gt;&lt;i&gt;n&lt;/i&gt;: %{marker.size}',\n            '&lt;br&gt;'),\n    xlab = \"Fold change (log2)\",\n    width = NULL,\n    height = NULL,\n    ...)\n{\n  volcano_plot(\n    data = data, \n    text = text, \n    x = x, \n    y = y,\n    marker = marker, \n    xlab = xlab,\n    hovertemplate = hovertemplate,\n    width = width,\n    height = height,\n    ...)\n}\n\n#' Wrap volcano_plot() output in a div html tag\n#' \n#' @param helptext Scalar character, text to display below the plot.\n#' @param style Scalar character, the style tag for the tag\n#' @param ... Arguments passed on to the `volcano_plot` function.\n#' @return A `shiny.tag` object.\n#' @importFrom htmltools div tagList p\n.volcano_plot_div &lt;- function(\n    helptext = paste(\"Draw a rectangle / use the lasso tool to select points,\",\n                     \"double-click to deselect all.\"), \n    style = paste0(\n      \"width: 50%;\",\n      \"float: left;\",\n      \"padding-right: 1rem;\",\n      \"padding-top: 4rem;\"\n    ), \n    ...) {\n  htmltools::div(\n    style = style, {\n      htmltools::tagList(\n        volcano_plot(...),\n        htmltools::p(helptext)\n      )\n    }\n  )\n}\n\n#' Helper function to combine gene-level outputs into a single div\n#' \n#' @param data A data.frame with gene-set results.\n#' @param stats A named list of data.frames whose names much match the `name`\n#' column of `data`.\n#' @param index Scalar count, the row of `data` to plot.\n#' @return A `shiny.tag` object containing the output of the \n#' `.volcano_plot_div()` and `.stats_table_div()` functions.\n#' @importFrom crosstalk SharedData\n#' @importfrom htmltools tagList div\n.row_details &lt;- function(data, mg, index) {\n  gene_data &lt;- .get_gene_data(mg = mg, gene_set_name = data$name[index])\n  gd &lt;- crosstalk::SharedData$new(gene_data)\n  htmltools::div(\n    htmltools::tagList(\n      # volcano plot\n      .volcano_plot_div(data = gd, title = data$name[index]),\n      # interactive gene-stat table\n      .stats_table_div(data = gd)\n    )\n  )\n}\n\n#' Create a nested gene set result table\n#' \n#' @param mg A `SparrowResult` object\n#' @param max.pval Scalar numeric, the largest (uncorrected) p-value for which\n#' to return results.\n#' @param max.results Scalar integer, the top number of rows to return\n#' (ordered by p-value).\n#' @param color.up Scalar character, the color for positive log2 fold changes.\n#' @param color.down Scalar character, the color for negative log2 fold changes.\n#' @param color.ns Scalar character, the color for zero log2 fold change.\n#' @param theme A `reactableTheme` object, typically generated with a call to\n#' the [reactable::reactableTheme] function.\n#' @param defaultColDef A list that defines the default configuration for a \n#' column, typically the output of the [reactable::colDef] function.\n#' @param columns A list of column definitions, each generated with the \n#' [reactable::colDef] function.\n#' @param bordered Scalar flag, display borders?\n#' @param highlight Scalar flag, highlight selected rows?\n#' @param searchable Scalar flag, add search box?\n#' @param striped Scalar flag, alternate row shading?\n#' @param defaultPageSize Scalar integer, the default number of rows to display.\n#' @param pageSizeOptions Integer vector that will populate the pagination menu.\n#' @param paginationType Scalar character, the pagination control to use. Either\n#' `numbers` for page number buttons (the default), `jump` for a page jump, or \n#' `simple` to show 'Previous' and 'Next' buttons only.\n#' @param elementId Scalar character, an (optional) element identifier\n#' @param defaultSorted Character vector of column names to sort by default. Or\n#' to customize sort order, a named list with values of `asc` or `desc`.\n#' @param name_url A function that returns a `shiny.tag` (usually an \n#' `&lt;a href&gt;&lt;/a&gt;` tag) for each element of the `name` column of `data` to link\n#' to more information about the gene set (e.g. on the MSigDb website, etc).\n#' @param ... Additional arguments for the [reactable::reactable] function.\n#' @importFrom reactable reactable reactableTheme colDef colFormat\n#' @return A `reactable` object with one row for each row in `data`, each of\n#' which can be expanded into the output of the `.row_details()` function\n#' for that specific gene set.\n#' @export\n#' @examples\n#' \\dontrun{\n#' library(sparrow)\n#' vm &lt;- sparrow::exampleExpressionSet()\n#' gdb &lt;- sparrow::exampleGeneSetDb()\n#' mg &lt;- sparrow::seas(vm, gdb, c(\"camera\"), design = vm$design, \n#'                     contrast = 'tumor')\n#' gene_set_table(mg, max.results = 10)\n#' }\ngene_set_table &lt;- function(\n    mg,\n    max.pval = 0.05,\n    max.results = Inf,\n    keep.cols = c(\"collection\", \"name\", \"n\", \"pval\", \"padj\", \n                      \"mean.logFC.trim\"),\n    method = resultNames(mg)[1],\n    color.up = \"firebrick\", \n    color.down = \"navy\",\n    color.ns = \"grey50\",\n    theme = reactable::reactableTheme(\n      stripedColor = \"grey95\",\n      highlightColor = \"grey80\",\n      cellPadding = \"8px 12px\",\n      style = list(\n        fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, \n        Arial, sans-serif\")\n    ),\n    defaultColDef = reactable::colDef(\n      header = function(value) value,\n      align = \"center\",\n      minWidth = 100,\n      headerStyle = list(background = \"#f7f7f8\"),\n      sortNALast = TRUE\n    ),\n    name_url = function(value) {value},\n    columns = list(\n      collection = reactable::colDef(\n        name = \"Collection\"),\n      name = reactable::colDef(\n        name = \"Gene set\",\n        cell = name_url,\n        minWidth = 150),\n      pval = reactable::colDef(\n        name = \"P-value\", aggregate = \"min\",\n        format = reactable::colFormat(digits = 4)),\n      padj = reactable::colDef(\n        name = \"FDR\", aggregate = \"min\",\n        format = reactable::colFormat(digits = 4)),\n      Direction =  reactable::colDef(\n        name = \"dir\", minWidth = 45, \n        cell = function(value) {\n          if (value == \"Up\")  \"\\u2B06\" else \"\\u2B07\"\n        }),\n      logFC = reactable::colDef(\n        name = \"logFC\", format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- color.up\n          } else if (value &lt; 0) {\n            color &lt;- color.down\n          } else {\n            color &lt;- color.ns\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      ),\n      mean.logFC.trim = reactable::colDef(\n        name = \"logFC\", format = reactable::colFormat(digits = 2),\n        style = function(value) {\n          if (value &gt; 0) {\n            color &lt;- color.up\n          } else if (value &lt; 0) {\n            color &lt;- color.down\n          } else {\n            color &lt;- color.ns\n          }\n          list(color = color, fontWeight = \"bold\")\n        }\n      )\n    ),\n    elementId = \"expansion-table\",\n    static = TRUE,\n    filterable = TRUE,\n    searchable = TRUE,\n    bordered = TRUE,\n    striped = FALSE,\n    highlight = TRUE,\n    defaultPageSize = 25L,\n    showPageSizeOptions = TRUE,\n    pageSizeOptions = sort(unique(c(25, 50, 100, nrow(data)))),\n    paginationType = \"simple\",\n    defaultSorted = list(pval = \"asc\")\n) {\n  data = sparrow::result(mg, method) %&gt;%\n    dplyr::slice_min(n = max.results, order_by = pval) %&gt;%\n    dplyr::filter(pval &lt;= max.pval) %&gt;%\n    dplyr::select(tidyselect::any_of(keep.cols))\n  if (nrow(data) == 0) {\n    warning(\"None of the gene sets pass the `max.pval` threshold.\")\n    return(NULL)\n  }\n  reactable::reactable(\n    data,\n    elementId = elementId,\n    defaultColDef = defaultColDef,\n    static = static,\n    filterable = filterable,\n    searchable = searchable,\n    bordered = bordered,\n    highlight = highlight,\n    theme = theme,\n    defaultPageSize = defaultPageSize,\n    showPageSizeOptions = showPageSizeOptions,\n    pageSizeOptions = pageSizeOptions,\n    paginationType = paginationType,\n    defaultSorted = defaultSorted,\n    columns = columns,\n    details = function(index) {\n      .row_details(data = data, mg = mg, index)\n    }\n  )\n}\n\n#' Wrapper to create a div HTML tag\n#' @param mg A `SparrowResult` object\n#' @param method Scalar character, which results to return from `mg`.\n#' @param max.pal Scalar numeric, return only results wiht an (uncorrected)\n#' &lt;= `max.pal`.\n#' @param verbose Scalar flag, show messages?\n#' @param title Scalar character, the `h1` title for the element\n#' @param elementId Scalar character, the element identifier for the interactive\n#' table.\n#' @param style Scalar character, the style tag for the tag\n#' @param ... Additional arguments passed on to the `gene_set_table` function.\n#' @return A `shiny.tag` object containing the output of the \n#' `gene_set_table()` function.\n#' @importFrom htmltools div h1 tagList tags\n#' @export\ngene_set_report &lt;- function(\n    mg,\n    method = resultNames(mg)[1], \n    max.pval = 0.05,\n    max.results = Inf,\n    verbose = TRUE,\n    title = \"Gene set enrichment analysis\",\n    elementId = \"expansion-table\",\n    style = \"\",\n    ...\n) {\n  if (!is.finite(max.results)) {\n    message.log &lt;- sprintf(\n      paste(\"Reporting all '%s' results with (uncorrected)\",\n            \"p-value &lt;= %s\"), \n      method, max.pval)\n  } else {\n    message.log &lt;- sprintf(\n      paste(\"Reporting up to %s '%s' results with (uncorrected)\",\n            \"p-value &lt;= %s\"), \n      max.results, method, max.pval)\n  }\n  if (isTRUE(verbose)) {\n    message(message.log)\n  }\n  htmltools::div(\n    style = style, \n    {\n      htmltools::tagList(\n        htmltools::h1(title),\n        htmltools::p(message.log),\n        # volcano plot\n        sparrow::result(mg, method) %&gt;%\n          dplyr::slice_min(n = max.results, order_by = pval) %&gt;%\n          volcano_gene_set_plot(width = \"50%\"),\n        htmltools::br(),\n        # expansion button\n        htmltools::tags$button(\n          \"Expand/collapse all rows\",\n          onclick = sprintf(\"Reactable.toggleAllRowsExpanded('%s')\", elementId)\n        ),\n        gene_set_table(mg = mg, max.pval = max.pval, max.results = max.results,\n                       ...)\n      )\n    })\n}\n\n\nNext, we perform a gene set enrichment analysis with sparrow’s seas function. It returns a convenient SparrowResult S4 object with both gene-set statistics and gene-level differential expression results.\n\n# gene set enrichment analysis with the sparrow Bioconductor package\nvm &lt;- exampleExpressionSet()\ngdb &lt;- exampleGeneSetDb()\nmg &lt;- seas(vm, gdb, methods = \"camera\", design = vm$design, contrast = 'tumor')\n\nFinally, we pass the mg object to our gene_set_report() function, together with arguments requesting that all gene-set results passing a p-value threshold of &lt; 0.05 are included. We also pass the .msigdb_url helper function to the name_url argument, to link the name of each gene set to its description on the msigdb website.\n\n# create an interactive report\nhtmltools::browsable(\n  gene_set_report(mg, method = \"camera\", max.pval = 0.05, max.results = Inf, \n                  name_url = .msigdb_url)\n)\n\nReporting all 'camera' results with (uncorrected) p-value &lt;= 0.05\n\n\n\n\nGene set enrichment analysis\nReporting all 'camera' results with (uncorrected) p-value &lt;= 0.05\n\n\n\nExpand/collapse all rows\nCollectionGene setnP-valueFDRlogFC​​c2SOTIRIOU_BREAST_CANCER_GRADE_1_VS_3_UP1490.00000.00001.83​​c2PUJANA_BREAST_CANCER_WITH_BRCA1_MUTATED_UP550.00000.00000.95​​c2LOPEZ_MESOTELIOMA_SURVIVAL_TIME_UP140.00000.00001.92​​c2TURASHVILI_BREAST_LOBULAR_CARCINOMA_VS_DUCTAL_NORMAL_DN910.00000.0001-0.94​​c2REACTOME_MRNA_SPLICING_MINOR_PATHWAY420.00000.00010.34​​c2BOYAULT_LIVER_CANCER_SUBCLASS_G123_DN410.00000.0005-0.65​​c7GSE3982_BCELL_VS_TH2_DN1750.00030.00310.33​​c7GSE22886_IGG_IGA_MEMORY_BCELL_VS_BLOOD_PLASMA_CELL_UP1760.00130.0106-0.29​​c2MOREAUX_MULTIPLE_MYELOMA_BY_TACI_DN1700.00130.01060.17​​c2BURTON_ADIPOGENESIS_PEAK_AT_2HR500.00150.0106-0.74​​c2CHARAFE_BREAST_CANCER_BASAL_VS_MESENCHYMAL_DN450.00150.0106-0.47​​c2SCHAEFFER_PROSTATE_DEVELOPMENT_48HR_DN3560.00170.0109-0.33​​c2HUPER_BREAST_BASAL_VS_LUMINAL_UP520.00220.0126-0.76​​c7GSE13485_CTRL_VS_DAY3_YF17D_VACCINE_PBMC_DN1910.00250.01370.27​​c2GUTIERREZ_WALDENSTROEMS_MACROGLOBULINEMIA_1_UP90.00730.0368-0.60​​c6KRAS.50_UP.V1_UP330.01330.0632-0.67​​c2YAMASHITA_LIVER_CANCER_STEM_CELL_DN470.01520.0662-0.44​​c6ATM_DN.V1_UP890.01570.0662-0.41​​c6KRAS.LUNG_UP.V1_DN980.02270.0823-0.43​​c2BIOCARTA_AGPCR_PATHWAY110.02270.0823-0.50​​c6KRAS.LUNG.BREAST_UP.V1_DN960.02270.0823-0.34​​c6KRAS.BREAST_UP.V1_DN900.02490.0848-0.31​​c6KRAS.600.LUNG.BREAST_UP.V1_DN1890.02560.0848-0.26​​c6TBK1.DN.48HRS_DN500.02960.08880.17​​c6CAMP_UP.V1_DN1830.03060.0888-0.231–25 of 31 rowsShow 253150100Previous1 of 2Next\n\n\n\n\n\n\n\nDetails\n\nGene set enrichment analysis\nIn this example, I am using Steve Lianoglou’s sparrow Bioconductor package to perform gene set enrichment analysis. But any other method could be used, as long as both set-level and gene-level differential expression statistics can be obtained.\nsparrow supports numerous GSEA and ORA methods. Here, I am using the camera algorithm from the limma Bioconductor package for illustration.\n\n\nReproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package          * version    date (UTC) lib source\n P annotate           1.78.0     2023-05-08 [?] Bioconductor\n P AnnotationDbi      1.62.2     2023-07-02 [?] Bioconductor\n P askpass            1.1        2019-01-13 [?] CRAN (R 4.3.0)\n P babelgene          22.9       2022-09-29 [?] CRAN (R 4.3.0)\n P backports          1.4.1      2021-12-13 [?] CRAN (R 4.3.0)\n P Biobase          * 2.60.0     2023-05-08 [?] Bioconductor\n P BiocGenerics     * 0.46.0     2023-06-04 [?] Bioconductor\n P BiocIO             1.10.0     2023-05-08 [?] Bioconductor\n P BiocManager        1.30.22    2023-08-08 [?] CRAN (R 4.3.0)\n P BiocParallel       1.34.2     2023-05-28 [?] Bioconductor\n P BiocSet            1.14.0     2023-05-08 [?] Bioconductor\n P Biostrings         2.68.1     2023-05-21 [?] Bioconductor\n P bit                4.0.5      2022-11-15 [?] CRAN (R 4.3.0)\n P bit64              4.0.5      2020-08-30 [?] CRAN (R 4.3.0)\n P bitops             1.0-7      2021-04-24 [?] CRAN (R 4.3.0)\n P blob               1.2.4      2023-03-17 [?] CRAN (R 4.3.0)\n P cachem             1.0.8      2023-05-01 [?] CRAN (R 4.3.0)\n P checkmate          2.2.0      2023-04-27 [?] CRAN (R 4.3.0)\n P circlize           0.4.15     2022-05-10 [?] CRAN (R 4.3.0)\n P cli                3.6.1      2023-03-23 [?] CRAN (R 4.3.0)\n P clue               0.3-64     2023-01-31 [?] CRAN (R 4.3.0)\n P cluster            2.1.4      2022-08-22 [?] CRAN (R 4.3.0)\n P codetools          0.2-19     2023-02-01 [?] CRAN (R 4.3.1)\n P colorspace         2.1-0      2023-01-23 [?] CRAN (R 4.3.0)\n P ComplexHeatmap     2.16.0     2023-05-08 [?] Bioconductor\n P crayon             1.5.2      2022-09-29 [?] CRAN (R 4.3.0)\n R credentials        1.3.2      &lt;NA&gt;       [?] &lt;NA&gt;\n P crosstalk        * 1.2.0      2021-11-04 [?] CRAN (R 4.3.0)\n P curl               5.0.2      2023-08-14 [?] CRAN (R 4.3.0)\n P data.table         1.14.8     2023-02-17 [?] CRAN (R 4.3.0)\n P DBI                1.1.3      2022-06-18 [?] CRAN (R 4.3.0)\n P digest             0.6.33     2023-07-07 [?] CRAN (R 4.3.0)\n P doParallel         1.0.17     2022-02-07 [?] CRAN (R 4.3.0)\n P dplyr            * 1.1.2      2023-04-20 [?] CRAN (R 4.3.0)\n P edgeR              3.42.4     2023-06-04 [?] Bioconductor\n P ellipsis           0.3.2      2021-04-29 [?] CRAN (R 4.3.0)\n P evaluate           0.21       2023-05-05 [?] CRAN (R 4.3.0)\n P fansi              1.0.4      2023-01-22 [?] CRAN (R 4.3.0)\n P fastmap            1.1.1      2023-02-24 [?] CRAN (R 4.3.0)\n P foreach            1.5.2      2022-02-02 [?] CRAN (R 4.3.0)\n P generics           0.1.3      2022-07-05 [?] CRAN (R 4.3.0)\n P GenomeInfoDb       1.36.1     2023-07-02 [?] Bioconductor\n P GenomeInfoDbData   1.2.10     2023-08-23 [?] Bioconductor\n P GetoptLong         1.0.5      2020-12-15 [?] CRAN (R 4.3.0)\n P ggplot2          * 3.4.3      2023-08-14 [?] CRAN (R 4.3.0)\n P GlobalOptions      0.1.2      2020-06-10 [?] CRAN (R 4.3.0)\n P glue               1.6.2      2022-02-24 [?] CRAN (R 4.3.0)\n P graph              1.78.0     2023-05-08 [?] Bioconductor\n P gridExtra          2.3        2017-09-09 [?] CRAN (R 4.3.0)\n P GSEABase           1.62.0     2023-05-08 [?] Bioconductor\n P gtable             0.3.4      2023-08-21 [?] CRAN (R 4.3.1)\n P htmltools        * 0.5.6      2023-08-10 [?] CRAN (R 4.3.0)\n P htmlwidgets      * 1.6.2      2023-03-17 [?] CRAN (R 4.3.0)\n P httpuv             1.6.11     2023-05-11 [?] CRAN (R 4.3.0)\n P httr               1.4.7      2023-08-15 [?] CRAN (R 4.3.0)\n P IRanges            2.34.1     2023-07-02 [?] Bioconductor\n P irlba              2.3.5.1    2022-10-03 [?] CRAN (R 4.3.0)\n P iterators          1.0.14     2022-02-05 [?] CRAN (R 4.3.0)\n P jsonlite           1.8.7      2023-06-29 [?] CRAN (R 4.3.0)\n P KEGGREST           1.40.0     2023-05-08 [?] Bioconductor\n P knitr              1.43       2023-05-25 [?] CRAN (R 4.3.0)\n P later              1.3.1      2023-05-02 [?] CRAN (R 4.3.0)\n P lattice            0.21-8     2023-04-05 [?] CRAN (R 4.3.1)\n P lazyeval           0.2.2      2019-03-15 [?] CRAN (R 4.3.0)\n P lifecycle          1.0.3      2022-10-07 [?] CRAN (R 4.3.0)\n P limma              3.56.2     2023-06-04 [?] Bioconductor\n P locfit             1.5-9.8    2023-06-11 [?] CRAN (R 4.3.0)\n P magrittr           2.0.3      2022-03-30 [?] CRAN (R 4.3.0)\n P Matrix             1.5-4.1    2023-05-18 [?] CRAN (R 4.3.1)\n P matrixStats        1.0.0      2023-06-02 [?] CRAN (R 4.3.0)\n P memoise            2.0.1      2021-11-26 [?] CRAN (R 4.3.0)\n P mime               0.12       2021-09-28 [?] CRAN (R 4.3.0)\n P munsell            0.5.0      2018-06-12 [?] CRAN (R 4.3.0)\n P ontologyIndex      2.11       2023-05-30 [?] CRAN (R 4.3.0)\n P openssl            2.1.0      2023-07-15 [?] CRAN (R 4.3.0)\n P pillar             1.9.0      2023-03-22 [?] CRAN (R 4.3.0)\n P pkgconfig          2.0.3      2019-09-22 [?] CRAN (R 4.3.0)\n P plotly           * 4.10.2     2023-06-03 [?] CRAN (R 4.3.0)\n P plyr               1.8.8      2022-11-11 [?] CRAN (R 4.3.0)\n P png                0.1-8      2022-11-29 [?] CRAN (R 4.3.0)\n P promises           1.2.1      2023-08-10 [?] CRAN (R 4.3.0)\n P purrr              1.0.2      2023-08-10 [?] CRAN (R 4.3.0)\n P R6                 2.5.1      2021-08-19 [?] CRAN (R 4.3.0)\n P RColorBrewer       1.1-3      2022-04-03 [?] CRAN (R 4.3.0)\n P Rcpp               1.0.11     2023-07-06 [?] CRAN (R 4.3.0)\n P RCurl              1.98-1.12  2023-03-27 [?] CRAN (R 4.3.0)\n P reactable        * 0.4.4.9000 2023-08-25 [?] Github (glin/reactable@86bd276)\n P reactR             0.4.4      2021-02-22 [?] CRAN (R 4.3.0)\n   renv               1.0.2      2023-08-15 [1] CRAN (R 4.3.0)\n P rjson              0.2.21     2022-01-09 [?] CRAN (R 4.3.0)\n P rlang              1.1.1      2023-04-28 [?] CRAN (R 4.3.0)\n P rmarkdown          2.24       2023-08-14 [?] CRAN (R 4.3.0)\n P RSQLite            2.3.1      2023-04-03 [?] CRAN (R 4.3.0)\n P rstudioapi         0.15.0     2023-07-07 [?] CRAN (R 4.3.0)\n P S4Vectors          0.38.1     2023-05-08 [?] Bioconductor\n P scales             1.2.1      2022-08-20 [?] CRAN (R 4.3.0)\n P sessioninfo        1.2.2      2021-12-06 [?] CRAN (R 4.3.0)\n P shape              1.4.6      2021-05-19 [?] CRAN (R 4.3.0)\n P shiny              1.7.5      2023-08-12 [?] CRAN (R 4.3.0)\n P sparrow          * 1.6.0      2023-05-08 [?] Bioconductor\n P stringi            1.7.12     2023-01-11 [?] CRAN (R 4.3.0)\n P stringr          * 1.5.0      2022-12-02 [?] CRAN (R 4.3.0)\n P sys                3.4.2      2023-05-23 [?] CRAN (R 4.3.0)\n P tibble             3.2.1      2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr              1.3.0      2023-01-24 [?] CRAN (R 4.3.0)\n P tidyselect         1.2.0      2022-10-10 [?] CRAN (R 4.3.0)\n P utf8               1.2.3      2023-01-31 [?] CRAN (R 4.3.0)\n P V8               * 4.3.3      2023-07-18 [?] CRAN (R 4.3.0)\n P vctrs              0.6.3      2023-06-14 [?] CRAN (R 4.3.0)\n P viridis            0.6.4      2023-07-22 [?] CRAN (R 4.3.0)\n P viridisLite        0.4.2      2023-05-02 [?] CRAN (R 4.3.0)\n P withr              2.5.0      2022-03-03 [?] CRAN (R 4.3.0)\n P xfun               0.40       2023-08-09 [?] CRAN (R 4.3.0)\n P XML                3.99-0.14  2023-03-19 [?] CRAN (R 4.3.0)\n P xtable             1.8-4      2019-04-21 [?] CRAN (R 4.3.0)\n P XVector            0.40.0     2023-05-08 [?] Bioconductor\n P yaml               2.3.7      2023-01-23 [?] CRAN (R 4.3.0)\n P zlibbioc           1.46.0     2023-05-08 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the fourth of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring & executing the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis analysis compares the performance of the nf-core/rnaseq workflow for QuantSeq FWD 3’ tag RNAseq data with unique molecular identifiers (UMIs).\nThe differential expression analysis results are highly concordant with those obtained in the original publication.\nWith the appropriate settings, the nf-core/rnaseq workflow is a valid data processing pipeline for this data type.\n\nThe first post in this series walked through the preprocesssing of QuantSeq FWD data published in a preprint by Nugent et al, 2020, who used the QuantSeq FWD library preparation protocol and added unique molecular identifiers (UMIs). The UMIs were used to identify and remove PCR duplicates during the data preprocessing steps.\nHere, we use Bioconductor/R packages to reproduce the downstream results. We perform the same analysis twice with either\n\nthe original counts matrix published by the authors 1\nthe output of the nf-core/rnaseq workflow\n\n\nlibrary(dplyr)\nlibrary(edgeR)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(org.Mm.eg.db)\nlibrary(statmod)\nlibrary(SummarizedExperiment)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#sample-annotations",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#sample-annotations",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Sample annotations",
    "text": "Sample annotations\nWe start by retrieving the sample annotation table, listing e.g. the sex, and genotype for each mouse, and the batch for each collected sample.\nThis information is available in the SRA Run Explorer. (I saved it in the sample_metadata.csv CSV file if you want to follow along&gt;.\n\nsample_sheet &lt;- file.path(work_dir, \"sample_metadata.csv\")\nsample_anno &lt;- read.csv(sample_sheet, row.names = \"Experiment\")\nhead(sample_anno[, c(\"Run\", \"Animal.ID\", \"Age\", \"age_unit\", \"Cell_type\",\n                     \"sex\", \"Genotype\", \"Sample.Name\")])\n\n                  Run Animal.ID Age age_unit Cell_type    sex  Genotype\nSRX6420531 SRR9659551       IL1   2   months astrocyte female TREM2 +/+\nSRX6420532 SRR9659552       IL1   2   months microglia female TREM2 +/+\nSRX6420533 SRR9659553      IL10   2   months astrocyte female TREM2 -/-\nSRX6420534 SRR9659554      IL10   2   months microglia female TREM2 -/-\nSRX6420535 SRR9659555      IL11  16   months astrocyte female TREM2 +/+\nSRX6420536 SRR9659556      IL11  16   months microglia female TREM2 +/+\n           Sample.Name\nSRX6420531  GSM3933549\nSRX6420532  GSM3933550\nSRX6420533  GSM3933551\nSRX6420534  GSM3933552\nSRX6420535  GSM3933553\nSRX6420536  GSM3933554\n\n\nBecause our SRA metadata doesn’t include the GEO sample title, I saved the identifier mappings in the GEO_sample_ids.csv CSV file.\n\ngeo_ids &lt;- read.csv(file.path(work_dir, \"GEO_sample_ids.csv\"))\nhead(geo_ids)\n\n  sample_name sample_id\n1  GSM3933549     IL1_A\n2  GSM3933550     IL1_M\n3  GSM3933551    IL10_A\n4  GSM3933552    IL10_M\n5  GSM3933553    IL11_A\n6  GSM3933554    IL11_M\n\n\n\n\nCode\ncolnames(sample_anno)&lt;- tolower(colnames(sample_anno))\ncolnames(sample_anno) &lt;- sub(\".\", \"_\", colnames(sample_anno), \n                             fixed = TRUE) \nsample_anno &lt;- sample_anno[, c(\"sample_name\", \"animal_id\", \"genotype\", \"sex\",\n                               \"age\", \"cell_type\")]\nsample_anno$genotype &lt;- factor(sample_anno$genotype, \n                               levels = c(\"TREM2 +/+\", \"TREM2 -/-\"))\nsample_anno$genotype &lt;- dplyr::recode_factor(\n  sample_anno$genotype,\"TREM2 +/+\" = \"WT\", \"TREM2 -/-\" = \"KO\")\nsample_anno$age &lt;- factor(sample_anno$age)\nsample_anno$sample_title &lt;- geo_ids[\n  match(sample_anno$sample_name, geo_ids$sample_name), \"sample_id\"]\nhead(sample_anno)\n\n\n           sample_name animal_id genotype    sex age cell_type sample_title\nSRX6420531  GSM3933549       IL1       WT female   2 astrocyte        IL1_A\nSRX6420532  GSM3933550       IL1       WT female   2 microglia        IL1_M\nSRX6420533  GSM3933551      IL10       KO female   2 astrocyte       IL10_A\nSRX6420534  GSM3933552      IL10       KO female   2 microglia       IL10_M\nSRX6420535  GSM3933553      IL11       WT female  16 astrocyte       IL11_A\nSRX6420536  GSM3933554      IL11       WT female  16 microglia       IL11_M\n\n\nThis experiment includes 56 samples of astrocytes or microglia cells obtained from 28 female mice that were either 2- or 16 months of age.\nThe animals are either wildtype (WT) or homozygous knockouts (KO) for the Trem2 gene."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#nugent-et-als-original-count-data",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#nugent-et-als-original-count-data",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Nugent et al’s original count data",
    "text": "Nugent et al’s original count data\nFirst, we retrieve the authors’ count matrix from NCBI GEO, available as a Supplementary tab-delimited text file.\n\ngeo_url &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE134031&\",\n                  \"format=file&file=GSE134031%5FDST120%2Etab%2Egz\")\nraw_counts &lt;- read.delim(textConnection(readLines(gzcon(url(geo_url)))))\nhead(colnames(raw_counts), 10)\n\n [1] \"mgi_symbol\"              \"gene_biotype\"           \n [3] \"ensembl_gene_id_version\" \"IL1_A\"                  \n [5] \"IL1_M\"                   \"IL10_A\"                 \n [7] \"IL10_M\"                  \"IL11_A\"                 \n [9] \"IL11_M\"                  \"IL12_A\"                 \n\n\nThe raw_counts data.frame contains information about the detected genes ( mgi_symbol, ensembl_gene_id_version) and the samples are identified by a shorthand of their GEO title (e.g. IL1_M, IL1_A).\nWe use the raw counts to populate a new DGEList object and perform Library Size Normalization with the TMM approach.\n\n\nCode\ncount_data &lt;- as.matrix(raw_counts[, grep(\"^IL\", colnames(raw_counts))])\nrow.names(count_data) &lt;- raw_counts$ensembl_gene_id_version\ncolnames(count_data) &lt;- row.names(sample_anno)[\n  match(colnames(count_data), sample_anno$sample_title)\n]\n\ngene_data &lt;- data.frame(\n  gene_id = raw_counts$ensembl_gene_id_version,\n  gene_name = raw_counts$mgi_symbol,\n  row.names = raw_counts$ensembl_gene_id_version\n)\n\ncol_data &lt;- data.frame(\n  sample_anno[colnames(count_data),\n              c(\"sample_title\", \"animal_id\",  \"age\", \"genotype\", \"cell_type\")],\n  workflow = \"geo\"\n)\n\ndge &lt;- DGEList(\n  counts = as.matrix(count_data), \n  samples = col_data[colnames(count_data), ], \n  genes = gene_data[row.names(count_data), ]\n)\n\ndge &lt;- calcNormFactors(dge, method = \"TMM\")\n\n\nThis is a large dataset, containing e.g. samples from two different cell types (microglia and astrocytes) and two different age groups (2 and 16 months).\nHere, we will restrict the analysis to microglia samples collected from older animals.\n\ndge &lt;- dge[, dge$samples$cell_type == \"microglia\" & dge$samples$age == \"16\"]\n\nLet’s identify which genes are significantly differentially expressed between the two genotypes!\n\nLinear modeling with limma/voom\nFirst, we use the edgeR::filterByExpr() function to identify genes with sufficiently large counts to be examined for differential expression.\n\ndesign &lt;- model.matrix(~ genotype, data = dge$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge, design = design)\n\nNext, we fit a linear model to the data using the limma/voom approach. The model only includes the genotype (with WT as the reference level) as a fixed effect.\n\nfit &lt;- voomLmFit(\n  dge[keep, row.names(design)], \n  design = design,\n  sample.weights = TRUE, \n  plot = FALSE\n)\nfit &lt;- eBayes(fit, robust=TRUE)\n\nThe following table displays the number of differentially up- and down-regulated genes after applying a false-discovery (adj.P.Val) threshold of 5%. We detect significant differences between KO and WT animals in a small number of genes\n\nsummary(decideTests(fit)[, \"KO\"])\n\n         KO\nDown     10\nNotSig 6343\nUp        2\n\n\nThe top 10 genes with the smallest p-values include well known markers of microglia activation:\n\ntopTreat(fit, coef = \"KO\")[, c(\"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")]\n\n                      gene_name      logFC      P.Value    adj.P.Val\nENSMUSG00000015568.16       Lpl -3.1524285 2.088121e-17 1.327001e-13\nENSMUSG00000023992.14     Trem2 -2.0556690 3.260855e-15 1.036137e-11\nENSMUSG00000079293.11    Clec7a -1.5285199 9.357950e-12 1.982326e-08\nENSMUSG00000029304.14      Spp1 -4.6265897 4.968572e-10 7.893819e-07\nENSMUSG00000003418.11   St8sia6 -1.6639344 7.279411e-09 8.240295e-06\nENSMUSG00000002602.16       Axl -1.2096969 7.779979e-09 8.240295e-06\nENSMUSG00000068129.5       Cst7 -2.0966828 9.881687e-08 8.971160e-05\nENSMUSG00000008845.9      Cd163  0.8165263 1.513307e-06 1.202133e-03\nENSMUSG00000039109.16     F13a1  0.8081490 1.735344e-06 1.225345e-03\nENSMUSG00000000682.7       Cd52 -0.7939756 4.846100e-06 3.079696e-03\n\n\nNext we repeat the same analysis with the output of the nf-core/rnaseq workflow."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#nf-corernaseq-results",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#nf-corernaseq-results",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "nf-core/rnaseq results",
    "text": "nf-core/rnaseq results\nWe start with the raw counts contained in the salmon.merged.gene_counts.rds file generated by the nf-core/rnaseq workflow.\nWe TMM-normalize the data, as before. (This step converts the SummarizedExperiment into a `DGEList object as well.)\n\ncount_file &lt;- file.path(work_dir, \"salmon.merged.gene_counts.rds\")\nse &lt;- readRDS(count_file)\nstopifnot(all(colnames(se) %in% row.names(sample_anno)))\ndge_nfcore &lt;- calcNormFactors(se, method = \"TMM\")\n\nNext, we add the sample metadata and fit the same linear model as before.\n\n\nCode\ndge_nfcore$samples &lt;- data.frame(\n  dge_nfcore$samples,\n  sample_anno[colnames(dge_nfcore),\n              c(\"sample_title\", \"animal_id\", \"age\", \"genotype\", \"cell_type\")],\n  workflow = \"nfcore\"\n)\nstopifnot(all(colnames(dge) %in% colnames(dge_nfcore)))\ndge_nfcore &lt;- dge_nfcore[, colnames(dge)]\n\ndesign &lt;- model.matrix(~ genotype, data = dge_nfcore$samples)\ncolnames(design) &lt;- sub(\"genotype\", \"\", colnames(design))\nkeep &lt;- filterByExpr(dge_nfcore, design = design)\nfit_nfcore &lt;- voomLmFit(\n  dge_nfcore[keep, row.names(design)], \n  design = design,\n  sample.weights = TRUE, \n  plot = FALSE\n)\n\n\nFirst sample weights (min/max) 0.3856833/1.6439976\n\n\nFinal sample weights (min/max) 0.3853171/1.6423319\n\n\nCode\nfit_nfcore &lt;- eBayes(fit_nfcore, robust=TRUE)\n\n\nAs with the original count data from NCBI GEO, we detect small number of differentially expressed genes (FDR &lt; 5%).\n\nsummary(decideTests(fit_nfcore)[, \"KO\"])\n\n         KO\nDown     10\nNotSig 7824\nUp        2\n\n\n\n\nCode\ncpms &lt;- local({\n  geo &lt;- cpm(dge, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"Nugent et al\")\n  \n  nfcore &lt;- cpm(dge_nfcore, normalized.lib.sizes = TRUE) %&gt;%\n    as.data.frame() %&gt;%\n    cbind(dge_nfcore$genes) %&gt;%\n    pivot_longer(cols = starts_with(\"SRX\"), \n                 names_to = \"sample_name\",\n                 values_to = \"cpm\") %&gt;%\n    dplyr::left_join(\n      tibble::rownames_to_column(dge_nfcore$samples, \"sample_name\"),\n      by = \"sample_name\"\n    ) %&gt;%\n    dplyr::mutate(dataset = \"nf-core\")\n  \n  dplyr::bind_rows(\n    dplyr::select(geo, any_of(intersect(colnames(geo), colnames(nfcore)))),\n    dplyr::select(nfcore, any_of(intersect(colnames(geo), colnames(nfcore))))\n  )\n})\n\ntt &lt;- rbind(\n  topTreat(fit, coef = \"KO\", number = Inf)[\n    , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"geo\"),\n   topTreat(fit_nfcore, coef = \"KO\", number = Inf)[\n     , c(\"gene_id\", \"gene_name\", \"logFC\", \"P.Value\", \"adj.P.Val\")] %&gt;%\n    dplyr::mutate(dataset = \"nfcore\")\n) %&gt;%\n  dplyr::mutate(adj.P.Val = signif(adj.P.Val, 2)) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"gene_name\"), \n    names_from = \"dataset\", \n    values_from = \"adj.P.Val\") %&gt;%\n  dplyr::arrange(nfcore) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"gene_id\")\n\n\n\nNormalized expression\nFirst, we examine the correlation between the normalized log-transformed gene expression estimates returned from the two workflows. We focus on those genes that passed the filterByExpr thresholds above, e.g. those genes deemed sufficiently highly expressed to be assessed for differential expression.\n\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nsum_stats &lt;- cpms %&gt;%\n  dplyr::filter(gene_id %in% common_genes) %&gt;%\n  tidyr::pivot_wider(\n    id_cols = c(\"gene_id\", \"sample_name\"),\n    values_from = \"cpm\",\n    names_from = \"dataset\") %&gt;%\n  dplyr::group_by(gene_id) %&gt;%\n  dplyr::summarise(\n    r = cor(log1p(`Nugent et al`), log1p(`nf-core`)),\n    mean_nugent = mean(`Nugent et al`),\n    mean_nfcore = mean(`nf-core`))\n\np &lt;- ggplot(data = sum_stats, aes(x = r)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(limits = c(0, 1.02), breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"Pearson correlation coefficient (R)\", \n       y = \"Number of genes\",\n       title = \"Correlation between normalized log2 counts\") +\n  theme_linedraw(14)\nprint(p)\n\nWarning: Removed 35 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nThe correlation between normalized log2 expression estimates is reasonably high, e.g. 80% of all genes showing a Pearson correlation coefficient &gt; 0.82.\nThe relatively low correlation might reflect the low RNA input of this experiment, e.g. only 6314 of the genes genes were detected with &gt; 10 UMI-corrected normalized counts per million reads.\n\np &lt;- ggplot(data = sum_stats, aes(x = mean_nugent + 1)) + \n  geom_histogram(bins = 50) +\n  scale_x_continuous(trans = scales::log10_trans(),\n                     labels = scales::comma_format()) +\n  labs(x = \"Mean normalized counts per million\", \n       y = \"Number of genes\",\n       title = \"Average expression\",\n       subtitle = \"Nugent et al\") +\n  theme_linedraw(14)\nprint(p)\n\n\n\n\nNext, we will examine the results of the differential expression analysis.\n\n\nDifferential expression results\nAnalyses based on either preprocessing pipeline yield similar numbers of differentially expressed genes.\n\n\nCode\ncommon_genes &lt;- intersect(row.names(fit), row.names(fit_nfcore))\nresults &lt;- cbind(\n  decideTests(fit)[common_genes, \"KO\"], \n  decideTests(fit_nfcore)[common_genes, \"KO\"]\n)\ncolnames(results) &lt;- c(\"Nugent et al\", \"nf-core\")\nclass(results) &lt;- \"TestResults\"\nsummary(results)\n\n\n   Nugent et al nf-core\n-1           10       9\n0          6302    6303\n1             2       2\n\n\nBut are these the same genes in both sets of results?\nWe can visualize the overlap between the sets of significant genes in a Venn diagram (FDR &lt; 5%). The majority of differentially expressed genes is detected with both quantitation approaches (for both up- and down-regulated genes.)\n\nlimma::vennDiagram(results, include = c(\"up\", \"down\"),\n                   counts.col=c(\"red\", \"blue\"), mar = rep(0,4))\n\n\n\n\nFor example, the following plots show the normalized expression of the most significantly differentially expressed genes (known markers of active microglia).\n\n\nCode\nfor (gene in topTreat(fit, coef = \"KO\", number = 6)[[\"gene_id\"]]) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a hard FDR threshold can inflate the number of apparent differences, e.g. when a gene is close to the significance threshold (see below).\n\np_cor &lt;- cor(\n  fit$coefficients[common_genes, \"KO\"], \n  fit_nfcore$coefficients[common_genes, \"KO\"])\n\nThe log2 fold estimates for the Hom vs WT comparison are well correlated across the two analysis workflows (Pearson correlation coefficient R = 0.88 ).\n\nsmoothScatter(\n  fit$coefficients[common_genes, \"KO\"], \n  fit_nfcore$coefficients[common_genes, \"KO\"],\n  ylab = \"nf-core (log2FC)\",\n  xlab = \"Nugent et al (log2FC)\",\n  main = \"Homozygous APP vs WT (effect size)\"\n)\ntext(x = 1, y = -4, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\nas are the t-statistics across all examined genes:\n\np_cor &lt;- cor(\n  fit$t[common_genes, \"KO\"], \n  fit_nfcore$t[common_genes, \"KO\"])\nsmoothScatter(\n  fit$t[common_genes, \"KO\"], \n  fit_nfcore$t[common_genes, \"KO\"],\n  ylab = \"nf-core (t-statistic)\",\n  xlab = \"Nugent et al (t-statistic)\",\n  main = \"Homozygous APP vs WT (t-statistic)\")\ntext(x = 3, y = -15, labels = sprintf(\"R = %s\", signif(p_cor, 2)))\nabline(0, 1)\nabline(h = 0, v = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause this comparison yields only a small number of bona-fide differentially expressed genes, we don’t expect to see a high correlation between the log2 fold changes or the t-statistics between the two analyses: most of the values are very close to zero.\n\n\n\nDiscordant significance calls\n\n# genes detected in Nugent et al, but not significant with nf-core\ngenes &lt;- row.names(results)[which(abs(results[, 1]) == 1 & results[, 2] == 0)]\n\nAt FDR &lt; 5% 2 genes were reported as significantly differentially expressed with the original Nugent et al count matrix but not with the output of the nf-core/rnaseq workflow.\nAs side-by-side comparison of the FDR (adj.P.Val) for these genes confirms that the one of them (Cd52) displays significance close to the 5% threshold in the nf-core/rnaseq output as well. The second gene (Slamf8) also displays the same trend in both datasets, but is detected at lower levels (e.g. lower normalized CPMs) in the nf-core/rnaseq output.\n\nprint(tt[genes, ])\n\n                     gene_name    geo nfcore\nENSMUSG00000000682.7      Cd52 0.0031   0.09\nENSMUSG00000053318.7    Slamf8 0.0190   0.27\n\n\n\n\nExamples\nFinally, we plot the normalized gene expression estimates for the 2 discordant genes.\n\n\nCode\nfor (gene in genes) {\n p &lt;- cpms %&gt;%\n    dplyr::filter(gene_id == gene) %&gt;%\n    ggplot(aes(x = genotype, y = cpm)) + \n    geom_point(position = position_jitter(width = 0.05), alpha = 0.8) + \n    facet_grid(dataset ~ ., scales = \"free\") + \n    labs(title = dge$genes[gene, \"gene_name\"],\n         y = \"Normalized expression (CPM)\",\n         x = element_blank(),\n         subtitle = sprintf(\"FDR nf-core: %s\\nFDR GEO: %s\",\n                       tt[gene, \"nfcore\"],\n                       tt[gene, \"geo\"]\n                       )\n         ) +\n   theme_linedraw(14)\n print(p)\n}"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#conclusions",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#conclusions",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Conclusions",
    "text": "Conclusions\n\nDifferential expression analyses of raw counts obtained with the nc-core/rnaseq workflow yields results that are highly concordant with those obtained with the raw counts the authors deposited in NCBI GEO.\nWith appropriate parameters the nf-core/rnaseq workflow can be applied to QuantSeq FWD 3’tag RNA-seq data that includes unique molecular identifiers."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#reproducibility",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#reproducibility",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package              * version   date (UTC) lib source\n P abind                  1.4-5     2016-07-21 [?] CRAN (R 4.3.0)\n P AnnotationDbi        * 1.62.2    2023-07-02 [?] Bioconductor\n P askpass                1.1       2019-01-13 [?] CRAN (R 4.3.0)\n P Biobase              * 2.60.0    2023-05-08 [?] Bioconductor\n P BiocGenerics         * 0.46.0    2023-06-04 [?] Bioconductor\n P BiocManager            1.30.22   2023-08-08 [?] CRAN (R 4.3.0)\n P Biostrings             2.68.1    2023-05-21 [?] Bioconductor\n P bit                    4.0.5     2022-11-15 [?] CRAN (R 4.3.0)\n P bit64                  4.0.5     2020-08-30 [?] CRAN (R 4.3.0)\n P bitops                 1.0-7     2021-04-24 [?] CRAN (R 4.3.0)\n P blob                   1.2.4     2023-03-17 [?] CRAN (R 4.3.0)\n P cachem                 1.0.8     2023-05-01 [?] CRAN (R 4.3.0)\n P cli                    3.6.1     2023-03-23 [?] CRAN (R 4.3.0)\n P colorspace             2.1-0     2023-01-23 [?] CRAN (R 4.3.0)\n P crayon                 1.5.2     2022-09-29 [?] CRAN (R 4.3.0)\n R credentials            1.3.2     &lt;NA&gt;       [?] &lt;NA&gt;\n P DBI                    1.1.3     2022-06-18 [?] CRAN (R 4.3.0)\n P DelayedArray           0.26.7    2023-07-30 [?] Bioconductor\n P digest                 0.6.33    2023-07-07 [?] CRAN (R 4.3.0)\n P dplyr                * 1.1.2     2023-04-20 [?] CRAN (R 4.3.0)\n P edgeR                * 3.42.4    2023-06-04 [?] Bioconductor\n P evaluate               0.21      2023-05-05 [?] CRAN (R 4.3.0)\n P fansi                  1.0.4     2023-01-22 [?] CRAN (R 4.3.0)\n P farver                 2.1.1     2022-07-06 [?] CRAN (R 4.3.0)\n P fastmap                1.1.1     2023-02-24 [?] CRAN (R 4.3.0)\n P generics               0.1.3     2022-07-05 [?] CRAN (R 4.3.0)\n P GenomeInfoDb         * 1.36.1    2023-07-02 [?] Bioconductor\n P GenomeInfoDbData       1.2.10    2023-08-23 [?] Bioconductor\n P GenomicRanges        * 1.52.0    2023-05-08 [?] Bioconductor\n P ggplot2              * 3.4.3     2023-08-14 [?] CRAN (R 4.3.0)\n P glue                   1.6.2     2022-02-24 [?] CRAN (R 4.3.0)\n P gtable                 0.3.4     2023-08-21 [?] CRAN (R 4.3.1)\n P here                 * 1.0.1     2020-12-13 [?] CRAN (R 4.3.0)\n P htmltools              0.5.6     2023-08-10 [?] CRAN (R 4.3.0)\n P htmlwidgets            1.6.2     2023-03-17 [?] CRAN (R 4.3.0)\n P httr                   1.4.7     2023-08-15 [?] CRAN (R 4.3.0)\n P IRanges              * 2.34.1    2023-07-02 [?] Bioconductor\n P jsonlite               1.8.7     2023-06-29 [?] CRAN (R 4.3.0)\n P KEGGREST               1.40.0    2023-05-08 [?] Bioconductor\n P KernSmooth             2.23-21   2023-05-03 [?] CRAN (R 4.3.1)\n P knitr                  1.43      2023-05-25 [?] CRAN (R 4.3.0)\n P labeling               0.4.2     2020-10-20 [?] CRAN (R 4.3.0)\n P lattice                0.21-8    2023-04-05 [?] CRAN (R 4.3.1)\n P lifecycle              1.0.3     2022-10-07 [?] CRAN (R 4.3.0)\n P limma                * 3.56.2    2023-06-04 [?] Bioconductor\n P locfit                 1.5-9.8   2023-06-11 [?] CRAN (R 4.3.0)\n P magrittr               2.0.3     2022-03-30 [?] CRAN (R 4.3.0)\n P Matrix                 1.5-4.1   2023-05-18 [?] CRAN (R 4.3.1)\n P MatrixGenerics       * 1.12.3    2023-07-30 [?] Bioconductor\n P matrixStats          * 1.0.0     2023-06-02 [?] CRAN (R 4.3.0)\n P memoise                2.0.1     2021-11-26 [?] CRAN (R 4.3.0)\n P munsell                0.5.0     2018-06-12 [?] CRAN (R 4.3.0)\n P openssl                2.1.0     2023-07-15 [?] CRAN (R 4.3.0)\n P org.Mm.eg.db         * 3.17.0    2023-08-23 [?] Bioconductor\n P pillar                 1.9.0     2023-03-22 [?] CRAN (R 4.3.0)\n P pkgconfig              2.0.3     2019-09-22 [?] CRAN (R 4.3.0)\n P png                    0.1-8     2022-11-29 [?] CRAN (R 4.3.0)\n P purrr                  1.0.2     2023-08-10 [?] CRAN (R 4.3.0)\n P R6                     2.5.1     2021-08-19 [?] CRAN (R 4.3.0)\n P Rcpp                   1.0.11    2023-07-06 [?] CRAN (R 4.3.0)\n P RCurl                  1.98-1.12 2023-03-27 [?] CRAN (R 4.3.0)\n   renv                   1.0.2     2023-08-15 [1] CRAN (R 4.3.0)\n P rlang                  1.1.1     2023-04-28 [?] CRAN (R 4.3.0)\n P rmarkdown              2.24      2023-08-14 [?] CRAN (R 4.3.0)\n P rprojroot              2.0.3     2022-04-02 [?] CRAN (R 4.3.0)\n P RSQLite                2.3.1     2023-04-03 [?] CRAN (R 4.3.0)\n P rstudioapi             0.15.0    2023-07-07 [?] CRAN (R 4.3.0)\n P S4Arrays               1.0.5     2023-07-30 [?] Bioconductor\n P S4Vectors            * 0.38.1    2023-05-08 [?] Bioconductor\n P scales                 1.2.1     2022-08-20 [?] CRAN (R 4.3.0)\n P sessioninfo            1.2.2     2021-12-06 [?] CRAN (R 4.3.0)\n P statmod              * 1.5.0     2023-01-06 [?] CRAN (R 4.3.0)\n P SummarizedExperiment * 1.30.2    2023-06-11 [?] Bioconductor\n P sys                    3.4.2     2023-05-23 [?] CRAN (R 4.3.0)\n P tibble               * 3.2.1     2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr                * 1.3.0     2023-01-24 [?] CRAN (R 4.3.0)\n P tidyselect             1.2.0     2022-10-10 [?] CRAN (R 4.3.0)\n P utf8                   1.2.3     2023-01-31 [?] CRAN (R 4.3.0)\n P vctrs                  0.6.3     2023-06-14 [?] CRAN (R 4.3.0)\n P withr                  2.5.0     2022-03-03 [?] CRAN (R 4.3.0)\n P xfun                   0.40      2023-08-09 [?] CRAN (R 4.3.0)\n P XVector                0.40.0    2023-05-08 [?] Bioconductor\n P yaml                   2.3.7     2023-01-23 [?] CRAN (R 4.3.0)\n P zlibbioc               1.46.0    2023-05-08 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-4-nugent/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-4-nugent/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (4): Validating published results (with UMIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFull disclosure: I am a co-author of this publication.↩︎"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html",
    "href": "posts/postgres-full-text-search/index.html",
    "title": "Full text search in Postgres - the R way",
    "section": "",
    "text": "I have been learning how to organize, search and modify data in a Postgres database by working through Anthony DeBarros’ excellent book Practical SQL.\nBecause I currently perform most of my data analyses in R, I am using the great RPostgres, DBI and glue packages to interface with Postgres - without ever leaving my R session.\nToday I learned how to create a full text search index and how to search it with one or more search terms.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "href": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "title": "Full text search in Postgres - the R way",
    "section": "Connecting to Postgres",
    "text": "Connecting to Postgres\nFor this example, I created a toy database full_text_search in my local Postgres server. I connect to it with the DBI::dbConnect command, and by passing it the RPostgres::Postgres() driver.\n\nlibrary(DBI)\nlibrary(glue)\nlibrary(RPostgres)\nlibrary(sessioninfo)\n\n# Connect to a (prexisting) postgres database called `full_text_search`\ncon &lt;- DBI::dbConnect(\n  dbname = \"full_text_search\",\n  drv = RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432L,\n  user = \"postgres\"\n  )"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "href": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating and populating a table",
    "text": "Creating and populating a table\nBecause this is a toy example, I start with a fresh table datasets. (In case it already exists from previous experimentation, I drop the table if necessary).\nLet’s define four fields for the table:\n\nid: the unique identifier\nname: the short name of each entry\ntitle: a longer title\ndescription: a paragraph describing the entry\ncreated: a date and time the entry was added to the database\n\n\n# drop the `datasets` table if it already exists\nif (DBI::dbExistsTable(con, \"datasets\")) DBI::dbRemoveTable(con, \"datasets\")\n\n# create the empty `datasets` table\nsql &lt;- glue_sql(\"\n      CREATE TABLE IF NOT EXISTS datasets (\n      id bigserial PRIMARY KEY,\n      name text,\n      title text,\n      description text,\n      created timestamp with time zone default current_timestamp not null\n    );\", .con = con)\nres &lt;- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbReadTable(con, \"datasets\")\n\n[1] id          name        title       description created    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nInitially, our new database is empty. Let’s populate them with three entries, each describing a popular dataset shipped with R’s built-in datasets package.\n\n# some example entries\nbuildin_datasets &lt;- list(\n  mtcars = list(\n    \"name\" = \"mtcars\", \n    \"title\" = \"The built-in mtcars dataset from the datasets R package.\",\n    \"description\" = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The data was extracted from the 1974 Motor Trend US magazine, and \ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973–74 models).\")\n  ), \n  airmiles = list(\n    name = \"airmiles\",\n    title = \"The built-in airmiles dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The revenue passenger miles flown by commercial airlines in the United\nStates for each year from 1937 to 1960.\")\n  ),\n  attitude = list(\n    name = \"attitude\", \n    title = \"The built-in attitude dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"From a survey of the clerical employees of a large financial\norganization, the data are aggregated from the questionnaires of the\napproximately 35 employees for each of 30 (randomly selected) departments. \nThe numbers give the percent proportion of favourable responses to seven\nquestions in each department.\")\n  )\n)\n\nNext, we loop over each element of the list and use the glue_sql() command to unpack both the names (names(dataset)) and the values of each field for this entry. Then we update the datasets table with this new information.\nAfterward, we retrieve the name and title fields to verify the correct import:\n\nfor (dataset in buildin_datasets) {\n  sql &lt;- glue_sql(\n    \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({dataset*});\", \n    .con = con)\n  res &lt;- suppressMessages(DBI::dbSendStatement(con, sql))\n  DBI::dbClearResult(res)\n}\nDBI::dbGetQuery(con, \"SELECT name, title from datasets;\")\n\n      name                                                     title\n1   mtcars  The built-in mtcars dataset from the datasets R package.\n2 airmiles The built-in airmiles dataset from the datasets R package\n3 attitude The built-in attitude dataset from the datasets R package"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#searching",
    "href": "posts/postgres-full-text-search/index.html#searching",
    "title": "Full text search in Postgres - the R way",
    "section": "Searching!",
    "text": "Searching!\nOur goal is to enable full-text search for the description field. Let’s look up the term data. To perform full-text search, both the records to search and our query need to be tokinzed first, with the to_tsvector and to_tsquery functions, respectively.\nHere is an example of the tokens that are generated:\n\nsql &lt;- glue_sql(\n  \"SELECT to_tsvector('This is a my test phrase, and what \n                       a beautiful phrase it is.')\n   to_tsquery\", con = con)\nDBI::dbGetQuery(con, sql)\n\n                          to_tsquery\n1 'beauti':10 'phrase':6,11 'test':5\n\n\nThe following query correctly returns all records whose descriptions contain the word data:\n\n# search the description field\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name\n1  1   mtcars\n2  3 attitude\n\n\nWe can enrich the output by returning the output of the ts_headline function, highlighting the location / context of the the matched term:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n\n\nWe can also combine search terms, e.g. searching for either employee or motor terms:\n\n# using multiple search terms\nterm &lt;- \"employee | motor\"  # OR\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars                from the 1974 &lt;Motor&gt; Trend US magazine\n2  3 attitude clerical &lt;employees&gt; of a large financial organization\n\n\nSimilarly, we can narrow our search by requiring both data and employee terms to appear in the same description:\n\nterm &lt;- \"data & employee\"  # AND\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  3 attitude clerical &lt;employees&gt; of a large financial organization"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-indices",
    "href": "posts/postgres-full-text-search/index.html#creating-indices",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating indices",
    "text": "Creating indices\nIn the examples above, we performed tokenization of the search term and the description field at run time, e.g. when the query was executed. As our database grows, this will soon become too cumbersome and degrade performance.\nAdding an index to our database will maintain full-text search speed even with large datasets. We have two different options:\n\nCreate an index based on an expression.\nCreate a new field to hold the output of the to_tsvector function, and then index this new field.\n\n\nCreating an expression index\nA simple way to create a full-text index is to include the to_tsvector() expression in the definition of the index itself. Here, we add a Generalized Inverted Index (GIN) index for the description column:\n\nsql = glue_sql(\n  \"CREATE INDEX description_idx ON datasets \n  USING gin(to_tsvector('english', description));\",\n  con = con\n)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nThe same type of query we issued above will now take advantage of the description_idx:\n\n# search the description field using its index\nterm &lt;- \"questioning\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                       ts_headline\n1  3 attitude responses to seven &lt;questions&gt; in each department\n\n\nThe description fields of new records, e.g those that are added later, will automatically be added to the index. Let’s create a new record for the euro dataset, for example.\n\nnew_data = list(\n  name = \"euro\", \n  title = \"The built-in euro dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"The data set euro contains the value of 1 Euro in all currencies\nparticipating in the European monetary union (Austrian Schilling ATS, \nBelgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, \nFrench Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, \nDutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were \nfixed by the European Union on December 31, 1998. To convert old prices to \nEuro prices, divide by the respective rate and round to 2 digits.\")\n)\nsql &lt;- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({new_data*});\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nThis new record will now be included in the search results for the term data, for example:\n\n# search the description field using its index\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n\n\n\n\nAdding a tokenized field for full-text searches\nAlternatively, another option is to create a new column to hold the output of the to_tsvector() function, and then to index it for future use. Let’s create a new column search_description_text:\n\n# create a column to hold tokens for full text search\nsql &lt;- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\nDBI::dbListFields(con, \"datasets\")\n\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n\n\nNext, we tokenize the descriptions field, and store the output in our new search_description_text column:\n\nsql &lt;- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 4\n\n\nHere are the tokens generated from the description of the first record, for example:\n\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n\n\nAs before, we can add an index - but this time, we index the pre-tokenized search_description_text column instead:\n\n# create the search index\nsql &lt;- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nTime to run our search again. When we search the search_description_text field, we can omit the to_tsvector() call, because its has been tokenized already:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n\n\n🚨 But beware: because we have precalculated the tokens, any new records added to the database will not automatically be processed, nor will they be indexed!\nLet’s add a final record, the morely dataset:\n\nmore_data = list(\n  name = \"morley\", \n  title = \"The built-in morley dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"A classical data of Michelson (but not this one with Morley) on \nmeasurements done in 1879 on the speed of light. The data consists of five \nexperiments, each consisting of 20 consecutive ‘runs’. The response is the speed\nof light measurement, suitably coded (km/sec, with 299000 subtracted).\")\n)\n\nTo enter this record, we not only have to populate the name, title and description fields - but also the list of tokens derived from the description in the search_description_text column. In other words, we have to execute the to_tsvector function inside our INSERT statement:\n\nsql &lt;- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*}, search_description_text)\n   VALUES ({more_data*}, to_tsvector({more_data[['description']]}));\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nNow, our query returns both the original matches and the new record:\n\n# search the description field and show the matching location\nterm &lt;- \"data\"\nsql &lt;- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = &lt;,\n      StopSel = &gt;,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               &lt;data&gt; was extracted from the 1974 Motor\n2  3 attitude financial organization, the &lt;data&gt; are aggregated from\n3  4     euro                     &lt;data&gt; set euro contains the value\n4  5   morley            classical &lt;data&gt; of Michelson (but not this\n\n\n\n\nChoosing between indexing strategies\nAccording to the Postgres documentation:\n\nOne advantage of the separate-column approach over an expression index is that it is not necessary to explicitly specify the text search configuration in queries in order to make use of the index. Another advantage is that searches will be faster, since it will not be necessary to redo the to_tsvector calls to verify index matches. The expression-index approach is simpler to set up, however, and it requires less disk space since the tsvector representation is not stored explicitly.\n\nThat’s it. Thanks again to Anthony DeBarros’ for his excellent introduction to Practical SQL!"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#reproducibility",
    "href": "posts/postgres-full-text-search/index.html#reproducibility",
    "title": "Full text search in Postgres - the R way",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-01-16\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n askpass       1.1     2019-01-13 [1] CRAN (R 4.2.0)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.2.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.2.0)\n blob          1.2.3   2022-04-10 [1] CRAN (R 4.2.0)\n cli           3.5.0   2022-12-20 [1] CRAN (R 4.2.0)\n credentials   1.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.0)\n evaluate      0.19    2022-12-13 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n hms           1.1.2   2022-08-19 [1] CRAN (R 4.2.0)\n htmltools     0.5.4   2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.2)\n jsonlite      1.8.4   2022-12-06 [1] CRAN (R 4.2.0)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.2.0)\n lubridate     1.9.0   2022-11-06 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n openssl       2.0.5   2022-12-06 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n Rcpp          1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.19    2022-12-15 [1] CRAN (R 4.2.0)\n RPostgres   * 1.4.4   2022-05-02 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n sys           3.4.1   2022-10-18 [1] CRAN (R 4.2.0)\n timechange    0.1.1   2022-11-04 [1] CRAN (R 4.2.0)\n vctrs         0.5.1   2022-11-16 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/upset_plots/index.html",
    "href": "posts/upset_plots/index.html",
    "title": "UpSet plots: comparing differential expression across contrasts",
    "section": "",
    "text": "Today I learned how to use UpSet plots to visualize the overlap between sets of differentially expressed genes.\nI often analyze RNA-seq experiments with multiple factors, e.g. different treatments, conditions, cell lines, genotypes, time points, etc. The scientific questions typically involve not just one, but multiple comparisons between experimental groups. For example:\nTo answer these questions, I typically fit a single linear model and then extract the comparisons of interest by specifying each of them as as contrast. (Check out the vignette of the excellent designmatrices Bioconductor package for details on creating design matrices and extracting contrasts.)\nAfter applying a suitable p-value / FDR threshold, each comparison / contrast yields a list of differentially expressed genes1. When the lists are long, it is difficult to assess the degree of overlap, e.g. the number of genes that were detected in multiple contrasts.\nIf the number of comparisons is small (say &lt; 5), then a Venn diagram is an excellent way of displaying how these sets of genes overlap. But when the number of sets increases, so does the number of intersections - and Venn diagrams soon become hard to draw (and interpret).\nUpset plots can be used to clearly visualize larger numbers of sets. Here, I am using the airway Bioconductor dataset, an RNA-Seq experiment on four human airway smooth muscle cell lines treated with dexamethasone, to illustrate how to\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/upset_plots/index.html#footnotes",
    "href": "posts/upset_plots/index.html#footnotes",
    "title": "UpSet plots: comparing differential expression across contrasts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBoth Venn diagrams and upset plots operate on sets, e.g. they require that a hard threshold has been applied to the results of a differential expression analysis. That’s problematic, because p-values themselves display high variability and dichotomizing quantitative information looses information.↩︎"
  },
  {
    "objectID": "posts/30days-of-streamlit/index.html",
    "href": "posts/30days-of-streamlit/index.html",
    "title": "Guess the correlation - a first streamlit app",
    "section": "",
    "text": "TL;DR\n\nI learned the basics of creating web applications with Streamlit\nBuild your intuition about correlation coefficients in my first app here!\n\nThis week, I learned about Streamlit, a python module to rapidly develop dashboards and (simple) web applications. Having used Posit’s shiny framework in the past (using R), I enjoyed diving into a solution that uses python.\nThere are numerous comparisons between different frameworks to develop dashboards with python (e.g.  this one ). Most recently, shiny for python has entered the stage as well.\nTo get started, I completed 30 days of Streamlit, short exercises that introduce key Streamlit elements.\nNext, I tried my hands at coding a simple app from scratch. To challenge myself, I implemented a simplified version of Omar Wagih’s awesome Guess The Corrlelation game. A user is presented with a scatter plot and prompted to guess the (Pearson) correlation coefficient between the x- and y-variables.\n\n\n\nMy first streamlit app\n\n\nTrue to its promise of “turning data scripts into shareable web apps in minutes” I was able to get a simple application up and running very quickly, with only a few lines of code.\nStreamlit makes it easy to add form elements, graphs or markdown-formatted text to a web application. While shiny defines which elements need to be refreshed based on user input explicitly (see with reactive epressions ), streamlit simply reruns the entire script whenever a user interacts with the application. That took some getting used to, e.g. as variables are reset in the process.\nTo store selections and variables across reruns, the Session State a field-based API, is available, and I used it extensively:\n# persistent variables\nwith st.sidebar:\n    st.subheader(\"Settings\")\n    st.session_state[\"n\"] = st.number_input(\"Number of data points\", 2, 1000, 100)\nif not \"data\" in st.session_state:\n    st.session_state[\"data\"] = dataset(st.session_state[\"n\"])\nif not \"cor\" in st.session_state:\n    st.session_state[\"cor\"] = correlation(st.session_state[\"data\"])\nif not \"guessed\" in st.session_state:\n    st.session_state[\"guessed\"] = False\nif not \"streak\" in st.session_state:\n    st.session_state[\"streak\"] = False\nif not \"streak_length\" in st.session_state:\n    st.session_state[\"streak_length\"] = 0\nif not \"coins\" in st.session_state:\n    st.session_state[\"coins\"] = 3\nI also wanted to display two alternative buttons, either offering the user the option to submit a guess (Submit!) or to refresh the chart and start over (Try again!).\nControlling the conditional flow of the app was a bit of a challenge (for a beginner like myself), but eventually I was able to accomplish it through liberal use of the experimental st.experimental_rerun() command.\nI deployed the final application in the streamlit cloud at https://correlation.streamlit.app/. (Any feedback is very welcome!)\nOverall, I was impressed how quickly I could put together a dashboard, and I am looking forward to sharing analysis results and interactive plots with my collaborators in the future. For more complex applications, I will look into shiny (R/phython), Flask or Django instead.\n\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/r-update-with-rig/index.html",
    "href": "posts/r-update-with-rig/index.html",
    "title": "Updating R the easy way: using rig command line tool",
    "section": "",
    "text": "Today it was time to update the R installation on my Mac OS X system, from R 4.2.1 to 4.2.2. Luckily, with Gábor Csárdi’s rig command line tool that was a breeze.\nI had previously installed rig with brew\nbrew tap r-lib/rig\nbrew install --cask rig\nso I first checked if there were any updates available for rig itself:\nbrew upgrade --cask rig\nThis command updated rig from version 0.5.0 to 0.5.2.\nThen I listed the R versions currently installed on my system:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.1)\nAt this point, I was using R release 4.2.1. Next, I updated to the latest release\nrig install\n\n[INFO] Downloading https://cloud.r-project.org/bin/macosx/base/R-4.2.2.pkg -&gt; /tmp/rig/x86_64-R-4.2.2.pkg\n[INFO] Running installer\n[INFO] &gt; installer: Package name is R 4.2.2 for macOS\n[INFO] &gt; installer: Installing at base path /\n[INFO] &gt; installer: The install was successful.\n[INFO] Forgetting installed versions\n[INFO] Fixing permissions\n[INFO] Adding R-* quick links (if needed)\n[INFO] Setting default CRAN mirror\n[INFO] Installing pak for R 4.2 (if not installed yet)\nOnce the rig install command had completed, my system had updated itself to R version 4.2.2:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.2)\nNow a new R session starts with R 4.2.2\n&gt;R\n\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nThank you, Gábor!\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/duckdb/index.html",
    "href": "posts/duckdb/index.html",
    "title": "Querying parquet files with duckdb",
    "section": "",
    "text": "Today I learned how to access and query CSV and parquet files with duckdb, using either the duckdb command line interface or the eponymous R package\n\n\nduckdb is a relational (table-oriented) database management system (RDMS) contained in a single executable. It excels at processing tabular datasets, e.g. from CSV or Parquet files, from local file systems or remote sources.\nApache Parquet is &gt; an open source, column-oriented data file format designed for efficient data storage and retrieval.\nHere, I am highlighting how to use duckdb to query remote parquet files without the need for retrieving the full dataset first. And that’s just one of the many functionalities offered by duckdb, truly a swiss army knife in the data science toolkit!\n\n  D-M Commons, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\n\nI installed the duckdb executable on my Mac OS system with homebrew:\nbrew install duckdb\nduckdb --version\n\n\n\nBy default, duckdb will create database in memory. Like other RMDS, it supports a core set of SQL statements and expressions. In addition, extensions provide additional functionality, e.g. connecting to Postgres databases or supporting JSON data.\nCommands can either be entered interactively, provided via the -c argument or in a text file. To access remote files, we first need to install the httpsfs` extension that allows reading remote/writing remote files 1.\nduckdb -c \"INSTALL httpfs\"\nTo get started, we read a small dataset from a CSV file hosted publicly on a webserver. For brevity, we store this URL in the environmental variable REMOTE_FILE:\nREMOTE_FILE=https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n\nduckdb -c \"SELECT species, island, sex, bill_length_mm, bill_depth_mm \\\n           FROM '$REMOTE_FILE' LIMIT 5;\" \n\n┌─────────┬───────────┬─────────┬────────────────┬───────────────┐\n│ species │  island   │   sex   │ bill_length_mm │ bill_depth_mm │\n│ varchar │  varchar  │ varchar │     double     │    double     │\n├─────────┼───────────┼─────────┼────────────────┼───────────────┤\n│ Adelie  │ Torgersen │ MALE    │           39.1 │          18.7 │\n│ Adelie  │ Torgersen │ FEMALE  │           39.5 │          17.4 │\n│ Adelie  │ Torgersen │ FEMALE  │           40.3 │          18.0 │\n│ Adelie  │ Torgersen │         │                │               │\n│ Adelie  │ Torgersen │ FEMALE  │           36.7 │          19.3 │\n└─────────┴───────────┴─────────┴────────────────┴───────────────┘\n\n\nBy default, duckdb will use a temporary, in-memory database. To open or create a persistent database, simply include a path as a command line argument, e.g. duckdb path/to/my_database.duckdb\nFor example, the following command will download the remote CSV file and import it into a duckdb database and store it in the penguins.duckdb file.\n\nduckdb \\\n  -c \"CREATE TABLE penguins AS SELECT * FROM '${REMOTE_FILE}';\" \\\n  penguins.duckdb \n\nNow, we can query the local file with duckdb or explore it interactive with the tad viewer 2\n\nduckdb \\\n  -c \"SELECT * from penguins WHERE sex = 'MALE' LIMIT 5;\" \\\n  penguins.duckdb\n\n┌─────────┬───────────┬────────────────┬───────────────┬───────────────────┬─────────────┬─────────┐\n│ species │  island   │ bill_length_mm │ bill_depth_mm │ flipper_length_mm │ body_mass_g │   sex   │\n│ varchar │  varchar  │     double     │    double     │       int64       │    int64    │ varchar │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼─────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ MALE    │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ MALE    │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ MALE    │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ MALE    │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ MALE    │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴─────────┘\n\n\n\n\n\nThe NYC Taxi & Limousine Commission has collected data on public NYC taxi and for-hire vehicle (Uber, Lyft) trips, going all the way back to 2009. The data is shared in the form of parquet files, and one parquet file is created for each month of data.\nHere, I will use the Yellow Taxi Trip records from January and February 2023 as examples. Let’s store the URLs pointing to the respective parquet files in environmental variables.\nPARQUET_FILE1=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\nPARQUET_FILE2=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\nEach parquet file stores a single table of data. To get an overview of the available information, we ask duckdb to DESCRIBE it:\n\nduckdb -c \"DESCRIBE SELECT * FROM '$PARQUET_FILE1'\";\n\n┌───────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│      column_name      │ column_type │  null   │   key   │ default │  extra  │\n│        varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├───────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ VendorID              │ BIGINT      │ YES     │         │         │         │\n│ tpep_pickup_datetime  │ TIMESTAMP   │ YES     │         │         │         │\n│ tpep_dropoff_datetime │ TIMESTAMP   │ YES     │         │         │         │\n│ passenger_count       │ DOUBLE      │ YES     │         │         │         │\n│ trip_distance         │ DOUBLE      │ YES     │         │         │         │\n│ RatecodeID            │ DOUBLE      │ YES     │         │         │         │\n│ store_and_fwd_flag    │ VARCHAR     │ YES     │         │         │         │\n│ PULocationID          │ BIGINT      │ YES     │         │         │         │\n│ DOLocationID          │ BIGINT      │ YES     │         │         │         │\n│ payment_type          │ BIGINT      │ YES     │         │         │         │\n│ fare_amount           │ DOUBLE      │ YES     │         │         │         │\n│ extra                 │ DOUBLE      │ YES     │         │         │         │\n│ mta_tax               │ DOUBLE      │ YES     │         │         │         │\n│ tip_amount            │ DOUBLE      │ YES     │         │         │         │\n│ tolls_amount          │ DOUBLE      │ YES     │         │         │         │\n│ improvement_surcharge │ DOUBLE      │ YES     │         │         │         │\n│ total_amount          │ DOUBLE      │ YES     │         │         │         │\n│ congestion_surcharge  │ DOUBLE      │ YES     │         │         │         │\n│ airport_fee           │ DOUBLE      │ YES     │         │         │         │\n├───────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 19 rows                                                           6 columns │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n\nA detailed description of the columns and their values is available in the metadata dictionary. For example, the payment_type field contains “A numeric code signifying how the passenger paid for the trip.” with the following encoding:\n\n1: Credit card\n2: Cash\n3: No charge\n4: Dispute\n5: Unknown\n6: Voided trip\n\nIn January, more than three million trips were recorded, but a query to return the total number of records executes almost instantaneously - because we don’t need to download the (very large) file first:\n\nduckdb -c \"SELECT count(*) FROM '$PARQUET_FILE1'\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      3066766 │\n└──────────────┘\n\n\nThe vast majority of trips was paid for by credit card (payment type 1), and a small subset of trips was performed free of charge (payment type 3).\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM '$PARQUET_FILE1' \\\n           GROUP BY payment_type LIMIT 5\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            0 │               71743 │\n│            1 │             2411462 │\n│            2 │              532241 │\n│            3 │               18023 │\n│            4 │               33297 │\n└──────────────┴─────────────────────┘\n\n\nWe can also query across multiple parquet files, e.g. retrieving the total number of trips for both January and February 2023:\n\nduckdb -c \"SELECT count(*) FROM \\\n           read_parquet(['$PARQUET_FILE1', '$PARQUET_FILE2'])\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      5980721 │\n└──────────────┘\n\n\nWe can also copy the output of a query into a new, local parquet file. For example, the following query will copy records for 100 trips that were performed free of charge into a new free_trips.parquet parquet file in the current working directory:\n\nduckdb -c \\\n  \"COPY (SELECT * FROM '$PARQUET_FILE1' \\\n         WHERE payment_type = 3 LIMIT 100) TO 'free_trips.parquet' \\\n  (FORMAT 'parquet');\"\n\nWe can now query the local parquet file to drill deeper into this data slice:\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM 'free_trips.parquet' \\\n           GROUP BY payment_type\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            3 │                 100 │\n└──────────────┴─────────────────────┘\n\n\n\n\n\nIn addition to using the duckdb command line interface (CLI), you can also use a library for your favorite programming language. For example, the duckdb R package provides a DBI interface that enables queries from within an R session. (The duckdb python module provides similar functionality.)\n\nif (!requireNamespace(\"duckdb\", quietly = TRUE)) {\n  install.packages(\"duckdb\")\n}\nsuppressPackageStartupMessages(library(\"duckdb\"))\nsuppressPackageStartupMessages(library(\"DBI\"))\n\n\ncon &lt;- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\ndbExecute(conn = con, \"INSTALL httpfs\")\n\n[1] 0\n\n\nFor example, we can use an in-memory duckdb instance to query the one (or more) of the remote parquet files we examined above:\n\nPARQUET_FILE1 = paste0(\"https://d37ci6vzurychx.cloudfront.net/\",\n                       \"trip-data/yellow_tripdata_2023-01.parquet\")\n\n\nsql &lt;- \"SELECT payment_type, count(payment_type) \\\n        FROM read_parquet([?]) \\\n        GROUP BY payment_type LIMIT 5\";\ndbGetQuery(con, sql, list(PARQUET_FILE1))\n\n  payment_type count(payment_type)\n1            0               71743\n2            1             2411462\n3            2              532241\n4            3               18023\n5            4               33297\n\n\nAlternatively, we can also access data (including CSV and parquet files) using dbplyr and dplyr\n\nsuppressPackageStartupMessages(library(dbplyr))\nsuppressPackageStartupMessages(library(dplyr))\n\ntbl(con, PARQUET_FILE1) |&gt;\n  group_by(payment_type) |&gt;\n  count() |&gt;\n  collect()\n\n# A tibble: 5 × 2\n# Groups:   payment_type [5]\n  payment_type       n\n         &lt;dbl&gt;   &lt;dbl&gt;\n1            0   71743\n2            1 2411462\n3            2  532241\n4            3   18023\n5            4   33297\n\n\nDon’t forget to disconnect from your duckdb database at the end of your R session!\n\ndbDisconnect(con, shutdown=TRUE)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/duckdb/index.html#tldr",
    "href": "posts/duckdb/index.html#tldr",
    "title": "Querying parquet files with duckdb",
    "section": "",
    "text": "Today I learned how to access and query CSV and parquet files with duckdb, using either the duckdb command line interface or the eponymous R package\n\n\nduckdb is a relational (table-oriented) database management system (RDMS) contained in a single executable. It excels at processing tabular datasets, e.g. from CSV or Parquet files, from local file systems or remote sources.\nApache Parquet is &gt; an open source, column-oriented data file format designed for efficient data storage and retrieval.\nHere, I am highlighting how to use duckdb to query remote parquet files without the need for retrieving the full dataset first. And that’s just one of the many functionalities offered by duckdb, truly a swiss army knife in the data science toolkit!\n\n  D-M Commons, CC BY-SA 3.0, via Wikimedia Commons\n\n\n\n\nI installed the duckdb executable on my Mac OS system with homebrew:\nbrew install duckdb\nduckdb --version\n\n\n\nBy default, duckdb will create database in memory. Like other RMDS, it supports a core set of SQL statements and expressions. In addition, extensions provide additional functionality, e.g. connecting to Postgres databases or supporting JSON data.\nCommands can either be entered interactively, provided via the -c argument or in a text file. To access remote files, we first need to install the httpsfs` extension that allows reading remote/writing remote files 1.\nduckdb -c \"INSTALL httpfs\"\nTo get started, we read a small dataset from a CSV file hosted publicly on a webserver. For brevity, we store this URL in the environmental variable REMOTE_FILE:\nREMOTE_FILE=https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\n\nduckdb -c \"SELECT species, island, sex, bill_length_mm, bill_depth_mm \\\n           FROM '$REMOTE_FILE' LIMIT 5;\" \n\n┌─────────┬───────────┬─────────┬────────────────┬───────────────┐\n│ species │  island   │   sex   │ bill_length_mm │ bill_depth_mm │\n│ varchar │  varchar  │ varchar │     double     │    double     │\n├─────────┼───────────┼─────────┼────────────────┼───────────────┤\n│ Adelie  │ Torgersen │ MALE    │           39.1 │          18.7 │\n│ Adelie  │ Torgersen │ FEMALE  │           39.5 │          17.4 │\n│ Adelie  │ Torgersen │ FEMALE  │           40.3 │          18.0 │\n│ Adelie  │ Torgersen │         │                │               │\n│ Adelie  │ Torgersen │ FEMALE  │           36.7 │          19.3 │\n└─────────┴───────────┴─────────┴────────────────┴───────────────┘\n\n\nBy default, duckdb will use a temporary, in-memory database. To open or create a persistent database, simply include a path as a command line argument, e.g. duckdb path/to/my_database.duckdb\nFor example, the following command will download the remote CSV file and import it into a duckdb database and store it in the penguins.duckdb file.\n\nduckdb \\\n  -c \"CREATE TABLE penguins AS SELECT * FROM '${REMOTE_FILE}';\" \\\n  penguins.duckdb \n\nNow, we can query the local file with duckdb or explore it interactive with the tad viewer 2\n\nduckdb \\\n  -c \"SELECT * from penguins WHERE sex = 'MALE' LIMIT 5;\" \\\n  penguins.duckdb\n\n┌─────────┬───────────┬────────────────┬───────────────┬───────────────────┬─────────────┬─────────┐\n│ species │  island   │ bill_length_mm │ bill_depth_mm │ flipper_length_mm │ body_mass_g │   sex   │\n│ varchar │  varchar  │     double     │    double     │       int64       │    int64    │ varchar │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼─────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ MALE    │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ MALE    │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ MALE    │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ MALE    │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ MALE    │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴─────────┘\n\n\n\n\n\nThe NYC Taxi & Limousine Commission has collected data on public NYC taxi and for-hire vehicle (Uber, Lyft) trips, going all the way back to 2009. The data is shared in the form of parquet files, and one parquet file is created for each month of data.\nHere, I will use the Yellow Taxi Trip records from January and February 2023 as examples. Let’s store the URLs pointing to the respective parquet files in environmental variables.\nPARQUET_FILE1=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\nPARQUET_FILE2=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\nEach parquet file stores a single table of data. To get an overview of the available information, we ask duckdb to DESCRIBE it:\n\nduckdb -c \"DESCRIBE SELECT * FROM '$PARQUET_FILE1'\";\n\n┌───────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│      column_name      │ column_type │  null   │   key   │ default │  extra  │\n│        varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├───────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ VendorID              │ BIGINT      │ YES     │         │         │         │\n│ tpep_pickup_datetime  │ TIMESTAMP   │ YES     │         │         │         │\n│ tpep_dropoff_datetime │ TIMESTAMP   │ YES     │         │         │         │\n│ passenger_count       │ DOUBLE      │ YES     │         │         │         │\n│ trip_distance         │ DOUBLE      │ YES     │         │         │         │\n│ RatecodeID            │ DOUBLE      │ YES     │         │         │         │\n│ store_and_fwd_flag    │ VARCHAR     │ YES     │         │         │         │\n│ PULocationID          │ BIGINT      │ YES     │         │         │         │\n│ DOLocationID          │ BIGINT      │ YES     │         │         │         │\n│ payment_type          │ BIGINT      │ YES     │         │         │         │\n│ fare_amount           │ DOUBLE      │ YES     │         │         │         │\n│ extra                 │ DOUBLE      │ YES     │         │         │         │\n│ mta_tax               │ DOUBLE      │ YES     │         │         │         │\n│ tip_amount            │ DOUBLE      │ YES     │         │         │         │\n│ tolls_amount          │ DOUBLE      │ YES     │         │         │         │\n│ improvement_surcharge │ DOUBLE      │ YES     │         │         │         │\n│ total_amount          │ DOUBLE      │ YES     │         │         │         │\n│ congestion_surcharge  │ DOUBLE      │ YES     │         │         │         │\n│ airport_fee           │ DOUBLE      │ YES     │         │         │         │\n├───────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 19 rows                                                           6 columns │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n\nA detailed description of the columns and their values is available in the metadata dictionary. For example, the payment_type field contains “A numeric code signifying how the passenger paid for the trip.” with the following encoding:\n\n1: Credit card\n2: Cash\n3: No charge\n4: Dispute\n5: Unknown\n6: Voided trip\n\nIn January, more than three million trips were recorded, but a query to return the total number of records executes almost instantaneously - because we don’t need to download the (very large) file first:\n\nduckdb -c \"SELECT count(*) FROM '$PARQUET_FILE1'\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      3066766 │\n└──────────────┘\n\n\nThe vast majority of trips was paid for by credit card (payment type 1), and a small subset of trips was performed free of charge (payment type 3).\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM '$PARQUET_FILE1' \\\n           GROUP BY payment_type LIMIT 5\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            0 │               71743 │\n│            1 │             2411462 │\n│            2 │              532241 │\n│            3 │               18023 │\n│            4 │               33297 │\n└──────────────┴─────────────────────┘\n\n\nWe can also query across multiple parquet files, e.g. retrieving the total number of trips for both January and February 2023:\n\nduckdb -c \"SELECT count(*) FROM \\\n           read_parquet(['$PARQUET_FILE1', '$PARQUET_FILE2'])\";\n\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│      5980721 │\n└──────────────┘\n\n\nWe can also copy the output of a query into a new, local parquet file. For example, the following query will copy records for 100 trips that were performed free of charge into a new free_trips.parquet parquet file in the current working directory:\n\nduckdb -c \\\n  \"COPY (SELECT * FROM '$PARQUET_FILE1' \\\n         WHERE payment_type = 3 LIMIT 100) TO 'free_trips.parquet' \\\n  (FORMAT 'parquet');\"\n\nWe can now query the local parquet file to drill deeper into this data slice:\n\nduckdb -c \"SELECT payment_type, count(payment_type) \\\n           FROM 'free_trips.parquet' \\\n           GROUP BY payment_type\";\n\n┌──────────────┬─────────────────────┐\n│ payment_type │ count(payment_type) │\n│    int64     │        int64        │\n├──────────────┼─────────────────────┤\n│            3 │                 100 │\n└──────────────┴─────────────────────┘\n\n\n\n\n\nIn addition to using the duckdb command line interface (CLI), you can also use a library for your favorite programming language. For example, the duckdb R package provides a DBI interface that enables queries from within an R session. (The duckdb python module provides similar functionality.)\n\nif (!requireNamespace(\"duckdb\", quietly = TRUE)) {\n  install.packages(\"duckdb\")\n}\nsuppressPackageStartupMessages(library(\"duckdb\"))\nsuppressPackageStartupMessages(library(\"DBI\"))\n\n\ncon &lt;- dbConnect(duckdb::duckdb(), dbdir = \":memory:\")\ndbExecute(conn = con, \"INSTALL httpfs\")\n\n[1] 0\n\n\nFor example, we can use an in-memory duckdb instance to query the one (or more) of the remote parquet files we examined above:\n\nPARQUET_FILE1 = paste0(\"https://d37ci6vzurychx.cloudfront.net/\",\n                       \"trip-data/yellow_tripdata_2023-01.parquet\")\n\n\nsql &lt;- \"SELECT payment_type, count(payment_type) \\\n        FROM read_parquet([?]) \\\n        GROUP BY payment_type LIMIT 5\";\ndbGetQuery(con, sql, list(PARQUET_FILE1))\n\n  payment_type count(payment_type)\n1            0               71743\n2            1             2411462\n3            2              532241\n4            3               18023\n5            4               33297\n\n\nAlternatively, we can also access data (including CSV and parquet files) using dbplyr and dplyr\n\nsuppressPackageStartupMessages(library(dbplyr))\nsuppressPackageStartupMessages(library(dplyr))\n\ntbl(con, PARQUET_FILE1) |&gt;\n  group_by(payment_type) |&gt;\n  count() |&gt;\n  collect()\n\n# A tibble: 5 × 2\n# Groups:   payment_type [5]\n  payment_type       n\n         &lt;dbl&gt;   &lt;dbl&gt;\n1            0   71743\n2            1 2411462\n3            2  532241\n4            3   18023\n5            4   33297\n\n\nDon’t forget to disconnect from your duckdb database at the end of your R session!\n\ndbDisconnect(con, shutdown=TRUE)"
  },
  {
    "objectID": "posts/duckdb/index.html#footnotes",
    "href": "posts/duckdb/index.html#footnotes",
    "title": "Querying parquet files with duckdb",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdditional options to parse / import CSV files is available in duckdb’s documentation↩︎\nThe tad viewer is a free tool to view CSV, Parquet, and SQLite and DuckDb database files↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Thomas Sandmann’s blog. Originally from Germany, my professional journey includes a degree in Biochemistry, a PhD in Developmental Biology from EMBL, postdoctoral research at Temasek Lifescience Laboratories and at the German Cancer research Center. Afterwards, I worked as a Computational Biologist at Genentech and Verily. In 2016 I joined Denali Therapeutics, where I am collaborating with colleagues across the organization to generate, analyze and understand genomics & genetics data.\nThis blog collects the personal lessons I am learning along the way.  (Opinions are my own and not the views of my employer.)\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\nNov 18, 2023\n\n\nEmbedding R into Quarto documents with quarto-webr\n\n\nThomas Sandmann\n\n\n8 min\n\n\n\n\nSep 17, 2023\n\n\nRetrieving access-controlled data from NCBI’s dbGAP repository\n\n\nThomas Sandmann\n\n\n6 min\n\n\n\n\nSep 13, 2023\n\n\nAdventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays\n\n\nThomas Sandmann\n\n\n11 min\n\n\n\n\nSep 5, 2023\n\n\nAdventures with parquet II: Implementing the parquetArraySeed S4 class\n\n\nThomas Sandmann\n\n\n11 min\n\n\n\n\nAug 31, 2023\n\n\nAdventures with parquet: Storing & querying gene expression data\n\n\nThomas Sandmann\n\n\n7 min\n\n\n\n\nAug 30, 2023\n\n\nOrganizing sequencing metadata: experimenting with S7\n\n\nThomas Sandmann\n\n\n10 min\n\n\n\n\nAug 28, 2023\n\n\ntourrr: Exploring multi-dimensional data\n\n\nThomas Sandmann\n\n\n6 min\n\n\n\n\nJul 31, 2023\n\n\nCustomizing my Quarto website\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nJul 24, 2023\n\n\nGuess the correlation - a first streamlit app\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nJul 21, 2023\n\n\nGrav - a lightweight content management system\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nJul 21, 2023\n\n\nGreg Wilson: Late Night Thoughts on Listening to Ike Quebec (2018)\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nJun 26, 2023\n\n\nDocumenting data wrangling with the dtrackr R package\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nMay 6, 2023\n\n\nQuerying parquet files with duckdb\n\n\nThomas Sandmann\n\n\n6 min\n\n\n\n\nMar 12, 2023\n\n\nLemur: analyzing multi-condition single-cell data\n\n\nThomas Sandmann\n\n\n14 min\n\n\n\n\n\nFeb 25, 2023\n\n\nSimultaneously inserting records into two tables with Postgres CTEs\n\n\nThomas Sandmann\n\n\n4 min\n\n\n\n\nJan 21, 2023\n\n\nDistributing R packages with a drat repository hosted on AWS S3\n\n\nThomas Sandmann\n\n\n12 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (2): Exploring nf-core/rnaseq output\n\n\nThomas Sandmann\n\n\n4 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (3): Validating published results (no UMIs)\n\n\nThomas Sandmann\n\n\n14 min\n\n\n\n\nJan 16, 2023\n\n\nQuantSeq RNAseq analysis (4): Validating published results (with UMIs)\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\nJan 2, 2023\n\n\nSQL and noSQL approaches to creating & querying databases (using R)\n\n\nThomas Sandmann\n\n\n13 min\n\n\n\n\n\nDec 27, 2022\n\n\nInteractive GSEA results: visualizations with reactable & plotly\n\n\nThomas Sandmann\n\n\n27 min\n\n\n\n\nDec 24, 2022\n\n\nUpSet plots: comparing differential expression across contrasts\n\n\nThomas Sandmann\n\n\n8 min\n\n\n\n\nDec 22, 2022\n\n\nFigure size, layout & tabsets with Quarto\n\n\nThomas Sandmann\n\n\n2 min\n\n\n\n\nDec 12, 2022\n\n\nFull text search in Postgres - the R way\n\n\nThomas Sandmann\n\n\n11 min\n\n\n\n\nDec 11, 2022\n\n\nUpdating R the easy way: using rig command line tool\n\n\nThomas Sandmann\n\n\n2 min\n\n\n\n\nDec 10, 2022\n\n\n2022 normconf: lightning talks\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nDec 8, 2022\n\n\nThe rlist R package\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 17, 2022\n\n\nCreating custom badges for your README\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 15, 2022\n\n\nLearning nextflow: blasting multiple sequences\n\n\nThomas Sandmann\n\n\n8 min\n\n\n\n\nNov 14, 2022\n\n\nPython type hints\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 13, 2022\n\n\nFujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nNov 13, 2022\n\n\nRefreshing & exporting temporary AWS credentials\n\n\nThomas Sandmann\n\n\n3 min\n\n\n\n\nNov 12, 2022\n\n\nInstalling pyroe with conda\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\nNov 12, 2022\n\n\nWelcome To My Blog\n\n\nThomas Sandmann\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/fujita_2022/index.html",
    "href": "posts/fujita_2022/index.html",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "",
    "text": "Today I read the preprint Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex by Masahi Fujita and co-authors, published on biorXiv (Fujita et al., n.d.). The authors generated the largest brain single-nuclei RNA-seq dataset to date (that I am aware of), collecting data on dorsolateral prefrontal cortex (DLPFC) samples of 424 individuals from the ROS/MAP cohort.\nThis large sample size enabled them to assess the effect of genetic variation (e.g. single-nucleotide variants) on gene expression - one cell type at a time. The authors created pseudo-bulk gene expression profiles for each patient for 7 cell types and 81 cell subtypes.\nBecause neurons are highly abundant in the DLPFC, the largest number of nuclei originated from neurons, and the statistical power to detect eQTLs was lower in rarer cell types (e.g. microglia). This highlights the potential of enrichment methods, e.g. by fluorescent activate nuclei sorting (FANS) approached. (See e.g. (Kamath et al. 2022), who specifically enriched dopaminergic neurons or (Sadick et al. 2022), who enriched astrocytes and oligodendrocytes.)\nFujita et al were able to identify ~ 10,000 eGenes1, about half of which were shared across cell types. For example, they identified a novel eQTL (rs128648) for the APOE gene specifically in microglia.\nHaving identified novel eQTL relationships in vivo, the authors then used bulk RNA-seq measurements from a panel of induced pluripotent stem cells that had been differentiated either into neurons (iNeurons) or astrocytes (iAstrocytes) to test whether they could also observe the variants’ effects in vitro.\nDespite a relatively small sample size, a subset of eQTLs were replicated. But the the authors also point out unexpected discrepancies in the MAPT locus where they observed variant effects in the opposite direction from what they had observed by snRNA-seq.\nGene expression was significantly heritable in most cell types (except for those from which only small numbers of nuclei had been sampled). This allowed the authors to use their snRNA-seq dataset to impute cell type specific gene expression for large GWAS studies, e.g. for Alzheimer’s Disease, ALS, Parkinson’s Disease, and schizophrenia. This TWAS analysis detected e.g. 48 novel loci associated with AD in microglia, 22 of which had not been implicated previously.\nIn summary, this work by Fujita et al is an impressive achievement, demonstrating that single-cell/single-nuclei approaches have now become sufficiently scalable to power human genetics analyses.\nThe authors have already made the raw data for their study available on the AD Knowledge Portal. Thank you for sharing your data!\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/fujita_2022/index.html#footnotes",
    "href": "posts/fujita_2022/index.html#footnotes",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGene whose expression was significantly associated with one or more genetic variants (FDR &lt; 5%)↩︎"
  },
  {
    "objectID": "posts/tourr/index.html",
    "href": "posts/tourr/index.html",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "",
    "text": "Today I learned about exploring multivariate data using tours of projections into lower dimensions. The tourr R package makes it easy to experiment with different tours. Let’s go on a grand tour!\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/tourr/index.html#tldr",
    "href": "posts/tourr/index.html#tldr",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "",
    "text": "Today I learned about exploring multivariate data using tours of projections into lower dimensions. The tourr R package makes it easy to experiment with different tours. Let’s go on a grand tour!"
  },
  {
    "objectID": "posts/tourr/index.html#introduction",
    "href": "posts/tourr/index.html#introduction",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "Introduction",
    "text": "Introduction\nEarlier this month, Dianne Cook and Ursula Laa published Interactively exploring high-dimensional data and models in R, a free online book accompanied by the mulgar R package. It’s a great introduction to exploratory analysis of multivariate data 🚀.\nThe authors introduce data tours to interactively visualize high-dimensional data. (And also highlight the rich history of this field, including the PRIM-9 system created at Stanford in the early 1970s ).\nThe tourr R package provides user-friendly functions to run a tour."
  },
  {
    "objectID": "posts/tourr/index.html#a-gorilla-hiding-in-plain-sight",
    "href": "posts/tourr/index.html#a-gorilla-hiding-in-plain-sight",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "A gorilla hiding in plain sight",
    "text": "A gorilla hiding in plain sight\nIn 2020, Itai Yanai and Martin Lercher asked whether “focus on a specific hypothesis prevents the exploration of other aspects of the data”. 1 They simulated a dataset with two variables, bmi and steps for both male and female subjects. Let’s start with a similar dataset 2.\n\ngorilla &lt;- read.csv(\n  paste0(\"https://gist.githubusercontent.com/tomsing1/\",\n         \"d29496382e8b8f4163c34df46b00686f/raw/\",\n         \"40c0b7b5d25fff188a7365df59aa8634fef9adb9/gorilla.csv\")\n)\nwith(gorilla, plot(steps, bmi, col = ifelse(group == \"M\", \"navy\", \"firebrick\")))\n\n\n\n\nHere, we want to examine only the numerical measurements (e.g. bmi and steps ), so let’s remove the categorical group variable and add two noise variables to create a dataset with five numerical variables.\n\nfor (dimension in paste0(\"noise\", 1:2)) {\n  gorilla[[dimension]] &lt;- rnorm(n = nrow(gorilla))\n}\nnumeric_cols &lt;- setdiff(colnames(gorilla), \"group\")\nhead(gorilla)\n\n       bmi      steps group     noise1     noise2\n1 29.96000   145.6311     F  0.2262401 -0.2269717\n2 29.89818 10048.5437     M -1.0365464 -0.9440436\n3 23.46909  3859.2233     M  0.5676465 -0.6378377\n4 26.03455  7718.4466     M -2.3049200 -0.5047930\n5 19.51273 10776.6990     M  0.4935926 -0.5288485\n6 29.65091  3932.0388     M  1.0021255 -1.1318410\n\n\nPlotting all pairwise combinations of the 5 variables quickly reveals the gorilla hidden in the bmi ~ steps relationship:\n\npairs(gorilla[, numeric_cols], pch = \".\")"
  },
  {
    "objectID": "posts/tourr/index.html#taking-tours",
    "href": "posts/tourr/index.html#taking-tours",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "Taking tours",
    "text": "Taking tours\n\nlibrary(tourr)\nlibrary(gifski)  # to create animated gifs\n\n\ngorilla[, numeric_cols] &lt;- tourr::rescale(gorilla[, numeric_cols])\nclrs &lt;- c(\"#486030\", \"#c03018\", \"#f0a800\")\ngroup_col &lt;- clrs[as.numeric(factor(gorilla$group))]\n\n\nTaking a little tour\nThe little tour cycles through all axis parallel projections, reproducing all of the static plots we obtained with the pairs() call above (corresponding to 90 degree angles between the axes) as well as additional projections in between.\nAs expected, the gorilla cartoon reveals itself whenever the steps and bmi variables are projected into the x and y coordinates.\n\nif (interactive()) {\n  tourr::animate(data = gorilla[, numeric_cols], \n                 tour_path = little_tour(d = 2), \n                 display = display_xy())\n} else {\n  tourr::render_gif(\n    data = gorilla[, numeric_cols],\n    little_tour(), \n    display_xy(),\n    gif_file = \"little_tour.gif\",\n    width = 300,\n    height = 300,\n    frames = 500,\n    loop = TRUE\n  )\n}\n\n\n\n\nLittle tour\n\n\n\n\nGrand tour\nThe grand tour picks a new projection at random and smoothly interpolates between them, eventually showing every possible projection of the data into the selected number of dimensions (here: 2). With a very high dimensional dataset, traversing all possibilities can take quite a while.\n\nif (interactive()) {\n  tourr::animate(data = gorilla[, numeric_cols], \n                 tour_path = grand_tour(d = 2), \n                 display = display_xy())\n} else {\n  tourr::render_gif(\n    data = gorilla[, numeric_cols],\n    grand_tour(d = 2), \n    display_xy(),\n    gif_file = \"grand_tour.gif\",\n    width = 300,\n    height = 300,\n    frames = 500,\n    loop = TRUE\n  )\n}\n\n\n\n\nGrand tour"
  },
  {
    "objectID": "posts/tourr/index.html#adding-interactivity",
    "href": "posts/tourr/index.html#adding-interactivity",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "Adding interactivity",
    "text": "Adding interactivity\nDianne Cook’s and Ursula Laa’s book also demonstrates how to make the tours more interactive with the plotly and htmlwidgets R packages.\n\n\nCode to generate interactive animation\nlibrary(plotly, quietly = TRUE)\nlibrary(htmlwidgets, quietly = TRUE)\nset.seed(123)\nsubsample &lt;- sample(nrow(gorilla), size = 500L)\npn_t &lt;- tourr::save_history(data = gorilla[subsample, numeric_cols], \n                            tour_path = grand_tour())\npn_t &lt;- interpolate(pn_t, angle = 1)\npn_anim &lt;- render_anim(gorilla[subsample, numeric_cols], frames = pn_t)\n\npn_gp &lt;- suppressWarnings({\n  ggplot() +\n    geom_path(\n      data = pn_anim$circle, \n      aes(x = c1, y = c2, frame = frame - 100), \n      linewidth = 0.1) +\n    geom_segment(\n      data = pn_anim$axes, \n      aes(x = x1, y = y1, xend = x2, yend = y2, frame = frame - 100), \n      linewidth = 0.1) +\n    geom_text(\n      data = pn_anim$axes, \n      aes(x = x2, y = y2, label = axis_labels, frame = frame - 100), \n      size = 5) +\n    geom_point(\n      data = pn_anim$frames, \n      aes(x = P1, y = P2, frame = frame - 100), \n      alpha = 0.8, size = 0.5) +\n    xlim(-0.8, 0.8) + ylim(-0.8, 0.8) +\n    coord_equal() +\n    theme_bw() +\n    theme(axis.text = element_blank(),\n          axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          panel.grid = element_blank())\n})\nggplotly(\n  pn_gp,\n  width = 500,\n  height = 550) %&gt;%\n  animation_button(label=\"Go\") %&gt;%\n  animation_slider(len = 0.8, x = 0.5, xanchor = \"center\", \n                   currentvalue = list(prefix = \"frame: \")) %&gt;%\n  animation_opts(easing = \"linear\", transition = 0)"
  },
  {
    "objectID": "posts/tourr/index.html#reproducibility",
    "href": "posts/tourr/index.html#reproducibility",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSession Information\n\n\nsessioninfo::session_info(\"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version  date (UTC) lib source\n P ggplot2     * 3.4.3    2023-08-14 [?] CRAN (R 4.3.0)\n P gifski      * 1.12.0-2 2023-08-12 [?] CRAN (R 4.3.0)\n P htmlwidgets * 1.6.2    2023-03-17 [?] CRAN (R 4.3.0)\n P plotly      * 4.10.2   2023-06-03 [?] CRAN (R 4.3.0)\n P tourr       * 1.1.0    2023-08-24 [?] CRAN (R 4.3.0)\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/tourr/index.html#footnotes",
    "href": "posts/tourr/index.html#footnotes",
    "title": "tourrr: Exploring multi-dimensional data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn addition to being asked what they could conclude from the dataset, half of the students were asked to also test specific hypotheses. […] students in the hypothesis-free group were almost 5 times more likely to observe an image of a gorilla when simply plotting the data, a proxy for an initial step towards data analysis.↩︎\nMatt Dray showed how to recreate the dataset using R in this great blog post., the code to generate the gorilla dataset I use here is in this gist.↩︎"
  },
  {
    "objectID": "posts/grav/index.html",
    "href": "posts/grav/index.html",
    "title": "Grav - a lightweight content management system",
    "section": "",
    "text": "Today I learned about Grav, a lightweight content-management system, from danwwilson. Grav is based on flat files, e.g. it does not require a database, and automatically renders pages written in markdown.\nThe optional admin plugin includes user management, 2-factor authentication and more.\nDefinitely something I will keep in mind, e.g. to distribute analysis results within an organization.\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/conda-speedup/index.html",
    "href": "posts/conda-speedup/index.html",
    "title": "2022 normconf: lightning talks",
    "section": "",
    "text": "I am really looking forward to the virtual #normconf conference on 2022-12-15. In addition to a great program the meeting also features numerous ~ 5 minute lightning talks\nThe full list of lightning talks is available here but here are my favorites:\n\nJacquelin Nolis: Alaska challenged my preconceived notions of storing sunset data\nJenny Bryan: How to name files like a normie\nChelsea Parlett: Why Are You The Way That You Are: Sklearn Quirks\nZachary Chetchavat: Hotkeys for Spreadsheets Cookbook, Practical Solutions from CTRL-Arrow, to F4\nShoili Pal: Data Science Intake Forms\nAmanda Fioritto: Qualify: The SQL Filtering Pattern You Never Knew You Needed\nJuulia Suvilehto: Trying to convince academics to use git\nAnuvabh Dutt: Config files for fast and reproducible ML experiments\nJane Adams: How to make six figures in an hour (slides)\nBryan Bischof: Toss that (model) in an endpoint\nSophia Yang: PyScript: Run Python in your HTML\nVictor Geislinger: Staying Alive: Persistent SSH Sessions w/ tmux\nTom Baldwin: Putting Git’s commit hash in version, two ways\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/pyroe-installation/index.html",
    "href": "posts/pyroe-installation/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "Alevin-fry is a highly accurate and performant method to process single-cell or single-nuclei RNA-seq data. For downstream processing, its output can be parsed into R with the fishpond::loadFry() function. For analysis using python, the pyroe module is available.\nIt can be installed either using pip or conda, and the latter will install additional dependencies (e.g. bedtools) and include the load_fry() as well.\nTo install pyroe with conda, I first followed bioconda’s instructions to add and configure the required channels:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nand then installed pyroe\nconda install pyroe\nNow I can convert alevin-fry output to one of the following formats: zarr, csvs, h5ad or loom.\npyroe convert --help\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/lemur/index.html",
    "href": "posts/lemur/index.html",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "",
    "text": "This week, Constantin Ahlmann-Eltze and Wolfgang Huber published a preprint describing LEMUR, a new approach to analyzing single-cell experiments that include samples from multiple conditions, e.g. drug treatments, disease-status, etc.\nTo date, such analyses often involve two separate steps, e.g.\nIn contrast, LEMUR considers the continuous latent space the individual cells occupy, incorporating the design of the experiment, and then performs differential expression analysis in this embedding space.\nAn R package implementing LEMUR is available from github and includes an example dataset 1.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/lemur/index.html#ellwanger-et-al-comparing-trem2-wildtype-and-knock-out-mouse-microglia",
    "href": "posts/lemur/index.html#ellwanger-et-al-comparing-trem2-wildtype-and-knock-out-mouse-microglia",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Ellwanger et al: Comparing Trem2 wildtype and knock-out mouse microglia",
    "text": "Ellwanger et al: Comparing Trem2 wildtype and knock-out mouse microglia\nHere, I am exploring LEMUR by examining scRNA-seq data published by Ellwanger et al, 2021, who injected three strains of 5XFAD mice, a murine model of familial Alzheimer’s Disease, either\n\ncarrying the wild-type (WT) Trem2 gene,\ncarrying the R47H Trem2 variant, believed to be a loss-of-function variant,\nor completely lacking Trem2 expression\n\nwith either a Trem2 agonist (hT2AB) or a negative control antibody (hIgG1).\n48 hours later, the authors isolated CD45-positive 2 cells from the cortex and performed single-cell RNA-seq analysis using the 10X Genomics platform.\n\nRetrieving the data\n\nlibrary(dplyr)\nlibrary(Matrix)\nlibrary(org.Mm.eg.db)\nlibrary(patchwork)\nlibrary(purrr)\nlibrary(readr)\nlibrary(scater)\nlibrary(SingleCellExperiment)\nlibrary(tidyr)\n\nEllwanger et al made both raw and processed data available via the NCBI GEO and SRA repositories under GEO accession GSE156183.\nThey also included complete metadata for each cell, making this a great dataset for re-analysis.\nLet’s start by retrieving the\n\nprocessed counts (500 Mb) and the\ncell metadata (13 Mb)\n\nfiles from GEO and store them in a temporary directory:\n\ntemp_dir &lt;- file.path(tempdir(), \"ellwanger\")\ndir.create(temp_dir, showWarnings = FALSE, recursive = TRUE)\n\noptions(timeout = 360)\nurl_root &lt;- paste0(\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=\",\n                   \"GSE156183&format=file&file=GSE156183%5F\")\n\nraw_counts &lt;- file.path(temp_dir, \"counts.mtx.gz\")\ndownload.file(\n  paste0(url_root, \"RAW%2Emtx%2Egz\"), \n  destfile = raw_counts)\n\ncell_metadata &lt;- file.path(temp_dir, \"cell_metadata.tsv.gz\")\ndownload.file(\n  paste0(url_root, \"Cell%5Fmetadata%2Etsv%2Egz\"), \n  destfile = cell_metadata)\n\nand read the sparse count matrix into our R session:\n\nm &lt;- Matrix::readMM(raw_counts)\ncell_anno &lt;- readr::read_tsv(cell_metadata, show_col_types = FALSE)\nstopifnot(nrow(cell_anno) == ncol(m))\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nUnfortunately, the GSE156183_Feature_metadata.tsv.gz feature (= gene) annotation file the authors deposited with GEO actually contains cell annotations. But luckily, they also deposited counts matrices in TSV format for each sample, which include the ENSEMBL gene identifier for each row.\nHere, I download the TAR archive that contains all of the TSV files, and then extract the gene identifiers from one of the files so I can add them to the experiment-wide raw count matrix.\n\nselected_sample &lt;- \"GSM4726219_RAW-R47H-male-IgG-rep2.tsv.gz\"\ntar_archive &lt;- file.path(temp_dir, \"RAW.tar\")\ndownload.file(\n  \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE156183&format=file\",\n  destfile = tar_archive)\nutils::untar(tar_archive, files = selected_sample, exdir = tempdir())\ngene_ids &lt;- readr::read_tsv(file.path(tempdir(), selected_sample), \n                            col_select = any_of(\"feature_id\"),\n                            show_col_types = FALSE) %&gt;%\n  dplyr::pull(feature_id)\nstopifnot(length(gene_ids) == nrow(m))\nrow.names(m) &lt;- gene_ids\n\n\n\n\n\n\nCreating a SingleCellExperiment object\nNow I have all the pieces of information required to create a SingleCellExperiment:\n\nthe raw counts (in the form of a sparse matrix),\nthe cell annotations (in the form of a data.frame)\nthe two UMAP dimensions used by the authors (included in the cell metadata).\n\nI choose to retain only a subset of the (many) cell-level annotation columns, add gene symbols as row annotations, extract the UMAP coordinates into a separate matrix - and store all of it in the sce object.\nNext, I am removing cells without an assigned cell type, and also add a coarser cell type annotation that collapses the different microglia states reported by the authors into a single category. Finally, I filter genes without a valid gene symbol and add an assay slot with the normalized log2 counts.\n\ncol_data &lt;- cell_anno %&gt;%\n  dplyr::select(cell_id, celltype, sample, sex, genotype, treatment, \n                starts_with(\"QC.\")\n  ) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(\"cell_id\")\ncolnames(m) &lt;- row.names(col_data)\n\nrow_data &lt;- data.frame(\n  symbol = AnnotationDbi::mapIds(org.Mm.eg.db, keys = gene_ids,\n                                 column = \"SYMBOL\", keytype = \"ENSEMBL\"),\n  row.names = gene_ids\n)\n\numap &lt;- cell_anno %&gt;%\n  dplyr::select(ends_with(\"CD45pos\")\n  ) %&gt;%\n  as.matrix()\nrow.names(umap) &lt;- colnames(m)\ncolnames(umap) &lt;- paste(\"UMAP\", seq.int(ncol(umap)))\n\nsce &lt;- SingleCellExperiment(\n  assays = list(counts = m),\n  rowData = row_data,\n  colData = col_data,\n  reducedDims = list(UMAP = umap)\n)\n\nsce &lt;- sce[, !is.na(sce$celltype)]\nsce$celltype_coarse &lt;- dplyr::case_when(\n    grepl(x = sce$celltype, pattern = \"Microglia\") ~ \"Microglia\",\n    TRUE ~ sce$celltype\n  )\nsce$treatment &lt;- factor(sce$treatment, levels = c(\"IgG\", \"hT2AB\"))\nsce$mg_type &lt;- factor(sub(\"Microglia.\", \"\", sce$celltype, fixed = TRUE))\nsce &lt;- sce[!is.na(rowData(sce)$symbol), ]\nsce &lt;- logNormCounts(sce)\n\nrm(list = c(\"m\", \"cell_anno\", \"gene_ids\", \"row_data\", \"col_data\"))\n\nThis SingleCellExperiment object is now ready for downstream analysis."
  },
  {
    "objectID": "posts/lemur/index.html#subsetting-the-experiment-to-samples-of-interest",
    "href": "posts/lemur/index.html#subsetting-the-experiment-to-samples-of-interest",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Subsetting the experiment to samples of interest",
    "text": "Subsetting the experiment to samples of interest\nThis study contains multiple experimental variables, e.g. each sample is annotated with one of the three genotypes, one of two treatments and the sex for each mouse.\nHere, I will focus only on the difference between TREM2 wildtype and TREM2 knock-out animals treated with the IgG control antibody. Only female knock-out animals were included in the study, so I exclude the male animals from the other strain as well.\n\nsce &lt;- sce[, which(sce$genotype %in% c(\"TREM2_CV-5XFAD\", \"Trem2_KO-5XFAD\"))]\nsce &lt;- sce[, which(sce$sex == \"female\" & sce$treatment == \"IgG\")]\nwith(colData(sce), table(genotype, treatment))\n\n                treatment\ngenotype          IgG hT2AB\n  TREM2_CV-5XFAD 3781     0\n  Trem2_KO-5XFAD 8911     0\n\n\nAfter subsetting, the experiment now contains 5 samples:\n\ncolData(sce) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::select(sample, treatment, genotype) %&gt;%\n  dplyr::distinct() %&gt;%\n  tibble::remove_rownames()\n\n              sample treatment       genotype\n1 CV-female-IgG-rep1       IgG TREM2_CV-5XFAD\n2 CV-female-IgG-rep2       IgG TREM2_CV-5XFAD\n3 KO-female-IgG-rep1       IgG Trem2_KO-5XFAD\n4 KO-female-IgG-rep2       IgG Trem2_KO-5XFAD\n5 KO-female-IgG-rep3       IgG Trem2_KO-5XFAD\n\n\nAt this point, I can reproduce e.g. a version of Figure 3E from the original paper, using the UMAP coordinates and cell type labels provided by the authors. (My version of the figure only includes cells from the selected subset of samples, not all cells captured in the study.)\n\ncolors &lt;- c(\"Microglia\" = \"darkgrey\",\n            \"T cells\" = \"skyblue\",\n            \"Macrophages\" = \"firebrick\",\n            \"MO:T\" = \"darkgreen\",\n            \"Dendritic cells\" = \"green\",\n            \"Monocytes\" = \"orange\",\n            \"B cells\" = \"navy\",\n            \"Neutrophils\" = \"darkblue\",\n            \"HCS\" = \"grey\", \n            \"Fibroblasts\" = \"yellow\")\nscater::plotReducedDim(sce, \"UMAP\", colour_by = \"celltype_coarse\") +\n  scale_color_manual(values = colors, name = \"Cell type\")\n\n\n\n\nBecause Ellwanger et al captured all cells with CD45 expression, the dataset includes other immune cell types besides microglia. Let’s remove those to focus only on the latter.\n\nsce &lt;- sce[, sce$celltype_coarse == \"Microglia\"]\nsce$mg_type &lt;- factor(\n  sce$mg_type, \n  levels = c(\"Resting\", \"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"IFN-R\", \"DAM\", \n             \"MHC-II\", \"Cyc-M\"))\n\nMost microglial states were captured in animals from both genotypes:\n\nmg_colors &lt;- c(\n  \"Cyc-M\" = \"navy\",\n  \"DAM\" = \"darkgreen\",\n  \"IFN-R\" = \"#C12131\", \n  \"MHC-II\" = \"green\",\n  \"Resting\" = \"grey50\",\n  \"t1\" = \"#FDF5EB\",   \n  \"t2\" =  \"#FFE2C0\", \n  \"t3\" = \"#FFC08E\",\n  \"t4\" = \"#FE945C\",\n  \"t5\" =  \"#EC5D2F\",\n  \"t6\" = \"#C12131\"\n)\nscater::plotReducedDim(sce, \"UMAP\", colour_by = \"mg_type\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  facet_wrap(~ sce$genotype)"
  },
  {
    "objectID": "posts/lemur/index.html#differential-expression-analysis-with-lemur",
    "href": "posts/lemur/index.html#differential-expression-analysis-with-lemur",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Differential expression analysis with lemur",
    "text": "Differential expression analysis with lemur\nNow I am ready to explore the lemur R package to ask: “which neighborhoods show for differential expression between samples from WT and knock-out animals?”\nThe following steps closely follow the examples outlined on the lemor github repository’s README - many thanks for the great documentation, Constantin! (All mistakes and misunderstandings in this post are my own, as always.)\n\nDependencies & installation\nFollowing the instructions from the lemu github repository I then installed the latest version of the glmGamPoi package, and then the lemur package itself from their github repositories.\nTo harmonize results across batches (in this case: samples), I will use harmony, so I need to install it from its github repository as well.\n\nremotes::install_github(\"const-ae/glmGamPoi\")\nremotes::install_github(\"const-ae/lemur\")\nremotes::install_github(\"immunogenomics/harmony\")\n\n\n\nSubsetting the experiment\n\nlibrary(lemur)\nn_cells &lt;- 1000L\n\nTo speed up my exploration of the LEMUR workflow, I subset the experiment to 1000 random cells from each of the two genotypes.\n\nset.seed(1L)\ngenotypes &lt;- unique(sce$genotype)\nselected_cells &lt;- as.vector(sapply(genotypes, \\(g) {\n  sample(which(sce$genotype == g), n_cells)\n}))\n\nAs expected, most microglial states described in the paper remain represented in the downsampled dataset:\n\ntable(sce$celltype[selected_cells], sce$genotype[selected_cells])\n\n                   \n                    TREM2_CV-5XFAD Trem2_KO-5XFAD\n  Microglia.Cyc-M               17             32\n  Microglia.DAM                122            127\n  Microglia.IFN-R               46             33\n  Microglia.MHC-II              16              0\n  Microglia.Resting            101            117\n  Microglia.t1                 145            133\n  Microglia.t2                 119             71\n  Microglia.t3                  84            140\n  Microglia.t4                 161             93\n  Microglia.t5                 132            209\n  Microglia.t6                  57             45\n\n\n\nscater::plotReducedDim(\n  sce[, selected_cells], \"UMAP\", colour_by = \"mg_type\") +\n  labs(title = sprintf(\"Subsampled to %s microglia\", length(selected_cells))) +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  facet_wrap(~ sce$genotype[selected_cells])\n\n\n\n\n\n\nFitting the LEMUR model\nNext, I fit the latent embedding multivariate regression (LEMUR) model with the lemur() function. Because the dataset is relatively homogeneous, e.g. it contains only microglia, I chose to consider only 25 Principal Components and used 15 dimensions for the LEMUR embedding (e.g. the default number).\n\nfit &lt;- lemur::lemur(sce[, selected_cells], design = ~ genotype,\n                    n_ambient = 25, n_embedding = 15, verbose = FALSE)\n\nBecause each sample was processed in a separate channel of the 10X Genomics microfluidics device, I am aligning the embeddings of similar cell clusters using harmony.\n\nfit &lt;- lemur::align_harmony(fit, stretching = FALSE)\nfit\n\nclass: lemur_fit \ndim: 23870 2000 \nmetadata(12): n_ambient n_embedding ... alignment_design\n  alignment_design_matrix\nassays(1): expr\nrownames(23870): ENSMUSG00000051951 ENSMUSG00000025900 ...\n  ENSMUSG00000094915 ENSMUSG00000079808\nrowData names(1): symbol\ncolnames(2000): CELL21559 CELL21072 ... CELL45651 CELL52059\ncolData names(22): celltype sample ... celltype_coarse mg_type\nreducedDimNames(2): linearFit embedding\nmainExpName: NULL\naltExpNames(0):\n\n\nThe returned lemur_fit object contains the embedding matrix, the latent space in which the differential expression analysis is performed.\n\ndim(fit$embedding)  # 15 dimensions, as specified above\n\n[1]   15 2000\n\n\nLet’s plot the first two dimensions against each other, coloring each cell by the microglial state Ellwanger et al identified through Louvain clustering. (This information has not been used by lemur):\n\n# plot dim 1 vs dim 2\nscater::plotReducedDim(\n  fit, \"embedding\", colour_by = \"mg_type\", shape_by = \"genotype\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  labs(\n    title = \"Embedding after accounting for genotype\",\n    subtitle = sprintf(\"Subsampled to %s microglia\", length(selected_cells)))\n\n\n\n\nCells group by Ellwanger et al’s subtype labels, and cells from both genotypes are intermixed. We can obtain an alternative visualization by arranging the cells in two dimensions using Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP):\n\n# run UMAP on the embedding\numap &lt;- uwot::umap(t(fit$embedding))\ncolnames(umap) &lt;- c(\"UMAP 1\", \"UMAP 2\")\nreducedDim(fit, \"UMAP\") &lt;- umap\n\n\nscater::plotReducedDim(\n  fit, \"UMAP\", colour_by = \"mg_type\", shape_by = \"genotype\") +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  labs(\n    title = \"Embedding after accounting for genotype (UMAP)\",\n    subtitle = sprintf(\"Subsampled to %s microglia\", length(selected_cells))) +\n  facet_wrap(~ colData(fit)$genotype)\n\n\n\n\n\n\nTesting for differential expression\nNext, the test_de function performs a differential expression analysis for locations in the embedding - by default, it will estimate it for each location an original cell was mapped to.\nThe find_de_neighborhoods function accepts the original counts and will estimate the log2 fold change for each neighborhood, based on aggregating the counts to pseudobulk measures across the cells in each neighborhood.\n\nfit &lt;- test_de(\n  fit, \n  contrast = cond(genotype = \"Trem2_KO-5XFAD\") - \n    cond(genotype = \"TREM2_CV-5XFAD\"))\nneighborhoods &lt;- find_de_neighborhoods(\n  fit, \n  counts = counts(sce)[, selected_cells],\n  group_by = vars(sample, genotype),\n  include_complement = FALSE) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::arrange(pval) %&gt;%\n  dplyr::left_join(\n    tibble::rownames_to_column(as.data.frame(rowData(fit)), \"gene_id\"), \n    by = c(name = \"gene_id\")) %&gt;%\n  dplyr::select(symbol, everything())\n\nThe neighborhoods data.frame contains differential expression statistics for each gene.\n\nhead(neighborhoods)\n\n# A tibble: 6 × 12\n  symbol name       region indices n_cells   mean     pval adj_p…¹ f_sta…²   df1\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  &lt;I&lt;lis&gt;   &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 Fxyd5  ENSMUSG00… 1      &lt;int&gt;      1809 -0.251 6.09e-10 1.45e-5   118.      1\n2 Il4i1  ENSMUSG00… 1      &lt;int&gt;      1069 -0.181 2.40e- 9 2.87e-5   101.      1\n3 Lpl    ENSMUSG00… 1      &lt;int&gt;       830 -0.469 5.82e- 9 4.63e-5    90.6     1\n4 Cd74   ENSMUSG00… 1      &lt;int&gt;       611 -1.12  9.25e- 9 5.52e-5    85.7     1\n5 Axl    ENSMUSG00… 1      &lt;int&gt;      1584 -0.257 1.78e- 8 8.49e-5    79.2     1\n6 H2-Aa  ENSMUSG00… 1      &lt;int&gt;      1172 -0.247 6.81e- 8 2.71e-4    67.1     1\n# … with 2 more variables: df2 &lt;dbl&gt;, lfc &lt;dbl&gt;, and abbreviated variable names\n#   ¹​adj_pval, ²​f_statistic\n\n\nA volcano plot shows that we recovered a number of genes with differential expression in one or more neighborhoods (after accounting for multiple testing):\n\n# volcano plot\nneighborhoods %&gt;%\n  ggplot(aes(x = lfc, y = -log10(pval))) +\n    geom_point(aes(color  = adj_pval &lt; 0.1), alpha = 0.5) +\n    labs(title = \"Volcano plot of the neighborhoods\") + \n  scale_color_manual(values = c(\"TRUE\" = \"firebrick\", \"FALSE\" = \"grey\")) +\n  theme_bw() + \n  theme(panel.grid = element_blank())\n\n\n\n\nFor example, transcripts of the Lipoprotein lipase (Lpl) gene are generally expressed at lower levels in the Trem2 knock-out than in wildtype samples.\nBut there is also evidence for stronger differences in expression in microglia that have adopted specific states. The largest log2 fold changes are observed in Cyc-M, and DAM microglia, while the Resting microglia are mainly excluded from the neighborhood detected by lemur.\n\nsel_gene &lt;- row.names(sce)[which(rowData(sce)$symbol == \"Lpl\")]\n\nneighborhood_coordinates &lt;- neighborhoods %&gt;%\n  dplyr::filter(name == sel_gene) %&gt;%\n  dplyr::mutate(cell_id = purrr:::map(indices, \\(idx) colnames(fit)[idx])) %&gt;%\n  tidyr::unnest(c(indices, cell_id)) %&gt;%\n  dplyr::left_join(as_tibble(umap, rownames = \"cell_id\"), by = \"cell_id\") %&gt;%\n  dplyr::select(name, cell_id, `UMAP 1`, `UMAP 2`)\n\np1 &lt;- as_tibble(umap) %&gt;%\n  mutate(expr = assay(fit, \"DE\")[sel_gene, ]) %&gt;%\n  ggplot(aes(x = `UMAP 1`, y = `UMAP 2`)) +\n  scale_color_gradient2() +\n  geom_point(aes(color = expr)) +\n  geom_density2d(data = neighborhood_coordinates, breaks = 0.1, \n                 contour_var = \"ndensity\", color = \"black\") +\n  labs(title = rowData(sce)[sel_gene, \"symbol\"]) + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\np2 &lt;- as_tibble(umap) %&gt;%\n  dplyr::bind_cols(as.data.frame(colData(fit))) %&gt;%\n  ggplot(aes(x = `UMAP 1`, y = `UMAP 2`)) +\n  geom_point(aes(color = mg_type)) +\n  scale_color_manual(values = mg_colors, name = element_blank()) + \n  geom_density2d(data = neighborhood_coordinates, breaks = 0.1, \n                 contour_var = \"ndensity\", color = \"black\") +\n  labs(title = \"Microglia states\") + \n  theme_bw() + \n  theme(panel.grid = element_blank())\n\np1 + p2\n\n\n\n\nNext, I examine each microglial subtype separately, and split cells according to whether they fall into a neighborhood with significant differential Lpl expression (DE) or not (not DE).\nMost Cyc-M and DAM microglia are located in neighborhoods with reduced Lpl expression in knock-out samples, e.g. the majority of these cells is in the DE column. The opposite is true for Resting microglia: nearly all of them are outside the significant Lpl differential expression neighborhood.\nThe transitional subtypes, t1 (most similar to resting microglia) to t6 (most similar to DAM, Cyc-M or IFN-R) fall in between, with a gradual increase along their proposed differentiation sequence.\n\nwithin_neighborhood &lt;- neighborhoods %&gt;%\n  dplyr::filter(name == sel_gene) %&gt;% \n  dplyr::pull(indices)\n\ncolData(fit)$neighborhood &lt;- ifelse(\n  seq.int(ncol(fit)) %in% within_neighborhood[[1]], \"DE\", \"not DE\")\n\ndata.frame(\n  colData(fit)\n) %&gt;%\n  ggplot(aes(x = neighborhood, fill = mg_type)) + \n  geom_bar(stat = \"count\", color = \"black\", linewidth = 0.2) +\n  facet_wrap(~ mg_type) +\n  labs(title = rowData(sce)[sel_gene, \"symbol\"], \n       x = element_blank(),\n       y = \"Cells per neighborhood\") +\n  scale_fill_manual(values = mg_colors, name = element_blank()) + \n  theme_linedraw(14) + \n  theme(panel.grid = element_blank())\n\n\n\n\nFinally, we can plot the estimated Lpl log2 fold changes for each cells annotated with the various subtype labels. This confirms the observations outlined above, providing an example of how insights from LEMUR can be combined with coarser, clustering-based insights.\n\ndata.frame(\n  expr = assay(fit, \"DE\")[sel_gene, ],\n  colData(fit)\n) %&gt;%\n  ggplot(aes(x = mg_type, y = expr, fill = mg_type)) + \n  geom_boxplot() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  scale_fill_manual(values = mg_colors, name = element_blank()) + \n  labs(title = rowData(sce)[sel_gene, \"symbol\"],\n       y = \"Genotype effect (log2 FC)\", \n       x = element_blank()) +\n  theme_linedraw(14) + \n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/lemur/index.html#conclusions",
    "href": "posts/lemur/index.html#conclusions",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Conclusions",
    "text": "Conclusions\n\nThis dataset starts off with a relatively homogeneous cell population, e.g. it is designed to examine subtle differences within a single cell type (microglia.)\nRemoving Trem2 activity is known to shift the composition of the microglial subsets, e.g. depleting DAM microglia and increasing the frequency of Resting in the knock-out versus the wildtype samples. This adds an additional challenge to the task of identifying differential expression.\nThe lemur() package nevertheless successfully identified genes that track the differential expression of the microglial sub-states reported by Ellwanger et al. (Even with only a subsample of the data.)\nI am looking forward to exploring LEMUR in future datasets, e.g. those examining the effects of drug perturbation in single-nuclei RNA-seq datasets sampling a large variety of CNS cell types.\n\n\n\nReproducibility\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-03-12\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date (UTC) lib source\n AnnotationDbi        * 1.60.0   2022-11-01 [1] Bioconductor\n askpass                1.1      2019-01-13 [1] CRAN (R 4.2.0)\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.2.0)\n beachmat               2.14.0   2022-11-01 [1] Bioconductor\n beeswarm               0.4.0    2021-06-01 [1] CRAN (R 4.2.0)\n Biobase              * 2.58.0   2022-11-01 [1] Bioconductor\n BiocGenerics         * 0.44.0   2022-11-01 [1] Bioconductor\n BiocNeighbors          1.16.0   2022-11-01 [1] Bioconductor\n BiocParallel           1.32.4   2022-12-01 [1] Bioconductor\n BiocSingular           1.14.0   2022-11-01 [1] Bioconductor\n Biostrings             2.66.0   2022-11-01 [1] Bioconductor\n bit                    4.0.5    2022-11-15 [1] CRAN (R 4.2.0)\n bit64                  4.0.5    2020-08-30 [1] CRAN (R 4.2.0)\n bitops                 1.0-7    2021-04-24 [1] CRAN (R 4.2.0)\n blob                   1.2.3    2022-04-10 [1] CRAN (R 4.2.0)\n cachem                 1.0.6    2021-08-19 [1] CRAN (R 4.2.0)\n cli                    3.5.0    2022-12-20 [1] CRAN (R 4.2.0)\n codetools              0.2-18   2020-11-04 [2] CRAN (R 4.2.2)\n colorspace             2.0-3    2022-02-21 [1] CRAN (R 4.2.0)\n cowplot                1.1.1    2020-12-30 [1] CRAN (R 4.2.0)\n crayon                 1.5.2    2022-09-29 [1] CRAN (R 4.2.0)\n credentials            1.3.2    2021-11-29 [1] CRAN (R 4.2.0)\n DBI                    1.1.3    2022-06-18 [1] CRAN (R 4.2.0)\n DelayedArray           0.24.0   2022-11-01 [1] Bioconductor\n DelayedMatrixStats     1.20.0   2022-11-01 [1] Bioconductor\n digest                 0.6.31   2022-12-11 [1] CRAN (R 4.2.0)\n dplyr                * 1.0.10   2022-09-01 [1] CRAN (R 4.2.0)\n ellipsis               0.3.2    2021-04-29 [1] CRAN (R 4.2.0)\n evaluate               0.19     2022-12-13 [1] CRAN (R 4.2.0)\n expm                   0.999-7  2023-01-09 [1] CRAN (R 4.2.0)\n fansi                  1.0.3    2022-03-24 [1] CRAN (R 4.2.0)\n farver                 2.1.1    2022-07-06 [1] CRAN (R 4.2.0)\n fastmap                1.1.0    2021-01-25 [1] CRAN (R 4.2.0)\n FNN                    1.1.3.1  2022-05-23 [1] CRAN (R 4.2.0)\n generics               0.1.3    2022-07-05 [1] CRAN (R 4.2.0)\n GenomeInfoDb         * 1.34.4   2022-12-01 [1] Bioconductor\n GenomeInfoDbData       1.2.9    2022-12-12 [1] Bioconductor\n GenomicRanges        * 1.50.2   2022-12-16 [1] Bioconductor\n ggbeeswarm             0.7.1    2022-12-16 [1] CRAN (R 4.2.0)\n ggplot2              * 3.4.0    2022-11-04 [1] CRAN (R 4.2.0)\n ggrepel                0.9.2    2022-11-06 [1] CRAN (R 4.2.0)\n glmGamPoi              1.11.7   2023-03-11 [1] Github (const-ae/glmGamPoi@78c5cff)\n glue                   1.6.2    2022-02-24 [1] CRAN (R 4.2.0)\n gridExtra              2.3      2017-09-09 [1] CRAN (R 4.2.0)\n gtable                 0.3.1    2022-09-01 [1] CRAN (R 4.2.0)\n harmony                0.1.1    2023-03-11 [1] Github (immunogenomics/harmony@63ebd73)\n here                   1.0.1    2020-12-13 [1] CRAN (R 4.2.0)\n hms                    1.1.2    2022-08-19 [1] CRAN (R 4.2.0)\n htmltools              0.5.4    2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets            1.5.4    2021-09-08 [1] CRAN (R 4.2.2)\n httr                   1.4.4    2022-08-17 [1] CRAN (R 4.2.0)\n IRanges              * 2.32.0   2022-11-01 [1] Bioconductor\n irlba                  2.3.5.1  2022-10-03 [1] CRAN (R 4.2.0)\n isoband                0.2.6    2022-10-06 [1] CRAN (R 4.2.0)\n jsonlite               1.8.4    2022-12-06 [1] CRAN (R 4.2.0)\n KEGGREST               1.38.0   2022-11-01 [1] Bioconductor\n knitr                  1.41     2022-11-18 [1] CRAN (R 4.2.0)\n labeling               0.4.2    2020-10-20 [1] CRAN (R 4.2.0)\n lattice                0.20-45  2021-09-22 [2] CRAN (R 4.2.2)\n lemur                * 0.0.9    2023-03-11 [1] Github (const-ae/lemur@efdb6b3)\n lifecycle              1.0.3    2022-10-07 [1] CRAN (R 4.2.0)\n magrittr               2.0.3    2022-03-30 [1] CRAN (R 4.2.0)\n MASS                   7.3-58.1 2022-08-03 [2] CRAN (R 4.2.2)\n Matrix               * 1.5-3    2022-11-11 [1] CRAN (R 4.2.0)\n MatrixGenerics       * 1.10.0   2022-11-01 [1] Bioconductor\n matrixStats          * 0.63.0   2022-11-18 [1] CRAN (R 4.2.0)\n memoise                2.0.1    2021-11-26 [1] CRAN (R 4.2.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.2.0)\n openssl                2.0.5    2022-12-06 [1] CRAN (R 4.2.0)\n org.Mm.eg.db         * 3.16.0   2022-12-29 [1] Bioconductor\n patchwork            * 1.1.2    2022-08-19 [1] CRAN (R 4.2.0)\n pillar                 1.8.1    2022-08-19 [1] CRAN (R 4.2.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.2.0)\n png                    0.1-8    2022-11-29 [1] CRAN (R 4.2.0)\n purrr                * 1.0.0    2022-12-20 [1] CRAN (R 4.2.0)\n R6                     2.5.1    2021-08-19 [1] CRAN (R 4.2.0)\n Rcpp                   1.0.9    2022-07-08 [1] CRAN (R 4.2.0)\n RCurl                  1.98-1.9 2022-10-03 [1] CRAN (R 4.2.0)\n readr                * 2.1.3    2022-10-01 [1] CRAN (R 4.2.0)\n rlang                  1.0.6    2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown              2.20     2023-01-19 [1] RSPM (R 4.2.2)\n rprojroot              2.0.3    2022-04-02 [1] CRAN (R 4.2.0)\n RSQLite                2.2.19   2022-11-24 [1] CRAN (R 4.2.0)\n rstudioapi             0.14     2022-08-22 [1] CRAN (R 4.2.0)\n rsvd                   1.0.5    2021-04-16 [1] CRAN (R 4.2.0)\n S4Vectors            * 0.36.1   2022-12-05 [1] Bioconductor\n ScaledMatrix           1.6.0    2022-11-01 [1] Bioconductor\n scales                 1.2.1    2022-08-20 [1] CRAN (R 4.2.0)\n scater               * 1.26.1   2022-11-13 [1] Bioconductor\n scuttle              * 1.8.3    2022-12-14 [1] Bioconductor\n sessioninfo            1.2.2    2021-12-06 [1] CRAN (R 4.2.0)\n SingleCellExperiment * 1.20.0   2022-11-01 [1] Bioconductor\n sparseMatrixStats      1.10.0   2022-11-01 [1] Bioconductor\n stringi                1.7.8    2022-07-11 [1] CRAN (R 4.2.0)\n stringr                1.5.0    2022-12-02 [1] CRAN (R 4.2.0)\n SummarizedExperiment * 1.28.0   2022-11-01 [1] Bioconductor\n sys                    3.4.1    2022-10-18 [1] CRAN (R 4.2.0)\n tibble                 3.1.8    2022-07-22 [1] CRAN (R 4.2.0)\n tidyr                * 1.2.1    2022-09-08 [1] CRAN (R 4.2.0)\n tidyselect             1.2.0    2022-10-10 [1] CRAN (R 4.2.0)\n tzdb                   0.3.0    2022-03-28 [1] CRAN (R 4.2.0)\n utf8                   1.2.2    2021-07-24 [1] CRAN (R 4.2.0)\n uwot                   0.1.14   2022-08-22 [1] CRAN (R 4.2.0)\n vctrs                  0.5.1    2022-11-16 [1] CRAN (R 4.2.0)\n vipor                  0.4.5    2017-03-22 [1] CRAN (R 4.2.0)\n viridis                0.6.2    2021-10-13 [1] CRAN (R 4.2.0)\n viridisLite            0.4.1    2022-08-22 [1] CRAN (R 4.2.0)\n withr                  2.5.0    2022-03-03 [1] CRAN (R 4.2.0)\n xfun                   0.36     2022-12-21 [1] RSPM (R 4.2.2)\n XVector                0.38.0   2022-11-01 [1] Bioconductor\n yaml                   2.3.6    2022-10-18 [1] CRAN (R 4.2.0)\n zlibbioc               1.44.0   2022-11-01 [1] Bioconductor\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/lemur/index.html#footnotes",
    "href": "posts/lemur/index.html#footnotes",
    "title": "Lemur: analyzing multi-condition single-cell data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nscRNA-seq data from glioblastoma slices cultured in vitro with either pamobinostat or a vehicle control, characterized in a terrific paper by Zhao et al, 2021↩︎\nCD45 is a cell surface antigen that is expressed on most hematopoietic lineage cells, including microglia.↩︎"
  },
  {
    "objectID": "posts/s7/index.html",
    "href": "posts/s7/index.html",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "",
    "text": "This week I learned about R’s new S7 object oriented programming (OOP) system by experimenting with classes that represent the main metadata entities of a next-generation sequencing experiment.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/s7/index.html#tldr",
    "href": "posts/s7/index.html#tldr",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "",
    "text": "This week I learned about R’s new S7 object oriented programming (OOP) system by experimenting with classes that represent the main metadata entities of a next-generation sequencing experiment."
  },
  {
    "objectID": "posts/s7/index.html#introduction",
    "href": "posts/s7/index.html#introduction",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Introduction",
    "text": "Introduction\nThis week, version 0.1.0 of the S7 R package was released on CRAN. Designed by a consortium spanning major R user communities it is designed to combine the simplicity of the S3 object oriented programming system with the control offered by S4. The authors have provided several vignettes, a great starting point for my first steps with S7."
  },
  {
    "objectID": "posts/s7/index.html#next-generation-sequencing-experiments",
    "href": "posts/s7/index.html#next-generation-sequencing-experiments",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Next-generation sequencing experiments",
    "text": "Next-generation sequencing experiments\nNext generation sequencing data is publicly available from multiple repositories, including The European Nucleotide Archive (ENA). Each dataset contains information about the overall study, the analyzed samples, the sequencing experiment (e.g. libraries) and the individual runs that generated the raw reads 1.\n\n\n\nENA metadata model\n\n\nUnderstanding the relationships between these entities is critical for interpreting and (re-)using the sequencing data. In short:\n\nOne or more runs are part of an experiment.\nOne or more experiments are part of a study.\nOne or more experiments are associated with a sample.\n\nLet’s represent these entities - and their relationships - as a set of S7 classes!\n\nlibrary(S7)\nlibrary(checkmate)"
  },
  {
    "objectID": "posts/s7/index.html#defining-s7-classes",
    "href": "posts/s7/index.html#defining-s7-classes",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Defining S7 classes",
    "text": "Defining S7 classes\nAn S7 class is defined with the new_class function, which defines its properties (e.g. corresponding to the slots of an S4 class).\nFirst, we define a basic Entity class with properties that are shared across all entities in our model. Specifically, we define the accession and name properties, each as a scalar character.\n\nEntity &lt;- new_class(\n  \"Entity\",\n  properties = list(\n    accession = class_character,\n    name = class_character\n  )\n)\n\nBy convention, the output of the new_class calls is assigned to an object with the same name.\n\nThe Run class\nLet’s start by defining the Run class as a container for metadata about each sequencing run. We also specify a validation function to ensure that the read_length is a positive integers (if it is provided).\n\nRun &lt;- new_class(\n  \"Run\",\n  parent = Entity,\n  properties = list(\n    paired_end = new_property(class_logical, default = FALSE),\n    read_length = class_integer,\n    qc_pass = class_logical\n  ),\n  validator = function(self) {\n    if (length(self@name) == 0) {\n      \"@name must be set\"\n    } else if (length(self@read_length) &gt; 0 && \n               !checkmate::test_count(self@read_length)) {\n      \"@read_length must a positive integer\"\n    }\n  }\n)\n\nCalling the class name (without parentheses) provides basic information about its definition:\n\nRun\n\n&lt;S7_class&gt;\n@ name  :  Run\n@ parent: &lt;Entity&gt;\n@ properties:\n $ accession  : &lt;character&gt;\n $ name       : &lt;character&gt;\n $ paired_end : &lt;logical&gt;  \n $ read_length: &lt;integer&gt;  \n $ qc_pass    : &lt;logical&gt;  \n\n\nCalling the constructor method (with parentheses) creates a new instance of the class, and the default print method lists its properties.\n\nfirst_run &lt;- Run(name = \"first run\", accession = \"r123\", read_length = 100L,\n                 qc_pass = TRUE)\nfirst_run\n\n&lt;Run&gt;\n @ accession  : chr \"r123\"\n @ name       : chr \"first run\"\n @ paired_end : logi FALSE\n @ read_length: int 100\n @ qc_pass    : logi TRUE\n\n\nGetter and setter methods are defined automatically and can be accessed with the @ operator:\n\nfirst_run@paired_end &lt;- TRUE\nfirst_run\n\n&lt;Run&gt;\n @ accession  : chr \"r123\"\n @ name       : chr \"first run\"\n @ paired_end : logi TRUE\n @ read_length: int 100\n @ qc_pass    : logi TRUE\n\n\nLet’s create a second run, this time with paired-end reads:\n\nsecond_run &lt;- Run(name = \"second run\", accession = \"r234\", paired_end = TRUE, \n                  qc_pass = FALSE)\nsecond_run\n\n&lt;Run&gt;\n @ accession  : chr \"r234\"\n @ name       : chr \"second run\"\n @ paired_end : logi TRUE\n @ read_length: int(0) \n @ qc_pass    : logi FALSE\n\n\n\n\nThe Sample class\nNext up, a class to hold metadata about each biological sample.\n\nSample &lt;- new_class(\n  \"Sample\",\n  parent = Entity,\n  properties = list(\n    description = class_character,\n    species  = new_property(class_character, default = \"Homo sapiens\"),\n    attributes = class_list,\n    external_ids = class_list\n  )\n)\n\n\n\nThe Experiment class\nLet’s define a container for Experiment metadata in a similar way. In addition to scalar properties, an Experiment also accepts a list of one or more Run objects, e.g. the sequencing runs associated with this experiment. Finally, we add the sample property to represent the biological sample of origin.\n\nExperiment &lt;- new_class(\n  \"Experiment\",\n  parent = Entity,\n  properties = list(\n    library_strategy = class_character,\n    library_selection = class_character,\n    library_source = class_character,\n    platform = class_character,\n    model = class_character,\n    runs = class_list,\n    sample = Sample\n  ),\n  validator = function(self) {\n    if (length(self@name) == 0) {\n      \"@name must be set\"\n    } else if (length(self@sample) == 0) {\n      \"@sample is required\"\n    } else if (length(self@sample@name) == 0) {\n      \"@sample@name is required\"\n     } else if (!checkmate::test_list(self@runs, types = \"Run\")) {\n      \"@runs must only contain instances of class `Run`\"\n    }\n  }\n)\n\nThe runs = class_list property definition does not specify the types of elements the list can contain, so we add a custom validator that requires all elements of runs to be instances of our previously defined Run S7 class.2\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe could also have defined an additional calls, e.g. RunList, and delegate validating that only Run objects are included. That would keep the code more modular, especially if the list of validation rules for the Experiment class increases. But it would also come at the cost of increasing the number of classes and the complexity of the code base.\n\n\n\nWe instantiate our first experiment and include our two Run objects:\n\nfirst_sample &lt;- Sample(name = \"first sample\")\nfirst_exp &lt;- Experiment(\n  name = \"first experiment\",\n  accession = \"exp567\",\n  platform = \"Illumina\",\n  model = \"NovaSeq 6000\",\n  runs = list(first_run, second_run),\n  sample = first_sample\n)\nfirst_exp\n\n&lt;Experiment&gt;\n @ accession        : chr \"exp567\"\n @ name             : chr \"first experiment\"\n @ library_strategy : chr(0) \n @ library_selection: chr(0) \n @ library_source   : chr(0) \n @ platform         : chr \"Illumina\"\n @ model            : chr \"NovaSeq 6000\"\n @ runs             :List of 2\n .. $ : &lt;Run&gt;\n ..  ..@ accession  : chr \"r123\"\n ..  ..@ name       : chr \"first run\"\n ..  ..@ paired_end : logi TRUE\n ..  ..@ read_length: int 100\n ..  ..@ qc_pass    : logi TRUE\n .. $ : &lt;Run&gt;\n ..  ..@ accession  : chr \"r234\"\n ..  ..@ name       : chr \"second run\"\n ..  ..@ paired_end : logi TRUE\n ..  ..@ read_length: int(0) \n ..  ..@ qc_pass    : logi FALSE\n @ sample           : &lt;Sample&gt;\n .. @ accession   : chr(0) \n .. @ name        : chr \"first sample\"\n .. @ description : chr(0) \n .. @ species     : chr \"Homo sapiens\"\n .. @ attributes  : list()\n .. @ external_ids: list()"
  },
  {
    "objectID": "posts/s7/index.html#custom-print-methods",
    "href": "posts/s7/index.html#custom-print-methods",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Custom print methods",
    "text": "Custom print methods\nAt this point, the output default print method for our class becomes a little unwieldy, as displaying information about all properties of the Experiment, its Sample and each of the associated Runs returns a lot of information. Let’s define a custom print method for the Run class that shows a concise summary instead.\n\n.run_message &lt;- function(x) {\n  sprintf(\n    \"%s Run%s%s %s\", \n    ifelse(x@paired_end, \"Paired-end\", \"Single-end\"),\n    ifelse(length(x@name) &gt; 0, paste0(\" '\", x@name, \"'\"),\"\"),\n    ifelse(length(x@accession) &gt; 0, paste0(\" (\", x@accession, \")\"), \"\"),\n    ifelse(length(x@qc_pass) == 0, \"❓\", ifelse(x@qc_pass, \"✅\",\"❌\")\n    )\n  )\n}\n\nmethod(print, Run) &lt;- function(x) {\n  cat(.run_message(x))\n}\nfirst_run\n\nPaired-end Run 'first run' (r123) ✅\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nDynamically choosing between the three symbols with nested ifelse statements is confusing - the dplyr::chase_when() function offers a nicer interface.\n\n\n\nGreat, that’s much shorter. Now we need a print method for the Experiment class as well:\n\n.experiment_msg &lt;- function(x) {\n  sprintf(\n    \"Experiment%s%s with %s runs.\", \n    ifelse(length(x@name) &gt; 0, paste0(\" '\", x@name, \"'\"),\"\"),\n    ifelse(length(x@accession) &gt; 0, paste0(\" (\", x@accession, \")\"), \"\"),\n    length(x@runs)\n  )\n}\n\nmethod(print, Experiment) &lt;- function(x) {\n  msg &lt;- .experiment_msg(x)\n  for (run in x@runs) {\n    msg &lt;- paste0(msg, \"\\n- \", .run_message(run))\n  }\n  cat(msg)\n}\n\nNow our first experiment is summarized as\n\nfirst_exp\n\nExperiment 'first experiment' (exp567) with 2 runs.\n- Paired-end Run 'first run' (r123) ✅\n- Paired-end Run 'second run' (r234) ❌\n\n\nLet’s create a second experiment before we move on:\n\nsecond_sample &lt;- Sample(name = \"second sample\")\nsecond_exp &lt;- Experiment(\n  name = \"second experiment\",\n  accession = \"exp628\",\n  platform = \"Illumina\",\n  model = \"NovaSeq 6000\",\n  runs = list(first_run),\n  sample = second_sample\n)"
  },
  {
    "objectID": "posts/s7/index.html#the-study-class",
    "href": "posts/s7/index.html#the-study-class",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "The Study class",
    "text": "The Study class\nNow that we have defined classes for all of the entities that constitute an NGS study, we finish by assembling them into a Study class. As before, we also define a custom print method to summarize it.\n\nStudy &lt;- new_class(\n  \"Study\",\n  parent = Entity,\n  properties = list(\n  primary_id = class_character,\n  secondary_id = class_character,\n  description = class_character,\n  abstract = class_character,\n  timestamp = new_property(class_POSIXct, default = as.POSIXct(Sys.time())),\n  experiments = class_list\n  ),\n  validator = function(self) {\n    if (length(self@name) == 0) {\n      \"@name must be set\"\n    } else if (!checkmate::test_list(self@experiments, types = \"Experiment\")) {\n      \"@experiments must only contain instances of class `Experiment`\"\n    }\n  }\n)\n\n.study_msg &lt;- function(x) {\n  sprintf(\n    \"Study%s%s with %s Experiment%s\", \n    ifelse(length(x@name) &gt; 0, paste0(\" '\", x@name, \"'\"),\"\"),\n    ifelse(length(x@accession) &gt; 0, paste0(\" (\", x@accession, \")\"), \"\"),\n    length(x@experiments),\n    ifelse(length(x@experiments) &gt; 1, \"s\", \"\")\n  )\n}\n\nmethod(print, Study) &lt;- function(x) {\n  msg &lt;- .studyt_msg(x)\n  for (experiment in x@experiments) {\n    msg &lt;- paste0(msg, \"\\n- \", .experiment_msg(experiment))\n  }\n  cat(msg)\n}\n\n\nfirst_study &lt;- Study(name = \"first study\", \n                     experiments = list(first_exp, second_exp))\nfirst_study@experiments\n\n[[1]]\nExperiment 'first experiment' (exp567) with 2 runs.\n- Paired-end Run 'first run' (r123) ✅\n- Paired-end Run 'second run' (r234) ❌\n[[2]]\nExperiment 'second experiment' (exp628) with 1 runs.\n- Paired-end Run 'first run' (r123) ✅"
  },
  {
    "objectID": "posts/s7/index.html#dynamic-methods",
    "href": "posts/s7/index.html#dynamic-methods",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Dynamic methods",
    "text": "Dynamic methods\nEach study contains a nested set of Experiment, Sample and Run objects. To access their properties, we redefine the Study class by adding computed properties, e.g. a runs property that will return a list of Runs from all experiments:\n\nStudy &lt;- new_class(\n  \"Study\",\n  parent = Entity,\n  properties = list(\n  primary_id = class_character,\n  secondary_id = class_character,\n  description = class_character,\n  abstract = class_character,\n  timestamp = new_property(class_POSIXct, default = as.POSIXct(Sys.time())),\n  experiments = class_list,\n  runs = new_property(\n    getter = function(self) {\n      exp_runs &lt;- setNames(\n        lapply(self@experiments, \\(x) x@runs),\n        vapply(self@experiments, \\(x) x@name, FUN.VALUE = character(1)))\n    }\n  )\n  ),\n  validator = function(self) {\n    if (length(self@name) == 0) {\n      \"@name must be set\"\n    } else if (!checkmate::test_list(self@experiments, types = \"Experiment\")) {\n      \"@experiments must only contain instances of class `Experiment`\"\n    }\n  }\n)\n\nNow, the study’s @runs property returns a nested list with one element for each experiment (and all runs for each experiment).\n\nsecond_study &lt;- Study(name = \"first study\", \n                     experiments = list(first_exp, second_exp))\nexp_runs &lt;- second_study@runs\nstr(exp_runs)\n\nList of 2\n $ first experiment :List of 2\n  ..$ : &lt;Run&gt;\n  .. ..@ accession  : chr \"r123\"\n  .. ..@ name       : chr \"first run\"\n  .. ..@ paired_end : logi TRUE\n  .. ..@ read_length: int 100\n  .. ..@ qc_pass    : logi TRUE\n  ..$ : &lt;Run&gt;\n  .. ..@ accession  : chr \"r234\"\n  .. ..@ name       : chr \"second run\"\n  .. ..@ paired_end : logi TRUE\n  .. ..@ read_length: int(0) \n  .. ..@ qc_pass    : logi FALSE\n $ second experiment:List of 1\n  ..$ : &lt;Run&gt;\n  .. ..@ accession  : chr \"r123\"\n  .. ..@ name       : chr \"first run\"\n  .. ..@ paired_end : logi TRUE\n  .. ..@ read_length: int 100\n  .. ..@ qc_pass    : logi TRUE"
  },
  {
    "objectID": "posts/s7/index.html#conclusions",
    "href": "posts/s7/index.html#conclusions",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Conclusions",
    "text": "Conclusions\nAfter working with the S4 OOP for a while, the new S7 package was a pleasant surprise. There was less boilerplate code to write, and the definition of classes and methods seems straightforward. I noticed a few issues, which had also been noted by other R users:\n\nAs pointed out by Kevin Kunzmann in this github issue the ability to define additional constraints for properties within e.g.  class_character would be a welcome addition. I found myself writing custom validators to check whether a property had been set (e.g. length(x@name) &gt; 0).\nThe Entity class defines the @name and @accession properties, along with a validator for the former. Unfortunately, that validator is not inherited by the child classes, as noted by Jamie Lendrum in this github issue, so I had to repeat the same code in each class definition."
  },
  {
    "objectID": "posts/s7/index.html#reproducibility",
    "href": "posts/s7/index.html#reproducibility",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSession Information\n\n\nsessioninfo::session_info(\"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package   * version date (UTC) lib source\n P checkmate * 2.2.0   2023-04-27 [?] CRAN (R 4.3.0)\n P S7        * 0.1.0   2023-08-24 [?] CRAN (R 4.3.0)\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/s7/index.html#footnotes",
    "href": "posts/s7/index.html#footnotes",
    "title": "Organizing sequencing metadata: experimenting with S7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFigure source: ENA documentation↩︎\nI am a big fan of the checkmate R package, which provides numerous useful functions to assert / check / test objects.↩︎"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "",
    "text": "Note\n\n\n\n\n\nThis is the first of four posts documenting my progress toward processing and analyzing QuantSeq FWD 3’ tag RNAseq data with the nf-core/rnaseq workflow.\n\nConfiguring the nf-core/rnaseq workflow\nExploring the workflow outputs\nValidating the workflow by reproducing results published by Xia et al (no UMIs)\nValidating the workflow by reproducing results published by Nugent et al (including UMIs)\n\nMany thanks to Harshil Patel, António Miguel de Jesus Domingues and Matthias Zepper for their generous guidance & input via nf-core slack. (Any mistakes are mine.)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#tldr",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#tldr",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "tl;dr",
    "text": "tl;dr\n\nThis tutorial documents how to configure & execute the nf-core/rnaseq workflow for the processing of raw QuantSeq 3’ FWD RNA-seq data.\nIt highlights custom nf-core/rnaseq parameters settings and applies them to two published datasets, one with and one without UMIs."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#installing-nextflow",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#installing-nextflow",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Installing Nextflow",
    "text": "Installing Nextflow\nTo run nextflow, we create a new conda environment. (See conda installation instructions if you are not yet familiar with the conda package manager.) Then we install nextflow and its dependencies from the bioconda repository.\n&gt; conda create -n nf\n&gt; conda install -c bioconda nextflow\nOnce the installation is complete, we activate our new conda environment and verify that Nextflow is installed:\n&gt; conda activate nf\n&gt; nextflow -v\nnextflow version 22.10.4.5836\n\nNextflow configs\nNextflow separates the definition of a workflow from the platform it is executed on. For example, you can run the same workflow on your local computer, a high-performance cluster or using a cloud provider like Amazon Web Services (AWS) or Google Cloud Services (GCS).\nEach nf-core workflow defines the required software sources, which can either be installed (e.g. via conda) or be made available through a container engine such as docker, singularity, etc. For details, please consult the Nextflow documentation.\nThe desired configuration is specified in one or more configuration file(s), and passed to the nextflow command via its -profile argument. Here, we execute the workflows on the local system, using docker containers by specifying the built-in -profile docker configuration."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#analysis-of-published-studies",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#analysis-of-published-studies",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Analysis of published studies",
    "text": "Analysis of published studies\nTo validate the nf-core/rnaseq workflow for QuantSeq 3’ tag RNA-seq data, we reanalyze data from two published studies, one with and one without the use of unique molecular identifiers (UMIs).\n\n“Fibrillar Aβ causes profound microglial metabolic perturbations in a novel APP knock-in mouse model” by Xia et al, 2021\n\n\nQuantSeq 3’ mRNA-Seq Library Prep Kit FWD FWD\nThis data does not include UMIs.\nRaw data are available via\n\nGEO accession GSE158152\nSRA accession SRP213880\n\n\n\n“TREM2 regulates microglial lipid metabolism during aging in mice” by Nugent et al, 2020.\n\n\nQuantSeq 3’ mRNA-Seq Library Prep Kit FWD FWD\nThis data includes UMIs, which and can be used to filter alignments originating from PCR duplicates.\nRaw data are available via\n\nGEO accession GSE134031\nSRA accession SRP213880"
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-without-umis-xia-et-al-dataset",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-without-umis-xia-et-al-dataset",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Quantseq FWD without UMIs: Xia et al dataset",
    "text": "Quantseq FWD without UMIs: Xia et al dataset\nHere, we will reanalyze RNA-seq data published in the preprint “Fibrillar Aβ causes profound microglial metabolic perturbations in a novel APP knock-in mouse model” by Xia et al, 2021.\n\nExperimental design\nThis study examines FACS-isolated microglia from 18 mice from three genetic backgrounds (N = 6 WT, N =6 heterozygous App-SAA and N = 6 homozygous App-SAA animals). Microglia were isolated from the brain (cortex & hippocampus) of each animal using FACS and gene expression changes were analyzed using 3-tag RNA-seq.\nFrom each mouse, two (technical) replicate pools of microglia were collected and processed into separate libraries with Lexogen’s QuantSeq FWD. This dataset does not include unique molecular identifiers (UMIs).\n\n\nRaw data retrieval with nf-core/fetchngs\nWe start the analysis by downloading the raw data (36 FASTQ files) and the sample metadata with the nf-core/fetchngs workflow. For datasets stored in SRA, we only need to paste the SRA Study identifier (SRP282921) into a text file for us as the input for the fetchngs workflow. The following command will download all of the FASTQ files associated with this study into the raw_data directory:\n&gt; mkdir SRP282921\n&gt; cd SRP282921\n&gt; echo SRP282921 &gt; ids.txt\n&gt; nextflow run nf-core/fetchngs \\\n      -revision 1.9 \\\n      -profile docker \\\n      --outdir raw_data \\\n      --input ids.txt \\\n      --nf_core_pipeline rnaseq \\\n      --nf_core_rnaseq_strandedness forward\nBy specifying the --nf_core_pipeline argument, the workflow creates a sample sheet that can be used as input for the nf-core/rnaseq pipeline. Because the data was generated with the QuantSeq FWD kit, we also know that the reads are from the forward strand and add this information to the sample sheet via the --nf_core_rnaseq_strandedness argument. 1\nOnce the workflow has completed, the raw_data output directory contains the following structure:\n&gt; tree raw_data\nraw_data/\n├── custom\n│   └── user-settings.mkfg\n├── fastq\n│   ├── md5\n│   │   ├── SRX9142647_SRR12661938.fastq.gz.md5\n│   │   ├── SRX9142648_SRR12661924.fastq.gz.md5\n│   │   ├── 34 additional .fastq.gz.md5 files (not shown)\n│   ├── SRX9142647_SRR12661938.fastq.gz\n│   ├── SRX9142648_SRR12661924.fastq.gz\n│   ├── 34 additional .fastq.gz files (not shown)\n├── metadata\n│   └── SRP282921.runinfo_ftp.tsv\n├── pipeline_info\n│   ├── execution_report_2023-01-12_01-49-02.html\n│   ├── execution_timeline_2023-01-12_01-49-02.html\n│   ├── execution_trace_2023-01-12_01-49-02.txt\n│   ├── pipeline_dag_2023-01-12_01-49-02.html\n│   └── software_versions.yml\n└── samplesheet\n    ├── id_mappings.csv\n    ├── multiqc_config.yml\n    ├── nf_params.json\n    ├── run.sh\n    └── samplesheet.csv\n6 directories, 90 files\n\nThe 36 FASTQ files are in the fastq subfolder, each accompanied by a MD5 checksum file in the md5 directory.\nIn the samplesheet subdirectory, we find the samplesheet.csv file that we can pass to the nf-core/rnaseq workflow.\n\nFor more details about the output oft he workflow, please check the next post in this series] and the nf-core/rnaseq output docs.\n\n\nConfiguring the nf-core/rnaseq workflow\n\nReference data\nAccording to the sample metadata in NCBI GEO the authors mapped the data to the mouse GRCm38 genome version, using Gencode annotations from release M17. To later compare our results with those obtained in the original publication, we use the same Gencode version (even though it is not the most recent).\nThe nf-core/rnaseq workflow can automatically generate all necessary indices when provided with\n\nA FASTA file with the genomic sequences, e.g. Gencode’s Primary genome assembly and\nA GFP file with matching gene annotations, e.g.  Gencode Comprehensive gene annotations\n\nBecause downloading and indexing the genome is time consuming, we will save them for later use.\n\n\nCustom arguments\nThe nf-core/rnaseq pipeline has robust default parameters that are suitable for whole transcriptome RNA-seq data. But this dataset was generated with Lexogen’s QuantSeq FWD 3’ mRNA library preparation kit, which specificially targets the 3’ end of polyadenylated transcripts. As a consequence, reads align primarily to the 3’ UTR and the final exon and coverage of the remainder of the gene body is minimal.\nTo account for these properties, we supply a number of custom arguments to the nextflow run command, which augment the workflow for this specific data type.\n\n\nExtra STAR arguments\nLexogen provides an example analysis workflow on their website, which uses the STAR aligner with several non-default parameters. Most of them parameters correspond to the ENCODE standard options listed in the STAR manual:\n\n--alignIntronMax 1000000\n--alignIntronMin 20\n--alignMatesGapMax 1000000\n--alignSJoverhangMin 8\n--alignSJDBoverhangMin 1\n--outFilterMismatchNmax 999\n--outFilterMultimapNmax 20\n--outFilterType BySJout\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nThe ENCODE project originally focussed on data from human and mouse. The parameters shown above are suitable for these genomes or those with comparable characteristics. For analysis of data from other organisms, e.g. those with shorter introns, you might need to modify them.\n\n\n\nIn addition, Lexogen also specified the\n\n--outFilterMismatchNoverLmax 0.1, decreasing the tolerance for mismatches.\n\nLexogen’s example workflow uses bbduk to trim adapters, poly(A) tails and low quality bases from the 3’ end of reads. The nf-core/rnaseq workflow uses Trim Galore to trim adapters, but does not remove poly(A) tails. Instead we use the STAR ligner to clip poly(A) tails during the alignment stage:\n\n--clip3pAdapterSeq AAAAAAAA\n\nWe will pass all of the arguments listed above to STAR via nf-core/rnaseq’s --extra_star_align_args argument (as a string, see example below).\n\n\nExtra Salmon arguments\nThe salmon algorithm takes into account transcript length when quantifying gene expression. Because QuantSeq (and other 3’ tag sequencing methods) only capture the 3’ most part of each transcript, not its full length, this feature needs to be deactivated:\n\n--noLengthCorrection\n\nThe nf-core/rnaseq workflow features a special argument, extra_salmon_quant_args to pass additional arguments to the salmon tool (see syntax below).\n\n\n\nStarting the workflow\nWe can pass the parameters we discussed above as individual arguments to the nextflow run command (prefixed with --). Alternatively, we can collect them in a JSON file and specify its name via the -params-file argument.\nFor example, the following JSON string specifies the parameters for analyzing sQuantSeq FWD data without UMIs:\n{\n    \"save_reference\": true,\n    \"fasta\": \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M17/GRCm38.primary_assembly.genome.fa.gz\",\n    \"gtf\": \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M17/gencode.vM17.primary_assembly.annotation.gtf.gz\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": false,\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nWe store the JSON string in a file called nf_params.json, and then pass it to the nextflow run command 2:\n&gt; nextflow run \\\n      nf-core/rnaseq \\\n      -r 3.10.1 \\\n      -profile docker \\\n      -params-file nf_params.json \\\n      -resume \\\n      --input raw_data/samplesheet/samplesheet.csv \\\n      --outdir SRP282921\n\n\nExamining the output files\n\n\nReusing reference files and indices\nRetrieving the reference files and indexing them is time consuming. If you are planning to map additional sample against the same genome / transcriptome in the future, you might want to reuse them. Because we set the save_reference argument to true in our workflow, the output directory contains a genome folder, featuring reference files (e.g. genome and transcriptome sequences in FASTQ formats) and indices (e.g. for STAR, salmon and rsem).\nLet’s move the genome folder to our current working directory (or any other suitable location). 3 For future runs, we can include paths to the files and indices in our nf_params.json file, e.g.\n{\n    \"save_reference\": false,\n    \"fasta\": \"genome/GRCm38.primary_assembly.genome.fa\",\n    \"gtf\": \"genome/gencode.vM17.primary_assembly.annotation.gtf\",\n    \"gene_bed\": \"genome/gencode.vM17.primary_assembly.annotation.bed\",\n    \"transcript_fasta\": \"genome/genome.transcripts.fa\",\n    \"star_index\": \"genome/index/star\",\n    \"salmon_index\": \"genome/index/salmon\",\n    \"rsem_index\": \"genome/rsem\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": false,\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nOur next run will execute much faster, because these reference files and indices are already available and don’t need to be created from scratch.\n\n\n\n\n\n\nNote\n\n\n\nIf you chose a location other than genome to store your reference data, please specify the (absolute) path in the JSON file instead."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-with-umis-nugent-et-al-dataset",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#quantseq-fwd-with-umis-nugent-et-al-dataset",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Quantseq FWD with UMIs: Nugent et al dataset",
    "text": "Quantseq FWD with UMIs: Nugent et al dataset\nEspecially when only low amounts of input material (total RNA) are available, the QuantSeq FWD procotol requires multiple PCR amplification steps. Each step generates clones of the originally captured tags, but does not provide new information about transcript abundance. Inclusion of unique molecular identifiers (UMIs) in the second strand synthesis guarantees that PCR duplicates can be identified and removed in the analysis (based on both the coordinates of their alignment and the UMI sequence).\nNugent et al used the QuantSeq FWD protocol with UMIs, and we will reanalyze this dataset to illustrate how to instruct UMI-tools to extract the UMIs from the raw reads and deduplicated the STAR alignments.\n\nExperimental design\nThis study examines FACS-isolated astroctyes and microglia from female mice that were either 2- or 16 months of age. The animals are either wildtype (WT) or\nhomozygous knockouts (KO) for the Trem2 gene.\nFrom each mouse, separate microglia and astrocyte samples were collected and processed into separate libraries with Lexogen’s QuantSeq FWD kit and the QuantSeq UMI add-on module.\n\n\nRaw data retrieval with nf-core/fetchngs\nAs in the first example, we use the nf-core/fetchngs workflow to retrieve the FASTQ files from SRA.\n&gt; mkdir SRP213880\n&gt; cd SRP213880\n&gt; echo SRP213880 &gt; ids.txt\n&gt; nextflow run nf-core/fetchngs \\\n      -revision 1.9 \\\n      -profile docker \\\n      --outdir raw_data \\\n      --input ids.txt \\\n      --nf_core_pipeline rnaseq \\\n      --nf_core_rnaseq_strandedness forward\n\n\nConfiguring the nf-core/rnaseq workflow\n\nReference data\nAccording to the sample metadata in NCBI GEO the authors used the same reference data as Xia et al, Gencode release M17. We can therefore reuse the reference files and indices we generated in the first analysis (see above).\n\n\nUnique molecular identifiers\nTo process QuantSeq data with UMIs, the following parameters need to be provided.\n\n--with_umi\n--umitools_extract_method \"regex\": The type of pattern to detect, either string (default) or regex\n--umitools_bc_pattern \"^(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*\": A regular expression that instructs UMI-tools to extract the first 6 bases (the UMI) from the 5’ end of the read 4 and to discard the following 4 bases. (Lexogen adds an invariant TATA sequence motif to each UMI.)\n--umitools_grouping_method \"unique\": The method used to group similar UMIs, e.g. to correct sequencing errors. The default method is directional, but it is computationally expensive. Here, we use the simpler unique method, suppressing grouping / error correction entirely.\n\n\n\n\n\n\n\nNote\n\n\n\nThese arguments shown above are suitable for UMIs offered by Lexogen for the QuantSeq protocol. If you use custom UMIs, please consult the nf-core/rnaseq documentation for full details and additional options for details on additional parameters you can set.\n\n\n\n\n\nStarting the workflow\nNext, we add the UMI-related parameters to the JSON file we used above, so we can pass them to nextflow run via the -params-file argument. Because Nugent et al used the same reference (Gencode M17), we can reuse the same reference we generated for Xia et al’s dataset. (Here, we assume the precomputed references are in the genome folder to our current working directory. If you chose to store them elsewhere, please provide absolute paths in the JSON file instead.)\n{\n    \"save_reference\": false,\n    \"fasta\": \"genome/GRCm38.primary_assembly.genome.fa\",\n    \"gtf\": \"genome/gencode.vM17.primary_assembly.annotation.gtf\",\n    \"gene_bed\": \"genome/gencode.vM17.primary_assembly.annotation.bed\",\n    \"transcript_fasta\": \"genome/genome.transcripts.fa\",\n    \"star_index\": \"genome/index/star\",\n    \"salmon_index\": \"genome/index/salmon\",\n    \"rsem_index\": \"genome/rsem\",\n    \"extra_star_align_args\": \"--alignIntronMax 1000000 --alignIntronMin 20 --alignMatesGapMax 1000000 --alignSJoverhangMin 8 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 --outFilterType BySJout --outFilterMismatchNoverLmax 0.1 --clip3pAdapterSeq AAAAAAAA\",\n    \"with_umi\": true,\n    \"umitools_extract_method\": \"regex\",\n    \"umitools_grouping_method\": \"unique\",\n    \"umitools_bc_pattern\": \"^(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*\",\n    \"extra_salmon_quant_args\": \"--noLengthCorrection\",\n    \"skip_stringtie\": true,\n    \"gencode\": true\n}\nWe store the JSON string in a file called nf_params.json, and then pass it to the nextflow run command:\n&gt; nextflow run \\\n      nf-core/rnaseq \\\n      -r 3.10.1 \\\n      -profile docker \\\n      -params-file nf_params.json \\\n      -resume \\\n      --input raw_data/samplesheet/samplesheet.csv \\\n      --outdir SRP213880\nUpon successful completion, the workflow’s output is available in the SRP213880 directory.\nNext, we will take a closer look at the available results in the second post in this series]."
  },
  {
    "objectID": "posts/nextflow-core-quantseq-1-settings/index.html#footnotes",
    "href": "posts/nextflow-core-quantseq-1-settings/index.html#footnotes",
    "title": "QuantSeq RNAseq analysis (1): configuring the nf-core/rnaseq workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe strandedness column in the sample sheet CSV file must contain one of forward, reverse, unstranded or auto. The latter will prompt the nf-core/rnaseq workflow to subsample reads and map them to the transcriptome with salmon to determine the likely orientation of the reads. Because this adds additional steps to the workflow execution, it is recommended that you specify the strandedness of your library explicitly whenever possible.↩︎\nThe -resume argument allows you to resume previous executions of the same workflow. Nextflow will reuse any existing outputs and generate only those that are missing.↩︎\nNextflow can interact with different storage backends. For example, you could provide URLs of remote files (e.g. with the ftp:// prefix) or in cloud storage (e.g. with the s3:// or gs:// prefix) and they will automatically be downloaded and staged when you start the run.↩︎\nTypically, QuantSeq libraries are single-end (not paired-end) and the UMI is therefore found in the R1 read.↩︎"
  },
  {
    "objectID": "posts/aws-export-credentials/index.html",
    "href": "posts/aws-export-credentials/index.html",
    "title": "Refreshing & exporting temporary AWS credentials",
    "section": "",
    "text": "To increase security when interacting with AWS services, the AWS IAM Identity Center (formerly known as AWS SSO) generates temporary credentials for different AWS roles.\nToday I learned how to configure and refresh these credentials in the command line, as well how to export them either as environmental variables or write them to the credentials file where tools that do not interact with AWS SSO natively can access them.\n\nConfiguring an AWS SSO profile\nFirst, we need to configure a named profile for use with AWS SSO. The following AWS CLI version 2 command will interactively walk you through the necessary steps:\naws configure sso\nThe information you provide will be written to the config file, located in the ~/.aws directory on Mac OS. Here is an example:\n[profile my-dev-profile]\nsso_start_url = https://my-sso-portal.awsapps.com/start\nsso_region = us-east-1\nsso_account_id = 123456789011\nsso_role_name = readOnly\nregion = us-west-2\noutput = json\n\n\nLogging into the AWS SSO profile\nNow we can log into AWS SSO and request temporary credentials:\naws sso login --profile my-dev-profile\nThis command will try to open a web browser for you and prompt you to confirm the login. Alternatively, you can copy & paste the displayed URL and manually enter the confirmation code output by the command.\nIf the login was successful, you can now adopt the my-dev-profile when using the AWS CLI, e.g.\naws s3 ls --profile my-dev-profile\nThe AWS SSO endpoint recognizes many environmental variables that you can use to specify defaults, e.g.\n\nAWS_PROFILE: The profile to use (e.g. my-dev-profile)\nAWS_SHARED_CREDENTIALS_FILE: the location of the shared credentials files (default on Mac OS: ~/.aws/.credentials)\nAWS_CONFIG_FILE: the location of the AWS CLI configuration file (default on Mac OS: ~/.aws.config)\n\n\n\nAccessing temporary credentials\nThe AWS CLI and many of the AWS SKDs will automatically detect and use SSO credentials. But other tools might not (yet) be compatible with this authentication route. Instead, they might\n\nread credentials for a profile from the credentials file\nrely on environmental variables, e.g. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n\nTo expose the temporary credentials, Ben Kehoe has made the aws-export-credentials tool available.\n\n\nInstalling aws-export-credentials\nThe recommended way to install aws-export-credentials is via pipx because it will automatically make it available in your PATH.\n\nIf you don’t have pipx available on your system, install it first.\nNext, install aws-export-credentials by executing the following steps in your shell:\n\npipx ensurepath  # in case you haven't run this before\npipx install aws-export-credentials\naws-export-credentials --version  # verify the installation\n\n\nUpdating the credentials file\nAt the beginning of your workday - or whenever needed - run the following set of commands. (Replace the SSO profile with the one you want to adopt.)\nPROFILE=\"my-dev-profile\"\n\n# retrieve new credentials from AWS\naws sso login --profile \"${PROFILE}\"\n\n# write the temporary credentials to the ~/.aws/credentials file\naws-export-credentials \\\n  --profile \"${PROFILE}\" \\\n  --credentials-file-profile \"${PROFILE}\"\nThis will refresh the credentials (via aws sso login) and then write them to the my-dev-profile profiles in the ~/.aws/.credentials file. Now we can access them e.g. in the aws.s3 R package:\nlibrary(aws.s3)\nlibrary(aws.signature)\naws.signature::use_credentials(profile = \"my-dev-profile\")\naws.s3::bucketlist()\n\n\nExposing environmental variables\nSome tools only recognize environmental variables. Luckily, aws-export-credentials can automate this process, too:\nexport $(aws-export-credentials --profile my-dev-profile --env-export)\nwill export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables in your shell session.\n\n\nSourcing credentials with an external process\nFinally, you can also include a command that looks up credentials as a credential_process in your config file. (More information here) But that’s not a use case I have explored, yet.\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-returning/index.html",
    "href": "posts/postgres-returning/index.html",
    "title": "Simultaneously inserting records into two tables with Postgres CTEs",
    "section": "",
    "text": "Today I learned how to\n\nUse Common Table Expressions (CTEs) to simultaneously insert data into two Postgres tables and\nUse the RETURNING SQL command to retrieve automatically created fields inside the same statement.\n\nGene expression data hosted at the NCBI’s Short Read Archive (SRA) or at the European Nucleotide Archive (ENA) are a great resource. Both repositories represent information for different entities that make up a project, e.g. study, sample, experiment, run and analysis information.\n\n\n\nRelationships between entities (source: ENA)\n\n\nFor example, ENA project PRJNA818657 is an RNA-seq study with data for 25 samples. For each sample, a single sequencing library (= experiment) was prepared and sequenced in two separate runs.\nIn other words, e.g.  sample SAMN26870486 produced experiment SRX14564817, which was then analyzed in run SRR18430942 and run SRR18430943.\nOne way to capture this information in a relational database is to set up three tables - one for each entity - and then use ENA’s unique sample-, experiment- and run-identifiers as natural primary keys.\nBut what if I don’t have suitable natural keys, or simply prefer to use surrogate keys?\nToday, I learned how to\n\nINSERT a new record into a Postgres database,\nautomatically generate a primary key,\nreturn the key and\ninclude it in a subsequent INSERT statement\n\n\n\nI am using a Postgres database called test, running on the local host and connect to it with the DBI R package, via the RPostgres::Postgres() driver.\nThen I pass the returned PqConnection object to the following SQL code cells in this Quarto document.\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  dbname = \"test\", \n  host = \"localhost\")\n\n\n\n\nInitially, the database is empty, so let’s create two tables:\n\nexperiment: sample-level information\nrun: run-level information\n\nEach table will include\n\nAn auto-generated primary key (experiment_id and run_id, respectively)\nA field to record the record’s ENA accession\nA time-stamp\n\nand the run table will reference its parent experiment via the experiment_id foreign key.\n\nCREATE TABLE IF NOT EXISTS experiment (\n  experiment_id SERIAL PRIMARY KEY,\n  accession text UNIQUE,\n  timestamp timestamp default current_timestamp not null\n)\n\n\nCREATE TABLE IF NOT EXISTS run (\n    run_id SERIAL PRIMARY KEY,\n    accession text UNIQUE,\n    timestamp timestamp default current_timestamp not null,\n    experiment_id integer,\n    FOREIGN KEY(experiment_id) REFERENCES experiment(experiment_id)\n)\n\n\n\n\nNext, we use a single SQL statement to insert both the experiment and its related runs:\n\nWITH\nexp AS (\n  INSERT INTO experiment (accession) \n  VALUES ('SRX14564817') \n  RETURNING experiment_id\n),\ndata(accession) AS (\n  VALUES\n  ('SRR18430942'),\n  ('SRR18430943')\n)\nINSERT INTO run (experiment_id, accession)\nSELECT e.experiment_id, d.accession\nFROM exp e, data d\n\nWe verify that the experiment has been accessioned into the experiment table, and the same identifier has then be inserted into the run table as well\n\nSELECT e.experiment_id, e.accession AS experiment_accession, \n       r.run_id, r.accession AS run_accession\nFROM experiment e\nINNER JOIN run r ON e.experiment_id = r.experiment_id\n\n\n2 records\n\n\nexperiment_id\nexperiment_accession\nrun_id\nrun_accession\n\n\n\n\n1\nSRX14564817\n1\nSRR18430942\n\n\n1\nSRX14564817\n2\nSRR18430943\n\n\n\n\n\nLet’s examine the individual parts of this query:\n\nThe WITH command creates a Common Table Expression (CTE), e.g. \n\na temporary named result set, derived from a simple query and defined within the execution scope of a SELECT, INSERT, UPDATE, or DELETE statement.\n\nIn this example, the exp temporary result is generated by the first INSERT statement, which updates the experiment table. It returns the automatically generated experiment_id via the RETURNING command. Let’s add another accession to the experiment table and examine the returned exp table:\n\n\n  WITH\n  exp AS (\n    INSERT INTO experiment (accession) \n    VALUES ('another accession') \n    RETURNING experiment_id\n  )\n  SELECT * FROM exp\n\n\n1 records\n\n\nexperiment_id\n\n\n\n\n2\n\n\n\n\n\nAs expected, the experiment_id has been incremented for the next experiment.\n\nNext, we provide the two run accessions by passing them as VALUES to the data table.\n\n\n  WITH\n  data(accession) AS (\n    VALUES\n    ('SRR18430942'),\n    ('SRR18430943')\n  )\n  SELECT * FROM data\n\n\n2 records\n\n\naccession\n\n\n\n\nSRR18430942\n\n\nSRR18430943\n\n\n\n\n\n\nFinally, the second INSERT statement adds the two runs to the run table, by retrieving the temporary values from both the exp and data result sets.\n\nBecause the CTE is a single SQL statement, it runs within a single transaction, e.g. it is committed only at the successful completion of the whole statement.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/postgres-returning/index.html#tldr",
    "href": "posts/postgres-returning/index.html#tldr",
    "title": "Simultaneously inserting records into two tables with Postgres CTEs",
    "section": "",
    "text": "Today I learned how to\n\nUse Common Table Expressions (CTEs) to simultaneously insert data into two Postgres tables and\nUse the RETURNING SQL command to retrieve automatically created fields inside the same statement.\n\nGene expression data hosted at the NCBI’s Short Read Archive (SRA) or at the European Nucleotide Archive (ENA) are a great resource. Both repositories represent information for different entities that make up a project, e.g. study, sample, experiment, run and analysis information.\n\n\n\nRelationships between entities (source: ENA)\n\n\nFor example, ENA project PRJNA818657 is an RNA-seq study with data for 25 samples. For each sample, a single sequencing library (= experiment) was prepared and sequenced in two separate runs.\nIn other words, e.g.  sample SAMN26870486 produced experiment SRX14564817, which was then analyzed in run SRR18430942 and run SRR18430943.\nOne way to capture this information in a relational database is to set up three tables - one for each entity - and then use ENA’s unique sample-, experiment- and run-identifiers as natural primary keys.\nBut what if I don’t have suitable natural keys, or simply prefer to use surrogate keys?\nToday, I learned how to\n\nINSERT a new record into a Postgres database,\nautomatically generate a primary key,\nreturn the key and\ninclude it in a subsequent INSERT statement\n\n\n\nI am using a Postgres database called test, running on the local host and connect to it with the DBI R package, via the RPostgres::Postgres() driver.\nThen I pass the returned PqConnection object to the following SQL code cells in this Quarto document.\n\nlibrary(DBI)\nlibrary(RPostgres)\n\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  dbname = \"test\", \n  host = \"localhost\")\n\n\n\n\nInitially, the database is empty, so let’s create two tables:\n\nexperiment: sample-level information\nrun: run-level information\n\nEach table will include\n\nAn auto-generated primary key (experiment_id and run_id, respectively)\nA field to record the record’s ENA accession\nA time-stamp\n\nand the run table will reference its parent experiment via the experiment_id foreign key.\n\nCREATE TABLE IF NOT EXISTS experiment (\n  experiment_id SERIAL PRIMARY KEY,\n  accession text UNIQUE,\n  timestamp timestamp default current_timestamp not null\n)\n\n\nCREATE TABLE IF NOT EXISTS run (\n    run_id SERIAL PRIMARY KEY,\n    accession text UNIQUE,\n    timestamp timestamp default current_timestamp not null,\n    experiment_id integer,\n    FOREIGN KEY(experiment_id) REFERENCES experiment(experiment_id)\n)\n\n\n\n\nNext, we use a single SQL statement to insert both the experiment and its related runs:\n\nWITH\nexp AS (\n  INSERT INTO experiment (accession) \n  VALUES ('SRX14564817') \n  RETURNING experiment_id\n),\ndata(accession) AS (\n  VALUES\n  ('SRR18430942'),\n  ('SRR18430943')\n)\nINSERT INTO run (experiment_id, accession)\nSELECT e.experiment_id, d.accession\nFROM exp e, data d\n\nWe verify that the experiment has been accessioned into the experiment table, and the same identifier has then be inserted into the run table as well\n\nSELECT e.experiment_id, e.accession AS experiment_accession, \n       r.run_id, r.accession AS run_accession\nFROM experiment e\nINNER JOIN run r ON e.experiment_id = r.experiment_id\n\n\n2 records\n\n\nexperiment_id\nexperiment_accession\nrun_id\nrun_accession\n\n\n\n\n1\nSRX14564817\n1\nSRR18430942\n\n\n1\nSRX14564817\n2\nSRR18430943\n\n\n\n\n\nLet’s examine the individual parts of this query:\n\nThe WITH command creates a Common Table Expression (CTE), e.g. \n\na temporary named result set, derived from a simple query and defined within the execution scope of a SELECT, INSERT, UPDATE, or DELETE statement.\n\nIn this example, the exp temporary result is generated by the first INSERT statement, which updates the experiment table. It returns the automatically generated experiment_id via the RETURNING command. Let’s add another accession to the experiment table and examine the returned exp table:\n\n\n  WITH\n  exp AS (\n    INSERT INTO experiment (accession) \n    VALUES ('another accession') \n    RETURNING experiment_id\n  )\n  SELECT * FROM exp\n\n\n1 records\n\n\nexperiment_id\n\n\n\n\n2\n\n\n\n\n\nAs expected, the experiment_id has been incremented for the next experiment.\n\nNext, we provide the two run accessions by passing them as VALUES to the data table.\n\n\n  WITH\n  data(accession) AS (\n    VALUES\n    ('SRR18430942'),\n    ('SRR18430943')\n  )\n  SELECT * FROM data\n\n\n2 records\n\n\naccession\n\n\n\n\nSRR18430942\n\n\nSRR18430943\n\n\n\n\n\n\nFinally, the second INSERT statement adds the two runs to the run table, by retrieving the temporary values from both the exp and data result sets.\n\nBecause the CTE is a single SQL statement, it runs within a single transaction, e.g. it is committed only at the successful completion of the whole statement."
  },
  {
    "objectID": "posts/drat/index.html",
    "href": "posts/drat/index.html",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "",
    "text": "Today I learned how to\n\nBuild an R package into source and binary bundles for distribution.\nCreate a local drat repository.\nAdd an R package to the repository and install it from there.\nHost the repository remotely in an AWS S3 bucket.\n\nMany thanks to Dirk Eddelbuettel for creating and documenting the drat R package! (As always, any mistakes are my own.)\n\n\nThere are multiple ways for developers to share R packages publicly, e.g.\n\nSubmit them to the The Comprehensive R Archive Network (CRAN),\nContribute them to the Bioconductor project,\nPublish them via rOpenSci’s R-universe\n\nUser can then install these packages via the familiar install.packages() command.\nAlternatively, authors can share their code through version control systems like github or gitlab, and users can install them with third-party tools e.g. the remotes R package.\nBut how can you make an R package available privately, e.g. for use within an organization?\nIn this tutorial, I demonstrate how to set up your own package repository with Dirk Eddelbuettel’s drat R package, add a package, make R aware of the new repo - and host it remotely on AWS S3.\n\n\n\nDirk Eddelbuettel highlights two main advantages:\n\nA package installed from a drat repository will be supported by install.packages() and update.packages(), so the user has easy methods for keeping up-to-date.\nThe package author has better control over the package version users install, because they actively push specific releases into the repository.\n\nPlease see Dirk’s Drat FAQ’s for additional points, e.g. ‘Why could install_github be wrong?’\n\n\n\nHadley Wickham and Jenny Bryan have documented how to author, document and build R packages in their freely-available R Packages book. In this walkthrough I am using Mac OS X (v13.1), but you can find instructions to set up Windows or Linux build environments in their R build toolchain chapter.\n\n\n\nFirst, we need an R package that’s ready for distribution. Here, I am using the toy R package that you can retrieve from github, either via git clone https://github.com/tomsing1/toy or by downloading its source code as a zip file. (Feel free to follow along with another R package instead - as long as you have the source package, the following steps apply.)\nNext, we bundle the package into a single compressed file with the .tar.gz file extension. Let’s download the .zip file linked above into the ~/Downloads folder and use the R CMD build command to create a source bundle 1:\n\ncd ~/Downloads\ncurl -s -L -O https://github.com/tomsing1/toy/archive/refs/heads/main.zip\nunzip -o -q main.zip\nrm main.zip\nR CMD build --force toy-main\n\n* checking for file ‘toy-main/DESCRIPTION’ ... OK\n* preparing ‘toy’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘toy_0.1.0.tar.gz’\n\n\nWe now have the toy_0.1.0.tar.gz file, ready to be inserted into a new (or existing) drat repository.\n\n\n\nTo create a new repository, we start by installing the drat R package itself (if it’s not available on your system already) with the following R commands:\n\nif (!requireNamespace(\"drat\", quietly = TRUE)) {\n  install.packages(\"drat\")\n}\nlibrary(drat)\n\nYou can specify the path of your drat repository either by setting the dratRepo option 2:\n\noptions(dratRepo = \"~/drat-tutorial\")\ngetOption(\"dratRepo\")\n\n[1] \"~/drat-tutorial\"\n\n\nor by providing it as an argument to the drat::insertPackage() function (see below).\nLet’s create a new drat repository in our home directory 3, and populate it with a minimal index.html file (to avoid HTTP 404 Not Found errors later).\n\ndir.create(\"~/drat-tutorial\", showWarnings = FALSE)\nwriteLines(\n  text = \"&lt;!doctype html&gt;&lt;title&gt;My awesome drat repository!&lt;/title&gt;\",\n  con = \"~/drat-tutorial/index.html\"\n)\n\nNow we are ready to insert the toy package bundle into the repository with drat’s insertPackage() command 4:\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tar.gz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains the following files:\n\n\n\ndrat repository\n\n\n\n\n\nWhen you prompt your R installation to install or update R packages, it searches repositories specified in the repos option. On my system, only the default repository is set in a fresh R session 5:\n\ngetOption(\"repos\")\n\n                                                BioCsoft \n           \"https://bioconductor.org/packages/3.17/bioc\" \n                                                 BioCann \n\"https://bioconductor.org/packages/3.17/data/annotation\" \n                                                 BioCexp \n\"https://bioconductor.org/packages/3.17/data/experiment\" \n                                           BioCworkflows \n      \"https://bioconductor.org/packages/3.17/workflows\" \n                                               BioCbooks \n          \"https://bioconductor.org/packages/3.17/books\" \n                                                    CRAN \n                           \"https://cloud.r-project.org\" \n\n\nIf I try to install our example toy R package, I don’t succeed:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20'\n(as 'lib' is unspecified)\n\n\nWarning: package 'toy' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nbecause R is not aware of our new repository, yet.\n\n\n\n\n\n\nInstalling from source\n\n\n\nAt this point, we must add the type=\"source\" argument, because we have only added the source bundle to the repository. We will add a compiled version in a moment - read on!\n\n\nTo test our local repository, we add its path to the list of known repositories.\n\ndrat::addRepo(\"LocalRepo\", \"file://Users/sandmann/drat-tutorial\")\ngetOption(\"repos\")\n\n                                                BioCsoft \n           \"https://bioconductor.org/packages/3.17/bioc\" \n                                                 BioCann \n\"https://bioconductor.org/packages/3.17/data/annotation\" \n                                                 BioCexp \n\"https://bioconductor.org/packages/3.17/data/experiment\" \n                                           BioCworkflows \n      \"https://bioconductor.org/packages/3.17/workflows\" \n                                               BioCbooks \n          \"https://bioconductor.org/packages/3.17/books\" \n                                                    CRAN \n                           \"https://cloud.r-project.org\" \n                                               LocalRepo \n                   \"file://Users/sandmann/drat-tutorial\" \n\n\n\n\n\n\n\n\nSpecifying file:// paths\n\n\n\nBy default, drat’s addRepo() command assumes that repositories are hosted on github-pages. Because we want to access a repo via the filesystem (either locally or on a network drive), we need to explicitly add the file:/ prefix - and use the absolute file path (e.g. returned by path.expand(\"~/drat-tutorial\")) to specify its location.\nIn this case, concatenating file:/ with /Users/sandmann/drat-tutorial produces the final file://Users/sandmann/drat-tutorial location (note the double forward slashes).\n\n\nNow, we can install it with the usual install.packages() command 6:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20'\n(as 'lib' is unspecified)\n\n\nGreat! We have successfully installed our toy R package from our brand new repository. Now it is time to make it available to other users as well.\n\n\n\nWindows and Mac users who install packages from CRAN or any user installing files from the Posit Public Package Manager (PPPM) will usually receive a binary package. CRAN accepts package bundles and creates the platform-specific binary file for distribution. To offer the same service to users of our drat repository, we need to compile the binary package ourselves.\nHere, I create the Mac OS binary package from the bundle we obtained above by executing the following command on my Mac OS operating system:\n\ncd ~/Downloads\nR CMD INSTALL --build toy_0.1.0.tar.gz\n\n* installing to library ‘/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20’\n* installing *source* package ‘toy’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* creating tarball\npackaged installation of ‘toy’ as ‘toy_0.1.0.tgz’\n* DONE (toy)\n\n\nThis command will first install the package into my default R library, and then create the binary toy_0.1.0.tgz file.\nNext, we add it to our local drat repository (note the .tgz file suffix).\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tgz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains a new subdirectory (bin) with the binary files for Mac OS X:\n\n\n\ndrat repository\n\n\nAt long last, now we can omit the type=\"source\" argument from calls to install.packages():\n\ninstall.packages(\"toy\")\n\nrenv was unable to query available packages from the following repositories:\n- # file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3 --------\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES.rds' [curl: (3) URL rejected: Bad file:// URL]\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES.gz' [curl: (3) URL rejected: Bad file:// URL]\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES' [curl: (3) URL rejected: Bad file:// URL]\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading toy from LocalRepo ...            OK [1.5 Kb]\nSuccessfully downloaded 1 package in 3.5 seconds.\n\nThe following package(s) will be installed:\n- toy [0.1.0]\nThese packages will be installed into \"~/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing toy ...                            OK [built from source and cached in 1.1s]\nSuccessfully installed 1 package in 1.1 seconds.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/drat/index.html#tldr",
    "href": "posts/drat/index.html#tldr",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "",
    "text": "Today I learned how to\n\nBuild an R package into source and binary bundles for distribution.\nCreate a local drat repository.\nAdd an R package to the repository and install it from there.\nHost the repository remotely in an AWS S3 bucket.\n\nMany thanks to Dirk Eddelbuettel for creating and documenting the drat R package! (As always, any mistakes are my own.)\n\n\nThere are multiple ways for developers to share R packages publicly, e.g.\n\nSubmit them to the The Comprehensive R Archive Network (CRAN),\nContribute them to the Bioconductor project,\nPublish them via rOpenSci’s R-universe\n\nUser can then install these packages via the familiar install.packages() command.\nAlternatively, authors can share their code through version control systems like github or gitlab, and users can install them with third-party tools e.g. the remotes R package.\nBut how can you make an R package available privately, e.g. for use within an organization?\nIn this tutorial, I demonstrate how to set up your own package repository with Dirk Eddelbuettel’s drat R package, add a package, make R aware of the new repo - and host it remotely on AWS S3.\n\n\n\nDirk Eddelbuettel highlights two main advantages:\n\nA package installed from a drat repository will be supported by install.packages() and update.packages(), so the user has easy methods for keeping up-to-date.\nThe package author has better control over the package version users install, because they actively push specific releases into the repository.\n\nPlease see Dirk’s Drat FAQ’s for additional points, e.g. ‘Why could install_github be wrong?’\n\n\n\nHadley Wickham and Jenny Bryan have documented how to author, document and build R packages in their freely-available R Packages book. In this walkthrough I am using Mac OS X (v13.1), but you can find instructions to set up Windows or Linux build environments in their R build toolchain chapter.\n\n\n\nFirst, we need an R package that’s ready for distribution. Here, I am using the toy R package that you can retrieve from github, either via git clone https://github.com/tomsing1/toy or by downloading its source code as a zip file. (Feel free to follow along with another R package instead - as long as you have the source package, the following steps apply.)\nNext, we bundle the package into a single compressed file with the .tar.gz file extension. Let’s download the .zip file linked above into the ~/Downloads folder and use the R CMD build command to create a source bundle 1:\n\ncd ~/Downloads\ncurl -s -L -O https://github.com/tomsing1/toy/archive/refs/heads/main.zip\nunzip -o -q main.zip\nrm main.zip\nR CMD build --force toy-main\n\n* checking for file ‘toy-main/DESCRIPTION’ ... OK\n* preparing ‘toy’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘toy_0.1.0.tar.gz’\n\n\nWe now have the toy_0.1.0.tar.gz file, ready to be inserted into a new (or existing) drat repository.\n\n\n\nTo create a new repository, we start by installing the drat R package itself (if it’s not available on your system already) with the following R commands:\n\nif (!requireNamespace(\"drat\", quietly = TRUE)) {\n  install.packages(\"drat\")\n}\nlibrary(drat)\n\nYou can specify the path of your drat repository either by setting the dratRepo option 2:\n\noptions(dratRepo = \"~/drat-tutorial\")\ngetOption(\"dratRepo\")\n\n[1] \"~/drat-tutorial\"\n\n\nor by providing it as an argument to the drat::insertPackage() function (see below).\nLet’s create a new drat repository in our home directory 3, and populate it with a minimal index.html file (to avoid HTTP 404 Not Found errors later).\n\ndir.create(\"~/drat-tutorial\", showWarnings = FALSE)\nwriteLines(\n  text = \"&lt;!doctype html&gt;&lt;title&gt;My awesome drat repository!&lt;/title&gt;\",\n  con = \"~/drat-tutorial/index.html\"\n)\n\nNow we are ready to insert the toy package bundle into the repository with drat’s insertPackage() command 4:\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tar.gz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains the following files:\n\n\n\ndrat repository\n\n\n\n\n\nWhen you prompt your R installation to install or update R packages, it searches repositories specified in the repos option. On my system, only the default repository is set in a fresh R session 5:\n\ngetOption(\"repos\")\n\n                                                BioCsoft \n           \"https://bioconductor.org/packages/3.17/bioc\" \n                                                 BioCann \n\"https://bioconductor.org/packages/3.17/data/annotation\" \n                                                 BioCexp \n\"https://bioconductor.org/packages/3.17/data/experiment\" \n                                           BioCworkflows \n      \"https://bioconductor.org/packages/3.17/workflows\" \n                                               BioCbooks \n          \"https://bioconductor.org/packages/3.17/books\" \n                                                    CRAN \n                           \"https://cloud.r-project.org\" \n\n\nIf I try to install our example toy R package, I don’t succeed:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20'\n(as 'lib' is unspecified)\n\n\nWarning: package 'toy' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nbecause R is not aware of our new repository, yet.\n\n\n\n\n\n\nInstalling from source\n\n\n\nAt this point, we must add the type=\"source\" argument, because we have only added the source bundle to the repository. We will add a compiled version in a moment - read on!\n\n\nTo test our local repository, we add its path to the list of known repositories.\n\ndrat::addRepo(\"LocalRepo\", \"file://Users/sandmann/drat-tutorial\")\ngetOption(\"repos\")\n\n                                                BioCsoft \n           \"https://bioconductor.org/packages/3.17/bioc\" \n                                                 BioCann \n\"https://bioconductor.org/packages/3.17/data/annotation\" \n                                                 BioCexp \n\"https://bioconductor.org/packages/3.17/data/experiment\" \n                                           BioCworkflows \n      \"https://bioconductor.org/packages/3.17/workflows\" \n                                               BioCbooks \n          \"https://bioconductor.org/packages/3.17/books\" \n                                                    CRAN \n                           \"https://cloud.r-project.org\" \n                                               LocalRepo \n                   \"file://Users/sandmann/drat-tutorial\" \n\n\n\n\n\n\n\n\nSpecifying file:// paths\n\n\n\nBy default, drat’s addRepo() command assumes that repositories are hosted on github-pages. Because we want to access a repo via the filesystem (either locally or on a network drive), we need to explicitly add the file:/ prefix - and use the absolute file path (e.g. returned by path.expand(\"~/drat-tutorial\")) to specify its location.\nIn this case, concatenating file:/ with /Users/sandmann/drat-tutorial produces the final file://Users/sandmann/drat-tutorial location (note the double forward slashes).\n\n\nNow, we can install it with the usual install.packages() command 6:\n\ninstall.packages(\"toy\", type = \"source\")\n\nInstalling package into '/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20'\n(as 'lib' is unspecified)\n\n\nGreat! We have successfully installed our toy R package from our brand new repository. Now it is time to make it available to other users as well.\n\n\n\nWindows and Mac users who install packages from CRAN or any user installing files from the Posit Public Package Manager (PPPM) will usually receive a binary package. CRAN accepts package bundles and creates the platform-specific binary file for distribution. To offer the same service to users of our drat repository, we need to compile the binary package ourselves.\nHere, I create the Mac OS binary package from the bundle we obtained above by executing the following command on my Mac OS operating system:\n\ncd ~/Downloads\nR CMD INSTALL --build toy_0.1.0.tar.gz\n\n* installing to library ‘/Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20’\n* installing *source* package ‘toy’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* creating tarball\npackaged installation of ‘toy’ as ‘toy_0.1.0.tgz’\n* DONE (toy)\n\n\nThis command will first install the package into my default R library, and then create the binary toy_0.1.0.tgz file.\nNext, we add it to our local drat repository (note the .tgz file suffix).\n\ndrat::insertPackage(file = \"~/Downloads/toy_0.1.0.tgz\",\n                    repodir = \"~/drat-tutorial\")\n\nNow, the ~/drat-tutorial folder contains a new subdirectory (bin) with the binary files for Mac OS X:\n\n\n\ndrat repository\n\n\nAt long last, now we can omit the type=\"source\" argument from calls to install.packages():\n\ninstall.packages(\"toy\")\n\nrenv was unable to query available packages from the following repositories:\n- # file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3 --------\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES.rds' [curl: (3) URL rejected: Bad file:// URL]\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES.gz' [curl: (3) URL rejected: Bad file:// URL]\nerror downloading 'file://Users/sandmann/drat-tutorial/bin/macosx/big-sur-arm64/contrib/4.3/PACKAGES' [curl: (3) URL rejected: Bad file:// URL]\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading toy from LocalRepo ...            OK [1.5 Kb]\nSuccessfully downloaded 1 package in 3.5 seconds.\n\nThe following package(s) will be installed:\n- toy [0.1.0]\nThese packages will be installed into \"~/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing toy ...                            OK [built from source and cached in 1.1s]\nSuccessfully installed 1 package in 1.1 seconds."
  },
  {
    "objectID": "posts/drat/index.html#hosting-your-drat-repository-on-aws-s3",
    "href": "posts/drat/index.html#hosting-your-drat-repository-on-aws-s3",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Hosting your drat repository on AWS S3",
    "text": "Hosting your drat repository on AWS S3\ndrat repositories can be hosted in any location\n\nthat you can write files to and\nthat can serve files via http\n\nBut unless you placed your drat repository into a network drive that is accessible by multiple users, it is currently only useful to yourself.\n\n\n\n\n\n\nSharing repository over a local network\n\n\n\n\n\nIf you chose a network drive as the location of your drat repository, then other user can benefit from it right - as long as they can read from the shared directory. As before, the absolute path must be prefixed with the file:/ prefix. For example, a repository that is available on the user’s systems at /nfs/groups/groupABC/R/drat would be added to the list of R repositories via drat::addRepo(\"workgroup\", \"file://nfs/groups/groupABC/R/drat\").\n\n\n\nThe drat documentation illustrates how you can use git and github pages to make your repository publicly available.\nHere, we are interested in hosting a repository privately instead, e.g. in a location that is only accessible from within our own organization:\n\nIf you already have access to a private server that serves files to your users (e.g. via HTTP), then you can simply copy your repository there.\nIf your organization uses Amazon Web Services (AWS), you can also use an S3 bucket to host your repository and take advantage of the access controls set by your organization.\n\n\n\n\n\n\n\nPublic repositories in S3 buckets\n\n\n\n\n\nAlthough this use case focuses on hosting private repositories, you can of course also make repositories in S3 buckets publicly available. Alas, data storage in S3 buckets incurs cost, while other options (e.g. github-pages, CRAN, Bioconductor, etc) are free, so this might not be your preferred option.\n\n\n\nWe will assume that you have write access to an S3 bucket that is configured to serve static files via HTTP. (For a brief outline of the necessary steps, please see the appendix ). Here, I am using a bucket called drat-tutorial - but you should create / access your own bucket to follow along.\n\n\n\n\n\n\nWarning\n\n\n\nAWS S3 buckets can be configured to either be visible publicly, or access can be restricted to specific IP addresses, security groups or other AWS resources. Please make sure you have configured your bucket in a way that suits your needs.\nS3 buckets do not support the HTTPS protocol. If you require an encrypted file transfer, you might need a different solution.\n\n\nTo share our repository, we must first copy its folder to the S3 bucket, either via the AWS Console or (more conveniently) with the aws command line interface7. (If you are adventurous, you can also mount an S3 bucket as a filey system with goofys).\nAssuming you have set the necessary AWS credentials, the following aws s3 sync command copies our repository to the repo folder within drat-tutorial bucket that I created in the us-west-1 AWS region.\n\naws s3 sync ~/drat-tutorial s3://drat-tutorial/repo\n\nWe can use the aws s3 ls command to confirm the upload:\n\naws s3 ls s3://drat-tutorial/repo/\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we make changes to our local repository, e.g. after adding new packages or package versions, we have to rerun the aws s3 sync command to copy the new files to the S3 bucket.\n\n\nNow that the files are in place, we can add our remote repository to the the list of R repositories in our R session. First, we remove the LocalRepo repository that we had added earlier, which points to the folder on our local filesystem.\n\noptions(repos = getOption(\"repos\")[\n  setdiff(names(getOption(\"repos\")), \"LocalRepo\")\n])\n\nThe we add the remote repository instead, by pointing to the URL of the S3 bucket 8.\n\ndrat::addRepo(\"S3repo\", \"http://drat-tutorial.s3.us-west-1.amazonaws.com/repo/\")\ngetOption(\"repos\")\n\nLet’s try to install the toy package from our S3 drat repository:\n\ninstall.packages(\"toy\")\n\nSuccess! R has successfully connected to the remote repository and installed the (binary) R package."
  },
  {
    "objectID": "posts/drat/index.html#conclusions",
    "href": "posts/drat/index.html#conclusions",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe drat R package makes it extremely simple to create a CRAN-like repository.\nThe static files can be served via HTTP, making it straightforward to host the repository e.g. in an AWS S3 bucket with a restrictive access policy."
  },
  {
    "objectID": "posts/drat/index.html#appendix",
    "href": "posts/drat/index.html#appendix",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Appendix",
    "text": "Appendix\n\nCreating and configuring an S3 bucket to host static files\nThe following steps briefly outline how to create and configure an S3 bucket to act as a static web server via the AWS web interface (e.g. the AWS Console). For more details, please read the AWS S3 documentation and / or consult your local AWS expert.\n\n\n\n\n\n\nWarning\n\n\n\nStoring files on AWS S3 is not free. In this tutorial, we only upload a limited number of small files, but please don’t forget to purge them from your AWS account afterward.\n\n\n\nCreate a new bucket (skip if you already have one)\n\n\nMake sure you create the bucket in the region that works best for your organization (e.g. us-west-1 if you want to host your files in California).\nYou do not need to enable public access, stick to the defaults for your organization.\n\n\n\nCreate an S3 bucket\n\n\n\n\nNext, navigate to your bucket’s properties,\n\nscroll all the way to the bottom of the page and enable Static website hosting.\n\n(Typically) specify index.html as the Index document.\n\nUnder the Permissions tab, add a bucket policy that makes your content available within your organization\n\n\n\n\n\n\nWarning\n\n\n\nThese settings determine who can access your files. Proceed with caution to avoid inadvertently exposing your data to the world!\n\n\nFor example, the following policy grants read access to all files in the s3://drat-tutorial/ bucket to requests originating (only) from the 192.0.2.0 IP address. (Your own configuration will be different, of course.)\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": [\n        \"s3:GetObject\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::drat-tutorial/*\"\n      ],\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"192.0.2.0/32\"\n        }\n      }\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/drat/index.html#footnotes",
    "href": "posts/drat/index.html#footnotes",
    "title": "Distributing R packages with a drat repository hosted on AWS S3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternatively, you can also create the bundle from within R using the devtools::build() command.↩︎\nYou might want to add this option to your .Rprofile file.↩︎\nOf course, you can place it anywhere you like, including e.g. network drives, as long as you can write to the directory. If you are using Windows, please remember to use backward instead of forward slashes in your paths.↩︎\nIn this tutorial, I use the :: notation to highlight which package a function originates from. Because we attached the package with the library(drat) command before, the drat:: prefix could be omitted.↩︎\nIn this tutorial, I use the :: notation to highlight in which package functions originate from. Because we attached the package with the library(drat) command before, the drat:: prefix could be omitted.↩︎\nIf you use Bioconductor, the BiocManager::repositories() specifies additional repositories that host its annotation and software packages.↩︎\nInstallation instructions.↩︎\nYou can look up the URL for your bucket in the AWS S3 console: ↩︎"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html",
    "href": "posts/geneset-sqlite-db/index.html",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "",
    "text": "The first step of any data analysis is to obtain and explore the available data, often by accessing and querying a database. There many great introductions on how to read data into an R session. But I found it harder to find tutorials on how to create and populate a new database from scratch.\nIn this document, I explore both noSQL and SQL approaches to data management. As an example use case, we store a collection of gene sets, specifically the mouse MSigDb hallmark gene sets (MH), either as unstructured documents or in relational tables.\n\n\nBioconductor offers well designed S4 Classes to store gene set collections, including e.g. in a list-like GSEABase::GeneSetCollection or a set of three tibbles within a BiocSet::BiocSet object. So why could we be interested in storing this information in a database?\n\nA database (e.g. SQLite, Postgres, etc) offers a standardized way to store, manage and access information in a language-agnostic way. E.g. some of my colleagues use python for their analyses and are comfortable retrieving gene set information from a database, but not necessarily from an R S4 object.\nGene sets capture knowledge from multiple experiments, studies and sources. If you are part of a larger organization a single source of truth, available in a central location, is very useful.\nCollaborators might not be interested / able to access information programmatically, e.g. they may prefer a web application to search, share and edit gene sets. Many tools to build web applications have built-in capabilities to interact with a database.\nAs the number of gene sets grows, sharing them in the form of one or more files might become cumbersome. A hosted database (e.g.  Postgres or MariaDB ) allows users to retrieve only the information they need.\n\nIn this tutorial, I am using the SQLite engine to explore both relational and non-relational ways to manage gene sets. SQLite can be embedded into applications, and does not require a central server, making it ideal for experimentation. (But as you move into a scenario where multiple users need to access a central, it is time to switch to a hosted database instead; my favorite is Postgres.)\n\nlibrary(BiocSet)\nlibrary(dm)\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(nodbi)\nlibrary(org.Mm.eg.db)\nlibrary(purrr)\nlibrary(DiagrammeR)\nlibrary(RSQLite)\nlibrary(tibble)\nlibrary(tidyr)\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#creating-polulating-and-querying-sql-and-nosql-databases-with-r",
    "href": "posts/geneset-sqlite-db/index.html#creating-polulating-and-querying-sql-and-nosql-databases-with-r",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "",
    "text": "The first step of any data analysis is to obtain and explore the available data, often by accessing and querying a database. There many great introductions on how to read data into an R session. But I found it harder to find tutorials on how to create and populate a new database from scratch.\nIn this document, I explore both noSQL and SQL approaches to data management. As an example use case, we store a collection of gene sets, specifically the mouse MSigDb hallmark gene sets (MH), either as unstructured documents or in relational tables.\n\n\nBioconductor offers well designed S4 Classes to store gene set collections, including e.g. in a list-like GSEABase::GeneSetCollection or a set of three tibbles within a BiocSet::BiocSet object. So why could we be interested in storing this information in a database?\n\nA database (e.g. SQLite, Postgres, etc) offers a standardized way to store, manage and access information in a language-agnostic way. E.g. some of my colleagues use python for their analyses and are comfortable retrieving gene set information from a database, but not necessarily from an R S4 object.\nGene sets capture knowledge from multiple experiments, studies and sources. If you are part of a larger organization a single source of truth, available in a central location, is very useful.\nCollaborators might not be interested / able to access information programmatically, e.g. they may prefer a web application to search, share and edit gene sets. Many tools to build web applications have built-in capabilities to interact with a database.\nAs the number of gene sets grows, sharing them in the form of one or more files might become cumbersome. A hosted database (e.g.  Postgres or MariaDB ) allows users to retrieve only the information they need.\n\nIn this tutorial, I am using the SQLite engine to explore both relational and non-relational ways to manage gene sets. SQLite can be embedded into applications, and does not require a central server, making it ideal for experimentation. (But as you move into a scenario where multiple users need to access a central, it is time to switch to a hosted database instead; my favorite is Postgres.)\n\nlibrary(BiocSet)\nlibrary(dm)\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(nodbi)\nlibrary(org.Mm.eg.db)\nlibrary(purrr)\nlibrary(DiagrammeR)\nlibrary(RSQLite)\nlibrary(tibble)\nlibrary(tidyr)"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#the-mouse-hallmarks-msigdb-collection",
    "href": "posts/geneset-sqlite-db/index.html#the-mouse-hallmarks-msigdb-collection",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "The Mouse Hallmarks MSigDB collection",
    "text": "The Mouse Hallmarks MSigDB collection\nAt the time of writing, Mouse Molecular Signatures Database (MSigDB) contains 15918 gene sets, organized into numerous different collections. For example, the 50 hallmark gene sets (MH) summarize and represent specific well-defined biological states or processes ( Liberzon et al, Cell Systems, 2015 ).\n\n\n\n\n\n\nGene symbols\n\n\n\nEach of the 50 sets in the collection contains between 32 and 200 official gene symbols, specifying the members of the gene set.\n\n\nHere, I will use the hallmarks collection as an example but the overall approach can be applied to other gene set collection in a similar way. (You might need additional / different annotation fields, though.)\nThe mouse hallmarks collection is available in different formats, including as a JSON file. Let’s start by reading it into an R session as nested list mh.\n\njson_file &lt;- paste0(\n  \"https://raw.githubusercontent.com/tomsing1/blog/main/posts/\",\n  \"geneset-sqlite-db/mh.all.v2022.1.Mm.json\")\nmh &lt;- jsonlite::read_json(json_file, simplifyVector = TRUE)\n\nEach of the 50 elements in the JSON file corresponds to a different gene set,\n\nhead(names(mh))\n\n[1] \"HALLMARK_ADIPOGENESIS\"        \"HALLMARK_ALLOGRAFT_REJECTION\"\n[3] \"HALLMARK_ANDROGEN_RESPONSE\"   \"HALLMARK_ANGIOGENESIS\"       \n[5] \"HALLMARK_APICAL_JUNCTION\"     \"HALLMARK_APICAL_SURFACE\"     \n\n\neach gene set is a nested list with the following elements,\n\nlengths(mh[[1]])\n\n              systematicName                         pmid \n                           1                            1 \n                 exactSource                  geneSymbols \n                           1                          200 \n                   msigdbURL           externalDetailsURL \n                           1                            1 \n        filteredBySimilarity externalNamesForSimilarTerms \n                           0                            0 \n                  collection \n                           1 \n\n\nand the gene symbols that make up the set are listed in the geneSymbols vector:\n\nhead(mh[[1]]$geneSymbols)\n\n[1] \"Abca1\" \"Abcb8\" \"Acaa2\" \"Acadl\" \"Acadm\" \"Acads\""
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#nosql-storing-gene-sets-as-unstructured-documents",
    "href": "posts/geneset-sqlite-db/index.html#nosql-storing-gene-sets-as-unstructured-documents",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "noSQL: storing gene sets as unstructured documents",
    "text": "noSQL: storing gene sets as unstructured documents\nEach gene set is represented as a list - so why not store it in the same way? A noSQL database is designed to store unstructed information, e.g. data models that are not organized in tables, making them flexible and scalable. Examples of noSQL databases include e.g. Mongodb, CouchDB or AWS dynamodb.\nIn addition, traditional relational database engines - including SQLite and Postgres - can also store unstructured data in dedicated JSON fields.\nThe nodbi R package provides a unified interface to multiple noSQL implementations, including SQLite. (If you are interested in a deeper look at how to create & query a JSON field in SQLite with raw SQL, check out this gist ).\n\nCreating & populating a noSQL database with the nodbi R package\nTo experiment with its noSQL mode, we create a temporary SQLite database in memory. (For real data, you definitely want to provide a file path as the dbname instead!)\n\nsrc &lt;- nodbi::src_sqlite(dbname = \":memory:\")\n\nRight now, the names of the gene sets are only stored as the names() of the list elements, e.g. not in a field within each sub-list itself. To make sure they are included in each database record, we add them to each sub-list in a new name field.\n\nmh2 &lt;- lapply(names(mh), \\(gs) c(\"name\" = gs, mh[[gs]]))\n\n\n\n\n\n\n\nUnique identifiers\n\n\n\nThe docdb_create() function accepts either a data.frame, a JSON string or a list as its value argument.\nIf you include a field _id in your list, it will be used as the primary key for each element. If no _id field is found, then the _id field is created automatically with a call to the uuid::UUIDgenerate() function.\nIf you provide a data.frames() with row.names, they will be used to populate the _id field.\n\n\nNow we are ready to create a new SQLite table hallmarks and populate it with the 50 gene sets.\n\ndocdb_create(src, key = \"hallmarks\", value = mh2)\n\n[1] 50\n\n\nWe can retrieve the full set of records as a data.frame with the docdb_get() function. (Here we select a subset of the returned columns due to space constraints.) Because each gene set contains multiple geneSymbols, this field is a list-column.\n\ndocdb_get(src, \"hallmarks\")[1:4, c(\"name\", \"geneSymbols\", \"pmid\")]\n\n                          name  geneSymbols     pmid\n1        HALLMARK_ADIPOGENESIS Abca1, A.... 30224793\n2 HALLMARK_ALLOGRAFT_REJECTION Aars, Ab.... 30224793\n3   HALLMARK_ANDROGEN_RESPONSE Abcc4, A.... 30224793\n4        HALLMARK_ANGIOGENESIS Apoh, Ap.... 30224793\n\n\n\n\nQuerying with JSON filters\nMore commonly, users might want to retrieve one or more gene sets by name. The docdb_query() function accepts a query argument specifying the desired filter criteria (as MongoDB JSON ).\n\nresults &lt;- nodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = '{\"name\": \"HALLMARK_ADIPOGENESIS\"}')\nresults[, c(\"name\", \"geneSymbols\", \"pmid\")]\n\n                   name  geneSymbols     pmid\n1 HALLMARK_ADIPOGENESIS Abca1, A.... 30224793\n\n\nThe fields argument allows us to return only specific columns. (Specifying a field as 1 or 0 will include or exclude it, respectively.)\n\nnodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = '{\"name\": \"HALLMARK_ADIPOGENESIS\"}',\n  fields = '{\"name\": 1, \"geneSymbols\": 1}'\n)\n\n                   name  geneSymbols\n1 HALLMARK_ADIPOGENESIS Abca1, A....\n\n\nWe can also identify gene sets containing at least one of the given gene symbols:\n\nresults &lt;- nodbi::docdb_query(\n  src = src, key = \"hallmarks\",\n  query = paste0('{\"$or\":[',\n                 '{\"geneSymbols\": \"Abca1\"},', \n                 '{\"geneSymbols\": \"Gapdh\"}',\n                 ']}'),\n  fields = '{\"name\": 1, \"geneSymbols\": 1}'\n)\n\n\n\n\n\n\n\nUnnesting columns\n\n\n\n\n\nBecause the set contains more than one geneSymbol, we obtain a nested data.frame. We can unnest it e.g. with the tidyr R package\n\ntidyr::unnest(results, cols = c(geneSymbols))\n\n# A tibble: 798 × 2\n   name                  geneSymbols\n   &lt;chr&gt;                 &lt;chr&gt;      \n 1 HALLMARK_ADIPOGENESIS Abca1      \n 2 HALLMARK_ADIPOGENESIS Abcb8      \n 3 HALLMARK_ADIPOGENESIS Acaa2      \n 4 HALLMARK_ADIPOGENESIS Acadl      \n 5 HALLMARK_ADIPOGENESIS Acadm      \n 6 HALLMARK_ADIPOGENESIS Acads      \n 7 HALLMARK_ADIPOGENESIS Acly       \n 8 HALLMARK_ADIPOGENESIS Aco2       \n 9 HALLMARK_ADIPOGENESIS Acox1      \n10 HALLMARK_ADIPOGENESIS Adcy6      \n# ℹ 788 more rows\n\n\n\n\n\n\n\nQuerying using SQL\nFormulating the queries as JSON strings is tedious, though. Alternatively, SQLite also supports querying JSON columns using SQL (muddying the border between noSQL and SQL). For example, we can use SQLite’s -&gt; and -&gt;&gt; operators and the json_each() SQL function to create a query that returns the names of all gene sets that include e.g. the Abca1 gene:\n\nSELECT hallmarks.json-&gt;&gt;'name' as name\nFROM hallmarks, json_each(hallmarks.json, '$.geneSymbols')\nWHERE json_each.value LIKE '%Abca1%'\n\n\n5 records\n\n\nname\n\n\n\n\nHALLMARK_ADIPOGENESIS\n\n\nHALLMARK_BILE_ACID_METABOLISM\n\n\nHALLMARK_INFLAMMATORY_RESPONSE\n\n\nHALLMARK_PROTEIN_SECRETION\n\n\nHALLMARK_TNFA_SIGNALING_VIA_NFKB\n\n\n\n\n\nDepending on comfortable you are reading / writing SQL, this might be a nicer approach.\n\n\n\n\n\n\nLimitations\n\n\n\nSQLite’s JSON operators are somewhat limited, e.g. there is no straightforward way to ask whether a column contains one or more gene identifiers (e.g. the query we performed above using a query JSON string). Indexing a SQLite JSON column also comes with limitations.\nThe Postgres database engine supports JSON and binary JSONB fields) with indexing & additional operators like the @&gt; contains operator.\n\n\n\n\nnoSQL summary\nThis example highlights some of the advantages of a noSQL solution:\n\nRapid ingestion of data without the need for a rigid schema.\nSimple retrieval of individual object identified by their primary key.\n\nBut also some of the disadvantages:\n\nQueries that descend into the (potentially nested) objects must be carefully constructed.\nIncreasing database performance with indices is more complicated than for relational databases (see below.)\n\nNext, we will try another approach: reshaping the gene set collection into a set of tables and modeling the relationship between them.s"
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#sql-storing-gene-sets-in-a-relational-database",
    "href": "posts/geneset-sqlite-db/index.html#sql-storing-gene-sets-in-a-relational-database",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "SQL: storing gene sets in a relational database",
    "text": "SQL: storing gene sets in a relational database\nR has excellent support for interacting with relational database, e.g. via the foundational ‘Common Database Interface’ (DBI) package and the numerous database-specific packages built on top of it, including the RSQLite, RPostgres and many others.\nTo take advantage of a relational database we have perform a little more work up-front. But this effort is amply repaid by simplifying subsequent queries.\n\nLearning from Bioconductor: BiocSet’s three tables\nThe BiocSet Class from the eponymous Bioconductor package represents a collection of gene sets in three tibbles. Let’s create a simple BiocSet with two gene sets for illustration:\n\nset_names &lt;- purrr::map_chr(mh2[1:2], \"name\")\ngene_ids &lt;- purrr::map(mh2[1:2], \"geneSymbols\")\nes &lt;- BiocSet(setNames(gene_ids, set_names))\n\nThe first two tibbles represent genes (called elements) and sets, respectively:\n\nes_element: one row per gene\n\n\nhead(es_element(es))\n\n# A tibble: 6 × 1\n  element\n  &lt;chr&gt;  \n1 Abca1  \n2 Abcb8  \n3 Acaa2  \n4 Acadl  \n5 Acadm  \n6 Acads  \n\n\n\nes_set: one row per gene set\n\n\nes_set(es)\n\n# A tibble: 2 × 1\n  set                         \n  &lt;chr&gt;                       \n1 HALLMARK_ADIPOGENESIS       \n2 HALLMARK_ALLOGRAFT_REJECTION\n\n\nThe third table establishes the many-to-many relationship between genes and sets, e.g. it tracks which gene is a member of each set.\n\nes_elementset: gene x set combination\n\n\n# we are showing 10 random rows\nset.seed(42)\nes_elementset(es)[sample(nrow(es_elementset(es)), size = 10), ]\n\n# A tibble: 10 × 2\n   element set                         \n   &lt;chr&gt;   &lt;chr&gt;                       \n 1 Coq5    HALLMARK_ADIPOGENESIS       \n 2 Irf7    HALLMARK_ALLOGRAFT_REJECTION\n 3 Qdpr    HALLMARK_ADIPOGENESIS       \n 4 Elovl6  HALLMARK_ADIPOGENESIS       \n 5 Cd28    HALLMARK_ALLOGRAFT_REJECTION\n 6 Ppm1b   HALLMARK_ADIPOGENESIS       \n 7 Mtarc2  HALLMARK_ADIPOGENESIS       \n 8 Zap70   HALLMARK_ALLOGRAFT_REJECTION\n 9 Ndufb7  HALLMARK_ADIPOGENESIS       \n10 Il15    HALLMARK_ALLOGRAFT_REJECTION\n\n\nEach of these tables can be augmented with additional metadata, e.g. we could add Entrez gene identifiers to the es_element (see below), or long-form descriptions for each set to the es_set tibble.\nThese three tables can easily be represented in a relational database, using the element and set columns as primary keys.\n\n\nCreating and populating a relational database\nLet’s start with a fresh SQLite database.\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nFirst, we create the geneset data.frame that lists all gene sets, and we also include their MSigDb URLs as metadata:\n\ngeneset &lt;- data.frame(\n  geneset = purrr::map_chr(mh2, \"name\"),\n  url = purrr::map_chr(mh2, \"msigdbURL\"))\nhead(geneset)\n\n                       geneset\n1        HALLMARK_ADIPOGENESIS\n2 HALLMARK_ALLOGRAFT_REJECTION\n3   HALLMARK_ANDROGEN_RESPONSE\n4        HALLMARK_ANGIOGENESIS\n5     HALLMARK_APICAL_JUNCTION\n6      HALLMARK_APICAL_SURFACE\n                                                                                 url\n1        https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ADIPOGENESIS\n2 https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ALLOGRAFT_REJECTION\n3   https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ANDROGEN_RESPONSE\n4        https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_ANGIOGENESIS\n5     https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_APICAL_JUNCTION\n6      https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HALLMARK_APICAL_SURFACE\n\n\nNext, we identify all unique gene symbols, annotate them with their Entrez ids (using the org.Mm.eg.db Bioconductor annotation package), and store both identifier types in the element data.frame.\n\ngene_symbols &lt;- unique(unlist(purrr::map(mh2, \"geneSymbols\")))\nelement &lt;- data.frame(\n  element = gene_symbols,\n  entrezid = mapIds(org.Mm.eg.db, keys = gene_symbols, keytype = \"SYMBOL\", \n                    column = \"ENTREZID\")\n  )\nhead(element)\n\n      element entrezid\nAbca1   Abca1    11303\nAbcb8   Abcb8    74610\nAcaa2   Acaa2    52538\nAcadl   Acadl    11363\nAcadm   Acadm    11364\nAcads   Acads    11409\n\n\nFinally, we create the element_set join table, connecting gene sets to their constituent genes:\n\nelementset &lt;- purrr::map_df(mh2, \\(gs) {\n  with(gs, \n       data.frame(\n         element = geneSymbols,\n         geneset = name\n       )\n  )\n})\nhead(elementset)\n\n  element               geneset\n1   Abca1 HALLMARK_ADIPOGENESIS\n2   Abcb8 HALLMARK_ADIPOGENESIS\n3   Acaa2 HALLMARK_ADIPOGENESIS\n4   Acadl HALLMARK_ADIPOGENESIS\n5   Acadm HALLMARK_ADIPOGENESIS\n6   Acads HALLMARK_ADIPOGENESIS\n\n\nNext, we write each data.frame into a separate table in our SQLite database.\n\n\n\n\n\n\nVerifying foreign keys\n\n\n\n\n\nBy default, SQLite does not verify that foreign keys actually exist in the referenced table. To make this a requirement, we can enable checking with the following command:\n\ndbExecute(con, 'PRAGMA foreign_keys = 1;')\n\n[1] 0\n\n\n\n\n\n\ndbExecute(con, \n          \"CREATE TABLE tbl_geneset (geneset TEXT PRIMARY KEY, url TEXT)\")\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_geneset\", value = geneset, overwrite = TRUE)\n\ndbExecute(con, \n          \"CREATE TABLE tbl_element (element TEXT PRIMARY KEY, entrezid TEXT)\")\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_element\", value = element, overwrite = TRUE)\n\ndbExecute(con, paste(\n  \"CREATE TABLE tbl_elementset (\",\n  \"element TEXT,\", \n  \"geneset TEXT,\",\n  \"FOREIGN KEY(geneset) REFERENCES tbl_geneset(geneset),\",\n  \"FOREIGN KEY(element) REFERENCES tbl_element(element)\",\n  \")\")\n  )\n\n[1] 0\n\ndbWriteTable(con, name = \"tbl_elementset\", value = elementset, overwrite = TRUE)\n\n\ndbListTables(con)\n\n[1] \"tbl_element\"    \"tbl_elementset\" \"tbl_geneset\"   \n\n\n\n\nPlotting relationships\nAs we create and need to keep track of multiple tables, it is useful to visualize their contents (fields, columns) and relationships in a model diagram. The awesome dm R package, designed to bring an existing relational data model into your R session, can be used to generate diagrams like the one shown below. (dm can identify the keys in postgres and SQL server database engines automatically, but for SQLite we need to specify them ourselves with the dm_add_pk() and dm_add_fk() functions.)\n\ndm_from_con(con, learn_keys = FALSE) %&gt;%\n  dm_add_pk(tbl_element, element) %&gt;%\n  dm_add_pk(tbl_geneset, geneset) %&gt;%\n  dm_add_fk(tbl_elementset, element, tbl_element) %&gt;%\n  dm_add_fk(tbl_elementset, geneset, tbl_geneset) %&gt;%\n  dm_draw(view_type = \"all\")\n\n\n\n\nModel diagram\n\n\n\n\nQuerying the database\nGreat! Now we are ready to query our database. To make our lives easier, we will use the dplyr package to translate our R syntax into SQL. (But we could just as well use plain SQL instead.)\nFirst we define the remote tables by connecting to our brand new database:\n\ntbl_geneset &lt;- tbl(con, \"tbl_geneset\")\ntbl_element &lt;- tbl(con, \"tbl_element\")\ntbl_elementset &lt;- tbl(con, \"tbl_elementset\")\n\nLet’s return the gene symbols and entrez identifiers that make up the HALLMARK_APOPTOSIS gene set and display the first 5 (in alphabetical order of the gene symbols).\n\nresult &lt;- tbl_elementset %&gt;% \n  dplyr::filter(geneset == \"HALLMARK_ADIPOGENESIS\") %&gt;%\n  dplyr::inner_join(tbl_element, by = \"element\") %&gt;%\n  dplyr::slice_min(n = 5, order_by = element)\nresult\n\n# Source:   SQL [5 x 3]\n# Database: sqlite 3.41.2 [:memory:]\n  element geneset               entrezid\n  &lt;chr&gt;   &lt;chr&gt;                 &lt;chr&gt;   \n1 Abca1   HALLMARK_ADIPOGENESIS 11303   \n2 Abcb8   HALLMARK_ADIPOGENESIS 74610   \n3 Acaa2   HALLMARK_ADIPOGENESIS 52538   \n4 Acadl   HALLMARK_ADIPOGENESIS 11363   \n5 Acadm   HALLMARK_ADIPOGENESIS 11364   \n\n\nAnd now let’s add the gene set’s URL as well:\n\nresult %&gt;%\n  dplyr::left_join(tbl_geneset, by = \"geneset\")\n\n# Source:   SQL [5 x 4]\n# Database: sqlite 3.41.2 [:memory:]\n  element geneset               entrezid url                                    \n  &lt;chr&gt;   &lt;chr&gt;                 &lt;chr&gt;    &lt;chr&gt;                                  \n1 Abca1   HALLMARK_ADIPOGENESIS 11303    https://www.gsea-msigdb.org/gsea/msigd…\n2 Abcb8   HALLMARK_ADIPOGENESIS 74610    https://www.gsea-msigdb.org/gsea/msigd…\n3 Acaa2   HALLMARK_ADIPOGENESIS 52538    https://www.gsea-msigdb.org/gsea/msigd…\n4 Acadl   HALLMARK_ADIPOGENESIS 11363    https://www.gsea-msigdb.org/gsea/msigd…\n5 Acadm   HALLMARK_ADIPOGENESIS 11364    https://www.gsea-msigdb.org/gsea/msigd…\n\n\n\n\nPulling data into a BiocSet\nFinally, we can easily pull selected (or even all) gene sets into a Bioconductor BiocSet object for analysis in R. Importantly, the database does not require us to use R: e.g. python users can connect to the same SQLite database (e.g. using sqlalchemy ) and retrieve the information in whatever form is most useful to them.\nFor example, let’s retrieve all gene sets whose name ends in the letter N, store them in a list and create a BiocSet object.\n\ngene_set_list &lt;- with(\n  tbl_elementset %&gt;% \n    dplyr::filter(geneset %like% '%N') %&gt;%\n    collect(), \n  split(element, geneset)\n)\nes &lt;- BiocSet(gene_set_list)\n\nNext, we add gene set metadata to the es_set tibble, by joining it with the (richer) information in the database. This will add the url column.\n\nes &lt;- left_join_set(es, \n  tbl_geneset, by = c(set = \"geneset\"), \n  copy = TRUE\n)\nes_set(es)\n\n# A tibble: 8 × 2\n  set                                        url                                \n  &lt;chr&gt;                                      &lt;chr&gt;                              \n1 HALLMARK_ALLOGRAFT_REJECTION               https://www.gsea-msigdb.org/gsea/m…\n2 HALLMARK_APICAL_JUNCTION                   https://www.gsea-msigdb.org/gsea/m…\n3 HALLMARK_COAGULATION                       https://www.gsea-msigdb.org/gsea/m…\n4 HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION https://www.gsea-msigdb.org/gsea/m…\n5 HALLMARK_KRAS_SIGNALING_DN                 https://www.gsea-msigdb.org/gsea/m…\n6 HALLMARK_OXIDATIVE_PHOSPHORYLATION         https://www.gsea-msigdb.org/gsea/m…\n7 HALLMARK_PROTEIN_SECRETION                 https://www.gsea-msigdb.org/gsea/m…\n8 HALLMARK_UV_RESPONSE_DN                    https://www.gsea-msigdb.org/gsea/m…\n\n\nAnd finally, let’s also add the entrezid column from out database to the es_element table:\n\nes &lt;- left_join_element(es, \n  tbl_element, by = \"element\", \n  copy = TRUE\n)\nes_element(es)\n\n# A tibble: 1,233 × 2\n   element entrezid\n   &lt;chr&gt;   &lt;chr&gt;   \n 1 Aars    234734  \n 2 Abce1   24015   \n 3 Abi1    11308   \n 4 Ache    11423   \n 5 Acvr2a  11480   \n 6 Akt1    11651   \n 7 Apbb1   11785   \n 8 B2m     12010   \n 9 Bcat1   12035   \n10 Bcl10   12042   \n# ℹ 1,223 more rows\n\n\n\n\nSQL summary\n\nFor this example the effort required to transform the dataset into a set of three tables - the starting point for import into a relational database - was minimal.\nGiven the use case, e.g. management of a gene set collections, the number of times that data is added to the database is likely much smaller than the number of times it is queried. That makes it worth the effort to transform it once - and benefit from this upfront cost ever after.\nBecause we knew exactly which properties / annotations we wanted to capture in the database, defining the database tables and their relationships (e.g. the schema ) was not an obstacle, either.\nEnabling users to query the data using simple SQL or via a higher level abstraction like dplyr makes it accessible to a broader audience.\n\n\nDefining a schema is much harder when we deal with datasets that are less standardized, deeply nested, changing over time, etc."
  },
  {
    "objectID": "posts/geneset-sqlite-db/index.html#references",
    "href": "posts/geneset-sqlite-db/index.html#references",
    "title": "SQL and noSQL approaches to creating & querying databases (using R)",
    "section": "References",
    "text": "References\nIf you are new to working with databases, then you might find these two great books useful:\n\nSQL for Data Scientists: A Beginner’s Guide for Building Datasets for Analysis by Renee M. P. Teate is a great starting place to learn SQL. It mainly focusses on accessing existing databases.\nPractical SQL: A Beginner’s Guide to Storytelling with Data by Anthony DeBarros teaches readers how to create & populate a Postgres database, and how to index and search it effectively.\n\n\n\nSessionInfo\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-08-30\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package          * version   date (UTC) lib source\n P AnnotationDbi    * 1.62.2    2023-07-02 [?] Bioconductor\n P askpass            1.1       2019-01-13 [?] CRAN (R 4.3.0)\n P backports          1.4.1     2021-12-13 [?] CRAN (R 4.3.0)\n P Biobase          * 2.60.0    2023-05-08 [?] Bioconductor\n P BiocGenerics     * 0.46.0    2023-06-04 [?] Bioconductor\n P BiocIO             1.10.0    2023-05-08 [?] Bioconductor\n P BiocManager        1.30.22   2023-08-08 [?] CRAN (R 4.3.0)\n P BiocSet          * 1.14.0    2023-05-08 [?] Bioconductor\n P Biostrings         2.68.1    2023-05-21 [?] Bioconductor\n P bit                4.0.5     2022-11-15 [?] CRAN (R 4.3.0)\n P bit64              4.0.5     2020-08-30 [?] CRAN (R 4.3.0)\n P bitops             1.0-7     2021-04-24 [?] CRAN (R 4.3.0)\n P blob               1.2.4     2023-03-17 [?] CRAN (R 4.3.0)\n P cachem             1.0.8     2023-05-01 [?] CRAN (R 4.3.0)\n P cli                3.6.1     2023-03-23 [?] CRAN (R 4.3.0)\n P crayon             1.5.2     2022-09-29 [?] CRAN (R 4.3.0)\n R credentials        1.3.2     &lt;NA&gt;       [?] &lt;NA&gt;\n P DBI                1.1.3     2022-06-18 [?] CRAN (R 4.3.0)\n P dbplyr             2.3.3     2023-07-07 [?] CRAN (R 4.3.0)\n P DiagrammeR       * 1.0.10    2023-05-18 [?] CRAN (R 4.3.0)\n P digest             0.6.33    2023-07-07 [?] CRAN (R 4.3.0)\n P dm               * 1.0.6     2023-07-21 [?] CRAN (R 4.3.0)\n P dplyr            * 1.1.2     2023-04-20 [?] CRAN (R 4.3.0)\n P ellipsis           0.3.2     2021-04-29 [?] CRAN (R 4.3.0)\n P evaluate           0.21      2023-05-05 [?] CRAN (R 4.3.0)\n P fansi              1.0.4     2023-01-22 [?] CRAN (R 4.3.0)\n P fastmap            1.1.1     2023-02-24 [?] CRAN (R 4.3.0)\n P generics           0.1.3     2022-07-05 [?] CRAN (R 4.3.0)\n P GenomeInfoDb       1.36.1    2023-07-02 [?] Bioconductor\n P GenomeInfoDbData   1.2.10    2023-08-23 [?] Bioconductor\n P glue               1.6.2     2022-02-24 [?] CRAN (R 4.3.0)\n P htmltools          0.5.6     2023-08-10 [?] CRAN (R 4.3.0)\n P htmlwidgets        1.6.2     2023-03-17 [?] CRAN (R 4.3.0)\n P httpuv             1.6.11    2023-05-11 [?] CRAN (R 4.3.0)\n P httr               1.4.7     2023-08-15 [?] CRAN (R 4.3.0)\n P igraph             1.5.1     2023-08-10 [?] CRAN (R 4.3.0)\n P IRanges          * 2.34.1    2023-07-02 [?] Bioconductor\n P jsonlite         * 1.8.7     2023-06-29 [?] CRAN (R 4.3.0)\n P KEGGREST           1.40.0    2023-05-08 [?] Bioconductor\n P knitr              1.43      2023-05-25 [?] CRAN (R 4.3.0)\n P later              1.3.1     2023-05-02 [?] CRAN (R 4.3.0)\n P lifecycle          1.0.3     2022-10-07 [?] CRAN (R 4.3.0)\n P magrittr           2.0.3     2022-03-30 [?] CRAN (R 4.3.0)\n P memoise            2.0.1     2021-11-26 [?] CRAN (R 4.3.0)\n P mime               0.12      2021-09-28 [?] CRAN (R 4.3.0)\n P nodbi            * 0.9.6     2023-08-07 [?] CRAN (R 4.3.0)\n P ontologyIndex      2.11      2023-05-30 [?] CRAN (R 4.3.0)\n P openssl            2.1.0     2023-07-15 [?] CRAN (R 4.3.0)\n P org.Mm.eg.db     * 3.17.0    2023-08-23 [?] Bioconductor\n P pillar             1.9.0     2023-03-22 [?] CRAN (R 4.3.0)\n P pkgconfig          2.0.3     2019-09-22 [?] CRAN (R 4.3.0)\n P plyr               1.8.8     2022-11-11 [?] CRAN (R 4.3.0)\n P png                0.1-8     2022-11-29 [?] CRAN (R 4.3.0)\n P promises           1.2.1     2023-08-10 [?] CRAN (R 4.3.0)\n P purrr            * 1.0.2     2023-08-10 [?] CRAN (R 4.3.0)\n P R6                 2.5.1     2021-08-19 [?] CRAN (R 4.3.0)\n P RColorBrewer       1.1-3     2022-04-03 [?] CRAN (R 4.3.0)\n P Rcpp               1.0.11    2023-07-06 [?] CRAN (R 4.3.0)\n P RCurl              1.98-1.12 2023-03-27 [?] CRAN (R 4.3.0)\n   renv               1.0.2     2023-08-15 [1] CRAN (R 4.3.0)\n P rlang              1.1.1     2023-04-28 [?] CRAN (R 4.3.0)\n P rmarkdown          2.24      2023-08-14 [?] CRAN (R 4.3.0)\n P RSQLite          * 2.3.1     2023-04-03 [?] CRAN (R 4.3.0)\n P rstudioapi         0.15.0    2023-07-07 [?] CRAN (R 4.3.0)\n P S4Vectors        * 0.38.1    2023-05-08 [?] Bioconductor\n P sessioninfo        1.2.2     2021-12-06 [?] CRAN (R 4.3.0)\n P shiny              1.7.5     2023-08-12 [?] CRAN (R 4.3.0)\n P stringi            1.7.12    2023-01-11 [?] CRAN (R 4.3.0)\n P sys                3.4.2     2023-05-23 [?] CRAN (R 4.3.0)\n P tibble           * 3.2.1     2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr            * 1.3.0     2023-01-24 [?] CRAN (R 4.3.0)\n P tidyselect         1.2.0     2022-10-10 [?] CRAN (R 4.3.0)\n P utf8               1.2.3     2023-01-31 [?] CRAN (R 4.3.0)\n P uuid               1.1-1     2023-08-17 [?] CRAN (R 4.3.0)\n P vctrs              0.6.3     2023-06-14 [?] CRAN (R 4.3.0)\n P visNetwork         2.1.2     2022-09-29 [?] CRAN (R 4.3.0)\n P withr              2.5.0     2022-03-03 [?] CRAN (R 4.3.0)\n P xfun               0.40      2023-08-09 [?] CRAN (R 4.3.0)\n P xtable             1.8-4     2019-04-21 [?] CRAN (R 4.3.0)\n P XVector            0.40.0    2023-05-08 [?] Bioconductor\n P yaml               2.3.7     2023-01-23 [?] CRAN (R 4.3.0)\n P zlibbioc           1.46.0    2023-05-08 [?] Bioconductor\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/quarto-css/index.html",
    "href": "posts/quarto-css/index.html",
    "title": "Customizing my Quarto website",
    "section": "",
    "text": "Today I gave my blog a facelift, switching to a different theme and also including some custom CSS. Christian Gebhard’s tutorial was super helpful. Many thanks, Christian!\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/quarto-figure-size-and-layout/index.html",
    "href": "posts/quarto-figure-size-and-layout/index.html",
    "title": "Figure size, layout & tabsets with Quarto",
    "section": "",
    "text": "In this document, I am experimenting with various attributes that organize the layout, size and placement of figures of Quarto document. For more details, please check out the official documentation, especially the topics on figures and article layout.\n\n\n\n\n\n\nNote\n\n\n\nFor illustration, I am displaying both the code that generates a simple plot as well as the attributes that determine how it is rendered, e.g. the ::: tags interspersed with the code blocks, and the #| attributes within individual code cells. See the documentation on executable blocks for details.\n\n\nFirst, let’s generate a simple plot, so we can see the effect of different attributes on how it is rendered in subsequent code cells.\nTo start, we render the output without specifying any custom attributes, e.g. using the default settings for this Quarto website:\n\nlibrary(ggplot2)\ntheme_set(theme_linedraw(base_size = 14))\np &lt;- ggplot(mtcars, aes(x = mpg, y = drat)) + \n  geom_point(color = \"skyblue\", size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  theme(panel.grid = element_blank())\np\n\n\n\n\n\nWidth and height of individual figures\nThe fig-width and fig-height attributes specify the dimensions of the image file that is generated. The out-width attribute determines the size at which that image is displayed in the rendered HTML page.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\nFor example, the same image can be displayed at 50% of the width of the enclosing &lt;div&gt;.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"25%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\n\n\nLayout: columns and rows\nThe layout-ncol and layout-nrow attributes govern the placement of multiple figures within the same element. For example, we can place two figures next to each other, in two column.\nThe fig-align attributes specify the figure alignment within each column.\n\n\n\n\n\n\nTip\n\n\n\nThe out-width attribute is always relative to its enclosing element, e.g. here out-width: \"50%\" refers to half of the width of a column, not the page width.\n\n\n::: {layout-ncol=2}\n\n\n\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\n\n#| out-width: \"30%\"\n#| fig-align: \"right\"\n\n\n\n\np\n\n\n\n\n\n\n\n\n\np\n\n\n\n\n\n\n\n\n\n\n:::\n\n\nTabsets\nTabsets can be used to organize contents, e.g. by hiding content until the other clicks on the tab’s header.\nThe layout of the first tabset contains just one column and row.\n::: {.panel-tabset}\n\npanel 1panel 2\n\n\n\np\n\n\n\n\n\n\n\n\n\n\nThe second panel is subdivided into two columns. (Note the use of the :::: tag, nested within the ::: parent tag.)\n:::: {layout-ncol=2}\n\n\n\np\n\n\n\n\n\np\n\n\n\n\n\n\n\n::::\n\n\n\n\n\n\n:::\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nextflow-blast-tutorial/index.html",
    "href": "posts/nextflow-blast-tutorial/index.html",
    "title": "Learning nextflow: blasting multiple sequences",
    "section": "",
    "text": "Nextflow is both a reactive workflow framework and a domain-specific language (DSL). It is gaining lots of tracking in bioinformatics thanks in large part to the nf-core open source community that develops and publishes reusable workflows for many use cases.\nTo start learning nextflow, I worked through Andrew Severin’s excellent Creating a NextFlow workflow tutorial. (The tutorial follows the older DSL1 specification of nextflow, but only a few small modifications were needed to run it under DSL2.)\nThe DSL2 code I wrote is here and these are notes I took while working through the tutorial:\n\nTo make a variable a pipeline parameter prepend it with params., then specify them in the command line:\nmain.nf:\n#! /usr/bin/env nextflow\nparams.query=\"file.fasta\"\nprintln \"Querying file $params.query\"\nshell command:\nnextflow run main.nf --query other_file.fasta\nThe -log argument directs logging to the specified file.\nnextflow -log nextflo.log run main.nf \nTo clean up intermediate files automatically upon workflow completion, use the cleanup parameter within a profile.\nprofiles {\n  standard {\n      cleanup = true\n  }\n  debug {\n      cleanup = false\n  }\n}\n\nBy convention the standard profile is implicitly used when no other\nprofile is specified by the user.\nCleaning up intermediate files precludes the use of -resume.\n\nThe nextflow.config file sets the global parameters, e.g.\n\nprocess\nmanifest\nexecutor\nprofiles\ndocker\nsingularity\ntimeline\nreport\netc\n\nContents of the work folder for a nextflow task:\n\n.command.begin is the begin script if you have one\n.command.err is useful when it crashes.\n.command.run is the full nextflow pipeline that was run, this is helpful when trouble shooting a nextflow error rather than the script error.\n.command.sh shows what was run.\n.exitcode will have the exit code in it.\n\nDisplaying help messages\nmain.nf\ndef helpMessage() {\nlog.info \"\"\"\n      Usage:\n      The typical command for running the pipeline is as follows:\n      nextflow run main.nf --query QUERY.fasta --dbDir \"blastDatabaseDirectory\" --dbName \"blastPrefixName\"\n\n      Mandatory arguments:\n       --query                        Query fasta file of sequences you wish to BLAST\n       --dbDir                        BLAST database directory (full path required)\n       [...]\n\"\"\"\n}\n\n// Show help message\nif (params.help) {\n    helpMessage()\n    exit 0\n}\nshell command:\nnextflow run main.nf --help\nThe publishDir directive accepts arguments like mode and pattern to fine tune its behavior, e.g.\noutput:\nfile(\"${label}/short_summary.specific.*.txt\")\npublishDir \"${params.outdir}/BUSCOResults/${label}/\", mode: 'copy', pattern: \"${label}/short_summary.specific.*.txt\"\nDSL2 allows piping, e.g.\nworkflow {\n  res = Channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true) |\n      runBlast\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\nAdd a timeline report to the output with\ntimeline {\n    enabled = true\n    file = \"$params.outdir/timeline.html\"\n}\n(in nextflow.config).\nAdd a detailed execution report with\nreport {\nenabled = true\nfile = \"$params.outdir/report.html\"\n}\n(in nextflow.config).\nInclude a profile-specific configuration file\nnextflow.config\nprofiles {\n    slurm { includeConfig './configs/slurm.config' }\n}\nconfigs/slurm.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 24:00:00'\n}\nand use it via nextflow run main.nf -profile slurm\nSimilarly, refer to a test profile, specified in a separate file:\nnextflow.config\ntest { includeConfig './configs/test.config' }\nAdding a manifest to nextflow.config\nmanifest {\n    name = 'isugifNF/tutorial'\n    author = 'Andrew Severin'\n    homePage = 'www.bioinformaticsworkbook.org'\n    description = 'nextflow bash'\n    mainScript = 'main.nf'\n    version = '1.0.0'\n}\nUsing a label for a process allows granular control of a process’ configuration\nmain.nf\nprocess runBlast { \n    label 'blast'\n}\nnextflow.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 02:00:00'\n    withLabel: blast { module = 'blast-plus' }\n}\n\nThe label has to be placed before the input section.\n\nLoading a module specifically for a process\nprocess runBlast {\n\n    module = 'blast-plus'\n    publishDir \"${params.outdir}/blastout\"\n\n    input:\n    path queryFile from queryFile_ch\n    .\n    .\n    . // these three dots mean I didn't paste the whole process.\n}\nEnabling docker in the nextflow.config\ndocker { docker.enabled = true }\n\nThe docker container can be specified in the process, e.g.\n\ncontainer = 'ncbi/blast'\nor\ncontainer = `quay.io/biocontainers/blast/2.2.31--pl526he19e7b1_5`\n\nWe can include additional options to pass to the container as well:\n\ncontainerOptions = \"--bind $launchDir/$params.outdir/config:/augustus/config\"\nprojectDir refers to the directory where the main workflow script is located. (It used to be called baseDir.)\nRefering to local directories from within a docker container: create a channel\n\nWorking in containers, we need a way to pass the database file location directly into the runBlast process without the need of the local path.\n\nRepeating a process over each element of a channel with each: input repeaters\nTurning a queue channel into a value channel, which can be used multiple times.\n\nA value channel is implicitly created by a process when it is invoked with a simple value.\nA value channel is also implicitly created as output for a process whose inputs are all value channels.\nA queue channel can be converted into a value channel by returning a single value, using e.g. first, last, collect, count, min, max, reduce, sum, etc. For example: the runBlast process receives three inputs in the following example:\n\nthe queryFile_ch queue channel, with multiple sequences.\nthe dbDir_ch value channel, created by calling .first(), which is reused for all elements of queryFile_ch\nthe dbName_ch value channel, which is also reused for all elements of queryFile_ch\n\n\nworkflow {\n  channel.fromPath(params.dbDir).first()\n  .set { dbDir_ch }\n\n  channel.from(params.dbName).first()\n  .set { dbName_ch }\n\n  queryFile_ch = channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true)\n     res = runBlast(queryFile_ch, dbDir_ch, dbName_ch)\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\n\n\nAdditional resources\n\nSoftware Carpentry course\nNextflow cheat sheet\nAwesome nextflow\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/parquetArray/index.html",
    "href": "posts/parquetArray/index.html",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "",
    "text": "Previously, I learned how to export gene expression data from Bioconductor/R objects as parquet files. Today, I am exploring Hervé Pagès’s awesome DelayedArray Bioconductor package to represent data stored in parquet files as R matrices - retrieving only those subsets needed for downstream analysis.\n\nsuppressPackageStartupMessages({\n  library(\"arrow\")\n  library(\"dplyr\")\n  library(\"duckdb\")\n  library(\"edgeR\")\n  library(\"glue\")\n  library(\"rnaseqExamples\")\n  library(\"tibble\")\n  library(\"tidyr\")\n  library(\"DelayedArray\")\n  library(\"DESeq2\")\n  library(\"Mus.musculus\")\n})\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/parquetArray/index.html#tldr",
    "href": "posts/parquetArray/index.html#tldr",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "",
    "text": "Previously, I learned how to export gene expression data from Bioconductor/R objects as parquet files. Today, I am exploring Hervé Pagès’s awesome DelayedArray Bioconductor package to represent data stored in parquet files as R matrices - retrieving only those subsets needed for downstream analysis.\n\nsuppressPackageStartupMessages({\n  library(\"arrow\")\n  library(\"dplyr\")\n  library(\"duckdb\")\n  library(\"edgeR\")\n  library(\"glue\")\n  library(\"rnaseqExamples\")\n  library(\"tibble\")\n  library(\"tidyr\")\n  library(\"DelayedArray\")\n  library(\"DESeq2\")\n  library(\"Mus.musculus\")\n})"
  },
  {
    "objectID": "posts/parquetArray/index.html#exporting-gene-expression-data-to-parquet-files",
    "href": "posts/parquetArray/index.html#exporting-gene-expression-data-to-parquet-files",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Exporting gene expression data to parquet files",
    "text": "Exporting gene expression data to parquet files\nTo get some example RNA-seq data, I am loading two SummarizedExperiments (tau and sarm1)with bulk RNA-seq data from my rnaseqExamples R package.\nFirst, I extract the raw counts into data.frames with a short tidy() helper function. Afterward, I store this data in parquet files in a temporary directory.\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that I am not including sample annotations in the parquet files, as I found it useful to keep this (relatively small) set of metadata in a separate object, file or (e.g. SQLite) database.\n\n\n\n\n# Coerce a DGEList or a SummarizedExperiment into a tibble\ntidy &lt;- function(x) {\n  # extract raw counts\n  edgeR::calcNormFactors(x)$counts %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(\"feature_id\") %&gt;%\n    tidyr::pivot_longer(cols = colnames(x), names_to = \"sample_id\", \n                        values_to = \"count\")\n}\n\n# Store gene expression data for two mouse RNA-seq studies as parquet files\"\nout_dir &lt;- file.path(tempdir(), \"parquet\")\ndir.create(out_dir, showWarnings = FALSE)\nfor (dataset in c(\"tau\", \"sarm1\")) {\n  df &lt;- tidy(get(dataset))\n  df$study &lt;- dataset  # add a columns with the name of the experiment\n  arrow::write_parquet(\n    x = df,\n    sink = file.path(out_dir, paste0(dataset, \".parquet\"))\n  )\n}\ndir(out_dir)\n\n[1] \"sarm1.parquet\" \"tau.parquet\""
  },
  {
    "objectID": "posts/parquetArray/index.html#creating-the-parquetarrayseed-s4-class",
    "href": "posts/parquetArray/index.html#creating-the-parquetarrayseed-s4-class",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Creating the ParquetArraySeed S4 class",
    "text": "Creating the ParquetArraySeed S4 class\nNow, with the two parquet files in place, I am ready to follow Hervé’s excellent instructions to define my very own ParquetArraySeed S4 class, which can then be passed into the DelayedArray() constructor function (see below).\nTo keep things simple, my ParquetArraySeed class will only store a single attribute: the path to the directory containing the parquet files.\n\nsetClass(\"ParquetArraySeed\",\n    contains = \"Array\",\n    slots = c(\n        filepath = \"character\"\n    )\n)\n\nTo simplify the instantiation of new objects, I also define a constructor function, which also ensures that the absolute file path is stored.\n\n#' @importFrom tools file_path_as_absolute\nParquetArraySeed &lt;- function(filepath) {\n  filepath &lt;- tools::file_path_as_absolute(filepath)\n  new(\"ParquetArraySeed\", filepath = filepath)\n}"
  },
  {
    "objectID": "posts/parquetArray/index.html#essential-methods",
    "href": "posts/parquetArray/index.html#essential-methods",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Essential methods",
    "text": "Essential methods\nTo power a DelayedArray object, I need to define at least three different S4 methods for my new class:\n\ndim() - returning an integer vector with the dimensions\ndimnames() - returning a list of character vectors with the dimension names (if any), e.g. the row and column names of a matrix.\nextract_array() - returning an ordinary array for a set of indices (see below), e.g. a subset of the dataset to realize in memory.\n\n\ndim and dimnames methods\nTo query the set of parquet files, I am using duckdb to retrieve the unique sample and feature identifiers. In case this information is useful later, I am caching the return values with memoise. That’s handy because I can simply return the lengths of the dimensions names via the dim() method - without accessing the same files again.\n\n.unique_values &lt;- function(x, column, con = NULL, suffix = \".parquet\") {\n  if (is.null(con)) {\n    con &lt;- duckdb::dbConnect(duckdb::duckdb())\n    on.exit(duckdb::dbDisconnect(con, shutdown=TRUE))\n  }\n  data_dir &lt;- file.path(x@filepath, paste0(\"*\", suffix))\n  dbGetQuery(\n    con = con,\n    glue_sql(\n      \"SELECT DISTINCT {`column`} \n       FROM read_parquet({data_dir}) \n       ORDER BY {`column`}\", \n     .con = con)\n  )[, 1]\n}\nunique_values &lt;- memoise::memoise(.unique_values)\n\nsetMethod(\"dimnames\", \"ParquetArraySeed\", function(x) {\n  samples &lt;- unique_values(x, column = \"sample_id\")\n  features &lt;- unique_values(x, column = \"feature_id\")\n  list(features, samples)\n})\n\nsetMethod(\"dim\", \"ParquetArraySeed\", function(x) {\n  lengths(dimnames(x))\n})\n\n\n\nThe extract_array method\nI also use duckdb() to retrieve the actual data for a subset of features, a subset of samples - or both. In case users only specify one or the other, e.g. by passing an index with NULL values, I explicitely define four different SQL queries returning:\n\nThe full dataset (when index = list(NULL, NULL)),\nAll features for a subset of samples,\nSelected features for all samples, or\nSelected features for selected samples.\n\nFinally, I pivot the data into a feature x sample matrix.\n\n.get_data &lt;- function(x, index, con = NULL, suffix = \".parquet\") {\n  if (is.null(con)) {\n    con &lt;- duckdb::dbConnect(duckdb::duckdb())\n    on.exit(duckdb::dbDisconnect(con, shutdown=TRUE))\n  }\n  data_dir &lt;- file.path(x@filepath, paste0(\"*\", suffix))\n  \n  # match indices to feature and sample identifiers\n  dims &lt;- dimnames(x)\n  keep_features &lt;- dims[[1]][index[[1]]]\n  \n  # no indices =&gt; return the full dataset\n  if (is.null(index[[1]]) & is.null(index[[2]])) {\n   dataset &lt;- dbGetQuery(\n      con = con,\n      glue_sql(\n        \"SELECT feature_id, sample_id, count\n       FROM read_parquet({data_dir})\", \n       .con = con)\n   )\n   keep_features &lt;- unique(dataset$feature_id)\n   keep_samples &lt;- unique(dataset$sample_id)\n   # no sample index =&gt; return all samples\n  } else if (!is.null(index[[1]]) && is.null(index[[2]])) {\n    keep_features &lt;- dims[[1]][index[[1]]]\n    dataset &lt;- dbGetQuery(\n      con = con,\n      glue_sql(\n        \"SELECT feature_id, sample_id, count\n       FROM read_parquet({data_dir}) \n       WHERE feature_id IN ({keep_features*})\", \n       .con = con)\n    )\n    keep_samples &lt;- unique(dataset$sample_id)\n    # no feature index =&gt; return all features\n  } else if (is.null(index[[1]]) && !is.null(index[[2]])) {\n    keep_samples &lt;- dims[[2]][index[[2]]]\n    dataset &lt;- dbGetQuery(\n      con = con,\n      glue_sql(\n        \"SELECT feature_id, sample_id, count\n       FROM read_parquet({data_dir}) \n       WHERE sample_id IN ({keep_samples*})\", \n       .con = con)\n    )\n    keep_features &lt;- unique(dataset$feature_id)\n  } else {\n    keep_features &lt;- dims[[1]][index[[1]]]\n    keep_samples &lt;- dims[[2]][index[[2]]]\n    dataset &lt;- dbGetQuery(\n      con = con,\n      glue_sql(\n        \"SELECT feature_id, sample_id, count\n       FROM read_parquet({data_dir}) \n       WHERE feature_id IN ({keep_features*})AND sample_id IN ({keep_samples*})\n       ORDER BY sample_id, feature_id\", \n       .con = con)\n    ) \n  }\n  # pivot the count data into a regular matrix\n  m &lt;- matrix(\n    data = NA_integer_, \n    nrow = length(keep_features),\n    ncol = length(keep_samples),\n    dimnames = list(keep_features, keep_samples))\n  matrix_index &lt;- cbind(\n    match(dataset$feature_id, row.names(m)), \n    match(dataset$sample_id, colnames(m))\n  )\n  m[matrix_index] &lt;- dataset$count\n  return(m)\n}\n\n.extract_array_from_ParquetArraySeed &lt;- function(x, index) {\n  .get_data(x = x, index = index)\n}\n\nsetMethod(\"extract_array\", \"ParquetArraySeed\", \n          .extract_array_from_ParquetArraySeed)"
  },
  {
    "objectID": "posts/parquetArray/index.html#creating-a-first-parquetarrayseed-object",
    "href": "posts/parquetArray/index.html#creating-a-first-parquetarrayseed-object",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Creating a first ParquetArraySeed object",
    "text": "Creating a first ParquetArraySeed object\nWith these three methods in place, I can instantiate my first ParquetArraySeed object, which is suitable as input to the DelayedArray constructor from the eponymous R package:\n\nseed &lt;- ParquetArraySeed(out_dir)\nda &lt;- DelayedArray(seed)\nda\n\n&lt;53801 x 48&gt; DelayedMatrix object of type \"integer\":\n                   DRN-10415 DRN-10418 DRN-10421 ... DRN-16866 DRN-16867\nENSMUSG00000000001      2754      2764      2907   .      1214      1002\nENSMUSG00000000003         0         0         0   .         0         0\nENSMUSG00000000028        54        61        66   .        39        57\nENSMUSG00000000031         0         0         3   .        16        17\nENSMUSG00000000037         9        50        26   .        58        78\n               ...         .         .         .   .         .         .\nENSMUSG00000116519         0         0         0   .         0         0\nENSMUSG00000116520         0         0         0   .         0         0\nENSMUSG00000116521         0         0         0   .         0         0\nENSMUSG00000116525         3         1         2   .        65        44\nENSMUSG00000116528         0         0         0   .         0         0\n                   DRN-16868\nENSMUSG00000000001       921\nENSMUSG00000000003         0\nENSMUSG00000000028        38\nENSMUSG00000000031        22\nENSMUSG00000000037        33\n               ...         .\nENSMUSG00000116519         0\nENSMUSG00000116520         0\nENSMUSG00000116521         0\nENSMUSG00000116525        45\nENSMUSG00000116528         0\n\n\nWhen the object is printed, we are presented with the first couple of rows and columns, but most of the data remains on disk (e.g. has not been retrieved from the parquet files).\nSubsetting the DelayedArray works as expected, e.g. to retain only samples from the tau study:\n\n# subset to one dataset - using gene and sample identifiers\nda[row.names(tau), colnames(tau)]\n\n&lt;53801 x 32&gt; DelayedMatrix object of type \"integer\":\n                   DRN-10415 DRN-10418 DRN-10421 ... DRN-10502 DRN-10505\nENSMUSG00000000001      2754      2764      2907   .      2260      3273\nENSMUSG00000000003         0         0         0   .         0         0\nENSMUSG00000000028        54        61        66   .        16        82\nENSMUSG00000000031         0         0         3   .         0         0\nENSMUSG00000000037         9        50        26   .        11        11\n               ...         .         .         .   .         .         .\nENSMUSG00000116519         0         0         0   .         0         0\nENSMUSG00000116520         0         0         0   .         0         0\nENSMUSG00000116521         0         0         0   .         0         0\nENSMUSG00000116525         3         1         2   .         1         3\nENSMUSG00000116528         0         0         0   .         0         0\n                   DRN-10508\nENSMUSG00000000001      2758\nENSMUSG00000000003         0\nENSMUSG00000000028        47\nENSMUSG00000000031         0\nENSMUSG00000000037         6\n               ...         .\nENSMUSG00000116519         0\nENSMUSG00000116520         0\nENSMUSG00000116521         0\nENSMUSG00000116525         1\nENSMUSG00000116528         0"
  },
  {
    "objectID": "posts/parquetArray/index.html#creating-a-parquet-backed-summarizedexperiment",
    "href": "posts/parquetArray/index.html#creating-a-parquet-backed-summarizedexperiment",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Creating a parquet-backed SummarizedExperiment",
    "text": "Creating a parquet-backed SummarizedExperiment\nFor downstream analyses, it is useful to combine sample- and feature-annotations with the gene expression counts in a single SummarizedExperiment.\nLet’s extract the sample metadata from the original data objects and add a study column.\n\npdata &lt;- lapply(c(\"tau\", \"sarm1\"), \\(x) {\n  col_data &lt;- colData(get(x))\n  col_data$study &lt;- x\n  return(col_data)\n  })\npdata &lt;- Reduce(rbind, pdata)\n\nThe original datasets only provide ensembl gene identifiers, so let’s retrieve the corresponding gene symbols and entrez identifiers from the Mus.musculus Bioconductor annotation package.\n\n# add Entrez identifiers and gene symbols for both human and mouse genes\nfdata &lt;- DataFrame(\n    ensembl = row.names(da),\n    entrez = mapIds(Mus.musculus, \n                    row.names(da), \n                    column = \"ENTREZID\", \n                    keytype = \"ENSEMBL\", \n                    multiVals = \"first\"),\n    symbol = mapIds(Mus.musculus, \n                    row.names(da), \n                    column = \"SYMBOL\", \n                    keytype = \"ENSEMBL\", \n                    multiVals = \"first\"),\n    row.names = row.names(da)\n)\n\nNow we are ready to compose meta- and gene expression data into a single object:\n\nse &lt;- SummarizedExperiment(\n  assays = list(counts = da), \n  rowData = fdata[row.names(da), ],\n  colData = pdata[colnames(da),,drop=FALSE ]\n)\nse\n\nclass: SummarizedExperiment \ndim: 53801 48 \nmetadata(0):\nassays(1): counts\nrownames(53801): ENSMUSG00000000001 ENSMUSG00000000003 ...\n  ENSMUSG00000116525 ENSMUSG00000116528\nrowData names(3): ensembl entrez symbol\ncolnames(48): DRN-10415 DRN-10418 ... DRN-16867 DRN-16868\ncolData names(2): group study\n\ntable(se$study)\n\n\nsarm1   tau \n   16    32 \n\n\nAt this point, we still haven’t retrieved the actual counts from the parquet files, e.g. they information is still on disk. It will automatically be retrieved when downstream functions coerce the DelayedArray into a regular matrix. For example, edgeR’s calcNormFactors function accepts a SummarizedExperiment and returns a DGEList, ready for exploratory analysis e.g. multi-dimensional scaling.\n\ny &lt;- edgeR::calcNormFactors(se[, se$study == \"tau\"])\ny$samples$genotype &lt;- factor(\n  ifelse(grepl(\"WT_\", y$samples$group), \"WT\", \"Transgenic\")\n)\ncolors &lt;- palette.colors(n = nlevels(y$samples$genotype), \n                         palette = \"Set1\")[as.integer(y$samples$genotype)]\nlimma::plotMDS(y, col = colors, pch = 19)\nlegend(\"topright\", legend = levels(y$samples$genotype), bty = \"n\", pch = 19, \n       col = palette.colors(n = nlevels(y$samples$genotype), palette = \"Set1\"),\n       y.intersp=2)"
  },
  {
    "objectID": "posts/parquetArray/index.html#summary",
    "href": "posts/parquetArray/index.html#summary",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Summary",
    "text": "Summary\nBecause the DelayedArray package is so thoughtfully designed, it was surprisingly straightforward to represent data stored in parquet files as a matrix. This way, I can leverage familiar Bioconductor tools and take advantage of the language-agnostic file format.\nA few directions and caveats that I haven’t explored, yet:\n\nIn this example, I am generating one parquet file for each of the studies, e.g. I am manually partitioning the data by study. When I want to retrieve all data for one of the studies, I could simply parse only the corresponding parquet file, instead of querying & filtering all files in the directory (as a Dataset ).\nMy parquet files are stored on the local system. Accessing them in a cloud storage location (e.g. on AWS S3) is supported by duckdb as well, but network speed will likely become limiting. I will explore the use of AWS Athena, a cloud-based SQL database, in a future post, as shown e.g. in this AWS blog post.\nThe parquet files I generated here are relatively small, e.g. ~ 5 Mb, reflecting the relatively small size of the example bulk RNA-seq datasets. The arrow project recommends avoiding accessing many files smaller than 20MB, but that’s likely not a problem until I need to handle thousands of studies.\nDisk-backed arrays are great if computations can be performed on small chunks of data, e.g. to visualize a subset of feature and / or samples. When it is necessary to realize the full dataset in memory, the DelayedArray returns a conventional matrix - e.g. sparse representations don’t seem to be supported right now (as far as I can tell).\nFinally, I have only implemented three essential methods, but have not touched on any optimized backend-specific methods. For example, we could issue a SQL query to calculate max or colSums values instead of loading the full data matrix into our R session. (This might be especially interesting if the database engine is running on a remote server.)\n\nLots more to learn - and a big thank you to Hervé Pagès and the entire Bioconductor team!"
  },
  {
    "objectID": "posts/parquetArray/index.html#reproducibility",
    "href": "posts/parquetArray/index.html#reproducibility",
    "title": "Adventures with parquet II: Implementing the parquetArraySeed S4 class",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSession Information\n\n\nsessioninfo::session_info(\"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Debian GNU/Linux 12 (bookworm)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-09-05\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package                            * version    date (UTC) lib source\n P abind                              * 1.4-5      2016-07-21 [?] CRAN (R 4.3.1)\n P AnnotationDbi                      * 1.62.2     2023-07-02 [?] Bioconductor\n P arrow                              * 13.0.0     2023-08-30 [?] CRAN (R 4.3.1)\n P Biobase                            * 2.60.0     2023-04-25 [?] Bioconductor\n P BiocGenerics                       * 0.46.0     2023-04-25 [?] Bioconductor\n P DBI                                * 1.1.3      2022-06-18 [?] CRAN (R 4.3.1)\n P DelayedArray                       * 0.26.7     2023-07-28 [?] Bioconductor\n P DESeq2                             * 1.40.2     2023-06-23 [?] Bioconductor\n P dplyr                              * 1.1.2      2023-04-20 [?] CRAN (R 4.3.1)\n P duckdb                             * 0.8.1-2    2023-08-25 [?] CRAN (R 4.3.1)\n P edgeR                              * 3.42.4     2023-05-31 [?] Bioconductor\n P GenomeInfoDb                       * 1.36.1     2023-06-21 [?] Bioconductor\n P GenomicFeatures                    * 1.52.2     2023-08-25 [?] Bioconductor\n P GenomicRanges                      * 1.52.0     2023-04-25 [?] Bioconductor\n P glue                               * 1.6.2      2022-02-24 [?] CRAN (R 4.3.1)\n P GO.db                              * 3.17.0     2023-09-04 [?] Bioconductor\n P IRanges                            * 2.34.1     2023-06-22 [?] Bioconductor\n P limma                              * 3.56.2     2023-06-04 [?] Bioconductor\n P Matrix                             * 1.5-4.1    2023-05-18 [?] CRAN (R 4.3.1)\n P MatrixGenerics                     * 1.12.3     2023-07-30 [?] Bioconductor\n P matrixStats                        * 1.0.0      2023-06-02 [?] CRAN (R 4.3.1)\n P Mus.musculus                       * 1.3.1      2023-09-04 [?] Bioconductor\n P org.Mm.eg.db                       * 3.17.0     2023-09-04 [?] Bioconductor\n P OrganismDbi                        * 1.42.0     2023-04-25 [?] Bioconductor\n P rnaseqExamples                     * 0.0.0.9000 2023-09-04 [?] Github (tomsing1/rnaseq-Examples@ac35304)\n P S4Arrays                           * 1.0.5      2023-07-24 [?] Bioconductor\n P S4Vectors                          * 0.38.1     2023-05-02 [?] Bioconductor\n P SummarizedExperiment               * 1.30.2     2023-06-06 [?] Bioconductor\n P tibble                             * 3.2.1      2023-03-20 [?] CRAN (R 4.3.1)\n P tidyr                              * 1.3.0      2023-01-24 [?] CRAN (R 4.3.1)\n P TxDb.Mmusculus.UCSC.mm10.knownGene * 3.10.0     2023-09-04 [?] Bioconductor\n\n [1] /home/sandmann/repositories/blog/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/sandmann/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html",
    "href": "posts/ParquetMatrix/index.html",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "",
    "text": "Today, I learned how to store single-cell RNA-seq data in a parquet file, created the ParquetMatrix class to retrieve them, and I started to compare the performance of reading from parquet and HDF5 files.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#tldr",
    "href": "posts/ParquetMatrix/index.html#tldr",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "",
    "text": "Today, I learned how to store single-cell RNA-seq data in a parquet file, created the ParquetMatrix class to retrieve them, and I started to compare the performance of reading from parquet and HDF5 files."
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#introduction",
    "href": "posts/ParquetMatrix/index.html#introduction",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Introduction",
    "text": "Introduction\nPreviously, I learned how to create Bioconductor S4 objects based on parquet files. As Aaron Lun pointed out Parquet format is similar to 10X Genomics’ HDF5 format for sparse matrices. This motivated me to look into storing single-cell RNA-seq data, which is very sparse as most genes are not detected in any given cell.\nToday, I am experimenting with coercing data from parquet files into sparse matrices and using them as a back-end for Hervé Pagès’s great DelayedArray S4 class. I used the awesome HDF5Array package to guide me, and learned more about arrow’s dictionary type.\nIn the end, I was positively surprised that my crude ParquetMatrix class was able to read either the full data matrix or a random subset of counts from a parquet file even a little faster than from HDF5-backed original dataset.\nAs a bonus, ParquetMatrix objects can be instantiated from local parquet files or from cloud storage (S3).\n\nsuppressPackageStartupMessages({\n  library(\"arrow\")\n  library(\"dplyr\")\n  library(\"fs\")\n  library(\"microbenchmark\")\n  library(\"rhdf5\")\n  library(\"tibble\")\n  library(\"tidyr\")\n  library(\"DelayedArray\")\n  library(\"Matrix\")\n  library(\"S4Vectors\")\n  library(\"SingleCellExperiment\")\n  library(\"TENxPBMCData\")\n})"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#retrieving-an-example-single-cell-rna-seq-dataset",
    "href": "posts/ParquetMatrix/index.html#retrieving-an-example-single-cell-rna-seq-dataset",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Retrieving an example Single-cell RNA-seq dataset",
    "text": "Retrieving an example Single-cell RNA-seq dataset\nAs an example dataset, I am using single-cell RNA-seq included as the pbmc4k dataset in the TENxPBMCData Bioconductor package, with counts from Peripheral Blood Mononuclear Cells (PBMCs) collected from a single donor.\nThe TENxPBMCData() function retrieves them from ExperimentHub and caches them on the local system the first time the object is loaded 1.\nIt downloads three files:\n\nAn HDF5 file with counts (dense assay)\nAn RDS file with row (= gene) annotations\nAn RDS file with column (= cell) annotations\n\n\ntenx_pbmc4k &lt;- suppressMessages(TENxPBMCData(dataset = \"pbmc4k\"))\ntenx_pbmc4k\n\nclass: SingleCellExperiment \ndim: 33694 4340 \nmetadata(0):\nassays(1): counts\nrownames(33694): ENSG00000243485 ENSG00000237613 ... ENSG00000277475\n  ENSG00000268674\nrowData names(3): ENSEMBL_ID Symbol_TENx Symbol\ncolnames: NULL\ncolData names(11): Sample Barcode ... Individual Date_published\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nThe TENxPBMCData() function combines counts with annotations, and returns a SingleCellExperiment with a sparse HDF5-backed DelayedMatrix in the counts slot and gene and cell annotations as rowData and colData DataFrames, respectively. It contains counts for 33694 genes in 4340 cells.\nThe counts are retrieved from the HDF5 file on demand, and the full matrix only uses 2.3 Mb of memory.\n\ncounts(tenx_pbmc4k)[1:10, 1:10]\n\n&lt;10 x 10&gt; sparse DelayedMatrix object of type \"integer\":\n                 [,1]  [,2]  [,3]  [,4] ...  [,7]  [,8]  [,9] [,10]\nENSG00000243485     0     0     0     0   .     0     0     0     0\nENSG00000237613     0     0     0     0   .     0     0     0     0\nENSG00000186092     0     0     0     0   .     0     0     0     0\nENSG00000238009     0     0     0     0   .     0     0     0     0\nENSG00000239945     0     0     0     0   .     0     0     0     0\nENSG00000239906     0     0     0     0   .     0     0     0     0\nENSG00000241599     0     0     0     0   .     0     0     0     0\nENSG00000279928     0     0     0     0   .     0     0     0     0\nENSG00000279457     0     0     0     0   .     0     0     0     1\nENSG00000228463     0     0     0     0   .     0     0     0     0\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe underlying HDF5 file only contains the gene x cell count matrix.\n\nrhdf5::h5ls(path(counts(tenx_pbmc4k)))\n\n  group   name       otype  dclass          dim\n0     / counts H5I_DATASET INTEGER 33694 x 4340\n\n\nThe\n\n\n\nLike most single-cell RNA-seq datasets, the data is very sparse: For example, 97.5% of the genes in the first four samples have zero counts."
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#handling-sparse-count-data-in-memory",
    "href": "posts/ParquetMatrix/index.html#handling-sparse-count-data-in-memory",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Handling sparse count data in memory",
    "text": "Handling sparse count data in memory\nLet’s load all counts into memory as a sparse dgCMatrix defined in the Matrix R package.\n\nm &lt;- as(counts(tenx_pbmc4k), \"dgCMatrix\")\ndim(m)\n\n[1] 33694  4340\n\n\nBecause only non-zero counts need to be represented, the sparse dgCMatrix matrix m is still relatively small, occupying 67.9 Mb of memory.\n\n\n\n\n\n\nNote\n\n\n\nInternally, a dgCMatrix is represented in the (sorted) compressed sparse column format (CSC). Each non-zero value is stored as three numbers:\n\ni: the row index\nj: the column pointer\nx: the value (= count)\n\n\n\nWe can extract this internal representation into a tall, thin data.frame with the Matrix::summary() function.\n\ndf &lt;- as.data.frame(\n  Matrix::summary(\n    as(counts(tenx_pbmc4k), \"dgCMatrix\")\n  ), row.names = NULL\n)\ndf$x &lt;- as.integer(df$x)\n\nThis data.frame only contains non-zero values, e.g. 19773 of the original 33694 genes and 4340 of the original 4340 cells. No counts were detected for the remaining (e.g. missing) genes and cells.\nThe data.frame requires 65.5 Mb of memory.\n\n\n\n\n\n\nNote\n\n\n\n\n\nInstead of a regular data.frame we can also work with arrow::Table objects. Here, I explicitly create a Table with three 32 bit integers. (Because single-cell RNA-seq counts are always positive and we have a good idea of their upper bound, I am using an unsigned 32 bit integer type for the x column, allowing values between 0 and 4,294,967,295.)\n\na_tbl &lt;- arrow::as_arrow_table(\n  df, \n  schema = arrow::schema(\n    i = int32(),\n    j = int32(),\n    x = uint32()\n  ))\na_tbl\n\nTable\n5727695 rows x 3 columns\n$i &lt;int32&gt;\n$j &lt;int32&gt;\n$x &lt;uint32&gt;\n\nSee $metadata for additional Schema metadata\n\n\nCreating an arrow::Table object appears to consume little additional memory (0.5 Kb) as it is managed by arrow and not R.\n\n\n\nAlternatively, I can also encode the row-names (= gene identifiers) and column-names (= cell barcodes) in the data.frame. Because each of the identifiers appears multiple times, they are best represented as factors.\nBy including all of the gene- and barcode identifiers as factor levels, including those genes / barcodes that were not detected (e.g. had a total of zero counts), I retain information about them as well.\n\ndf$i &lt;- factor(row.names(tenx_pbmc4k)[df$i], levels = row.names(tenx_pbmc4k))\ndf$j = factor(tenx_pbmc4k$Barcode[df$j], levels = tenx_pbmc4k$Barcode)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBy the way, arrow tables (and parquet files) have an equivalent data type and R factors stored as type dictionary.\n\narrow::as_arrow_table(\n  df\n)\n\nTable\n5727695 rows x 3 columns\n$i &lt;dictionary&lt;values=string, indices=int32&gt;&gt;\n$j &lt;dictionary&lt;values=string, indices=int16&gt;&gt;\n$x &lt;int32&gt;\n\nSee $metadata for additional Schema metadata"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#writing-parquet-files",
    "href": "posts/ParquetMatrix/index.html#writing-parquet-files",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Writing parquet files",
    "text": "Writing parquet files\nNext, let’s store the df data.frame in a (single) parquet file on the local filesystem.\n\nparquet_file &lt;- tempfile(fileext = \".parquet\")\narrow::write_parquet(x = df, sink = parquet_file, use_dictionary = TRUE)\n\nThis yields a parquet file that’s 15.6M in size. (For comparison, the original HDF5 file was 12.8M in size, but did contained neither the gene- nor cel (=barcode) identifiers.)\n\nhead(arrow::read_parquet(parquet_file))\n\n# A tibble: 6 × 3\n  i               j                      x\n  &lt;fct&gt;           &lt;fct&gt;              &lt;int&gt;\n1 ENSG00000187608 AAACCTGAGAAGGCCT-1     1\n2 ENSG00000116251 AAACCTGAGAAGGCCT-1     4\n3 ENSG00000177674 AAACCTGAGAAGGCCT-1     1\n4 ENSG00000117118 AAACCTGAGAAGGCCT-1     1\n5 ENSG00000179051 AAACCTGAGAAGGCCT-1     1\n6 ENSG00000053371 AAACCTGAGAAGGCCT-1     1"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#a-parquet-file-backed-delayedarray",
    "href": "posts/ParquetMatrix/index.html#a-parquet-file-backed-delayedarray",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "A parquet-file backed DelayedArray",
    "text": "A parquet-file backed DelayedArray\nThe original tenx_pbmc4k object represents the counts as a sparse DelayedMatrix object of type \"integer\", e.g. it copies the data from the HDF5 file into memory only when it is necessary.\nHere, I will reproduce this behavior with my parquet file by implementing a minimal seed for the DelayedArray S4 class.\n\nThe ParquetArraySeed S4 class\nUnder the hood, each DelayedMatrix object contains a seed object. For example, the tenx_pbmc4k object contains a HDF5ArraySeed seed:\n\nseed &lt;- seed(counts(tenx_pbmc4k))\nclass(seed)\nis_sparse(seed)\n\nLet’s create a similar ParquetArraySeed class that inherits from the Array class.\n\nsetClass(\"ParquetArraySeed\",\n    contains = \"Array\",\n    slots = c(\n        filepath = \"character\",\n        dim = \"integer\",\n        dimnames = \"list\"\n    )\n)\n\nTo power a DelayedArray object, I need to define at least three different S4 methods for my new class:\n\ndim() - returning an integer vector with the dimensions\ndimnames() - returning a list of character vectors with the dimension names (if any), e.g. the row and column names of the matrix.\nextract_array() - returning an ordinary array for a set of indices (see below), e.g. a subset of the dataset to realize in memory.\n\nLet’s start with the dim() and dimnames() methods. We will rely on the constructor function (see below) to retrieve the unique row and column names from the parquet files, and then populate the @dimnames and @dim slots for future reference.\n\nsetMethod(\"dimnames\", \"ParquetArraySeed\", function(x) x@dimnames)\nsetMethod(\"dim\", \"ParquetArraySeed\", function(x) x@dim)\n\nI also create a constructor function, which precalculates the dimensions of the dataset and populates the @dim and @dimnames slots.\n\n.get_dimnames &lt;- function(filepath) {\n  list(\n    levels(read_parquet(filepath, col_select = \"i\")[[1]]),\n    levels(read_parquet(filepath, col_select = \"j\")[[1]])\n  )\n}\n.get_dim &lt;- function(filepath) {\n  n_i &lt;- read_parquet(filepath, col_select = \"i\") %&gt;%\n    collect() %&gt;%\n    pull(i) %&gt;%\n    nlevels()\n  n_j &lt;- read_parquet(filepath, col_select = \"j\") %&gt;%\n    collect() %&gt;%\n    pull(j) %&gt;%\n    nlevels()\n  c(n_i, n_j)\n}\n\nParquetArraySeed &lt;- function(filepath, dim = NULL, dimnames = NULL) {\n  if (is.null(dimnames)) {\n    dimnames &lt;- .get_dimnames(filepath)\n  }\n  if (is.null(dim)) {\n    if (is.null(dimnames)) {\n      dim &lt;- .get.dim(filepath)\n    } else {\n      dim &lt;- lengths(dimnames)\n    }\n  }\n  x &lt;- new(\"ParquetArraySeed\", filepath = filepath, dim = .get_dim(filepath), \n           dimnames = dimnames)\n  return(x)\n}\n\nFinally, I need a function that subsets the dataset to a user-specified set of genes and / or cells. I also need to ensure that passing an empty query returns the full dataset. (In a previous post I used duckdb to queries parquet files; here I am using arrow’s dplyr bindings instead.)\n\n.extract_array_from_ParquetArraySeed &lt;- function(x, index) {\n  if (identical(index, list(integer(0), integer(0)))) {\n    # zero indices =&gt; return empty matrix\n    return(matrix(0L, nrow = 0, ncol = 0))\n  } \n  keep_i &lt;- seq.int(dim(x)[1])\n  keep_j &lt;- seq.int(dim(x)[2])\n  \n  # to simplify lookups, I convert the arrow dictionary to integer indices \n  arrow_tbl &lt;- read_parquet(x@filepath, as_data_frame = FALSE)\n  arrow_tbl$i &lt;- Array$create(arrow_tbl$i)$indices() + 1\n  arrow_tbl$j &lt;- Array$create(arrow_tbl$j)$indices() + 1\n  \n  if (is.null(index[[1]]) & is.null(index[[2]])) {\n  # NULL indices =&gt; return the full dataset\n    dataset &lt;- arrow_tbl\n  } else if (!is.null(index[[1]]) && is.null(index[[2]])) {\n    # no column index =&gt; return all columns\n    keep_i &lt;- index[[1]]\n    dataset &lt;- filter(arrow_tbl, i %in% keep_i)\n  } else if (is.null(index[[1]]) && !is.null(index[[2]])) {\n    # no row index =&gt; return all rows\n    keep_j &lt;- index[[2]]\n    dataset &lt;- filter(arrow_tbl, j %in% keep_j)\n  } else {\n    # return requested rows and requested columns\n    keep_i &lt;-index[[1]]\n    keep_j &lt;- index[[2]]\n    dataset &lt;- filter(arrow_tbl, i %in% keep_i, j %in% keep_j)\n  }\n  # pivot the count data into a matrix\n  dataset &lt;- collect(dataset)\n  m &lt;- matrix(\n    data = 0L,\n    nrow = length(keep_i),\n    ncol = length(keep_j),\n    dimnames = list(dimnames(x)[[1]][keep_i], \n                    dimnames(x)[[2]][keep_j])\n  )\n  matrix_index &lt;- cbind(\n    match(dataset[[\"i\"]], keep_i),\n    match(dataset[[\"j\"]], keep_j)\n  )\n  m[matrix_index] &lt;- dataset$x\n  return(m)\n}\n\nsetMethod(\"extract_array\", \"ParquetArraySeed\", \n          .extract_array_from_ParquetArraySeed)"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#creating-a-first-parquet-backed-delayedarray-object",
    "href": "posts/ParquetMatrix/index.html#creating-a-first-parquet-backed-delayedarray-object",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Creating a first parquet-backed DelayedArray object",
    "text": "Creating a first parquet-backed DelayedArray object\nWith these three methods in place, I can instantiate my first ParquetArraySeed object, which is suitable as input to the DelayedArray constructor from the eponymous R package.\n\nseed &lt;- ParquetArraySeed(parquet_file)\nda &lt;- DelayedArray(seed)\nclass(da)\n\n[1] \"DelayedMatrix\"\nattr(,\"package\")\n[1] \"DelayedArray\"\n\n\nNext, let’s test different ways of subsetting our DelayedArray and make sure the returned dimensions match those of the requested indices:\n\nstopifnot(\n  identical(dim(da[1:10, 1:100]), c(10L, 100L)),\n  identical(dim(da[1:10, ]), c(10L, ncol(da))),\n  identical(dim(da[, 1:10]), c(nrow(da), 10L)),\n  identical(dim(da[\"ENSG00000243485\", 1:10, drop = FALSE]), c(1L, 10L)),\n  identical(\n    dim(da[c(1, 1, 2, 98), c(\"AACTCAGTCCAACCAA-1\", \"AACTCCCAGAAACCTA-1\")]),\n  c(4L, 2L))\n)\n\nFinally, let’s retrieve the (raw) counts for the GAPDH gene (ENSG00000111640) and ensure that the same results are retrieved from both objects:\n\nplot(\n  counts(tenx_pbmc4k[ \"ENSG00000111640\", ]), \n  da[ \"ENSG00000111640\", tenx_pbmc4k$Barcode],\n  xlab = \"GAPDH (ParquetArray)\", ylab = \"GAPDH (SingleCellExperiment)\")\nabline(0, 1)\n\n\n\n\n\nThe ParquetMatrix class\nNow that I have defined a seed, I can add a higher level class to facilitate working with parquet-backed matrices. The ParquetMatrix inherits from the DelayedMatrix class. It will automatically create the necessary seed, so all I have to provide is the path to the parquet file.\n\nsetClass(\"ParquetMatrix\",\n    contains = \"DelayedMatrix\",\n    representation(seed = \"ParquetArraySeed\")\n)\n\nsetMethod(\"DelayedArray\", \"ParquetArraySeed\",\n    function(seed) new_DelayedArray(seed, Class=\"ParquetMatrix\")\n)\n\nParquetMatrix &lt;- function(filepath, ...) {\n  seed &lt;- ParquetArraySeed(filepath = filepath, ...)\n  new(\"ParquetMatrix\", seed = seed)\n}\n\n\npm &lt;- ParquetMatrix(parquet_file)\ndim(pm)\n\n[1] 33694  4340\n\n\nAs I learned in a previous post gene and cell annotations can be combined with the ParquetMatrix into a SingleCellExperiment.\n\nsce &lt;- SingleCellExperiment(\n  assays = list(counts = pm),\n  colData = colData(tenx_pbmc4k)[match(colnames(pm), tenx_pbmc4k$Barcode), ],\n  rowData = rowData(tenx_pbmc4k)[row.names(pm), ]\n)\nsce\n\nclass: SingleCellExperiment \ndim: 33694 4340 \nmetadata(0):\nassays(1): counts\nrownames(33694): ENSG00000243485 ENSG00000237613 ... ENSG00000277475\n  ENSG00000268674\nrowData names(3): ENSEMBL_ID Symbol_TENx Symbol\ncolnames(4340): AAACCTGAGAAGGCCT-1 AAACCTGAGACAGACC-1 ...\n  TTTGTCAGTTAAGACA-1 TTTGTCATCCCAAGAT-1\ncolData names(11): Sample Barcode ... Individual Date_published\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#comparison-to-hdf5-backed-delayedarrays",
    "href": "posts/ParquetMatrix/index.html#comparison-to-hdf5-backed-delayedarrays",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Comparison to HDF5-backed DelayedArrays",
    "text": "Comparison to HDF5-backed DelayedArrays\nLet’s finish this example of a minimal implementation of the ParquetMatrix class by comparing its performance with the original hdf5-backed object.\n\nReading the full dataset into memory\nFirst, let’s read the full matrix into memory from either our parquet or the original HDF5 files. (We read each file ten times to get an idea of the average time it takes on my system.)\n\nmb &lt;- microbenchmark(\n    parquet = as.matrix(counts(sce)),\n    hdf5 = as.matrix(counts(tenx_pbmc4k)),\n  times = 10, unit = \"s\")\nprint(mb, signif = 2)\n\nUnit: seconds\n    expr  min   lq mean median   uq  max neval\n parquet 0.54 0.57 0.59   0.58 0.61 0.65    10\n    hdf5 0.81 0.82 0.85   0.83 0.86 0.94    10\n\n\nNot too bad! Loading the count matrix into memory from the parquet file is slightly faster than from the HDF5 file on average.\n\n\nSubsetting to 50 random rows and columns\nBoth parquet and HDF5 formats are optimized for column-oriented data. Let’s try to retrieve a random subset of counts to see how they fare:\n\nrows &lt;- sample(nrow(pm), 50, replace = FALSE)\ncols &lt;- sample(ncol(pm), 50, replace = FALSE)\n\n\nmb &lt;- microbenchmark(\n    parquet = as.matrix(counts(sce)[rows, cols]),\n    hdf5 = as.matrix(counts(tenx_pbmc4k)[rows, cols]),\n  times = 10, unit = \"s\")\nprint(mb, signif = 2)\n\nUnit: seconds\n    expr   min    lq mean median   uq  max neval\n parquet 0.093 0.095 0.11   0.11 0.12 0.14    10\n    hdf5 0.120 0.120 0.12   0.12 0.12 0.12    10\n\n\nExtracting the 50 x 50 sub-matrix takes roughly the same amount of time with both file types."
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#conclusion",
    "href": "posts/ParquetMatrix/index.html#conclusion",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Conclusion",
    "text": "Conclusion\nToday, I learned a lot about working with arrow objects in R, and got the chance to explore the DelayedArray infrastructure further. I am certain that the methods I wrote can be improved - but even my crude implementation of the ParquetMatrix class seems to be about as performant as the HDF5-backed version when reading from a local file.\nThe arrow project supports reading parquet files from cloud storage (S3), something I found challenging (e.g. slow) with HDF5 files. All I need to do is pass an S3 URL as the filepath argument to the ParquetMatrix() function, and (assuming I have set up the right access credentials) I can work with remote files in the same way.2"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#reproducibility",
    "href": "posts/ParquetMatrix/index.html#reproducibility",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nSession Information\n\n\nsessioninfo::session_info(\"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-09-13\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package              * version date (UTC) lib source\n P abind                * 1.4-5   2016-07-21 [?] CRAN (R 4.3.0)\n P arrow                * 13.0.0  2023-08-30 [?] CRAN (R 4.3.0)\n P Biobase              * 2.60.0  2023-05-08 [?] Bioconductor\n P BiocGenerics         * 0.46.0  2023-06-04 [?] Bioconductor\n P DelayedArray         * 0.26.7  2023-07-30 [?] Bioconductor\n P dplyr                * 1.1.2   2023-04-20 [?] CRAN (R 4.3.0)\n P fs                   * 1.6.3   2023-07-20 [?] CRAN (R 4.3.0)\n P GenomeInfoDb         * 1.36.1  2023-07-02 [?] Bioconductor\n P GenomicRanges        * 1.52.0  2023-05-08 [?] Bioconductor\n P HDF5Array            * 1.28.1  2023-05-08 [?] Bioconductor\n P IRanges              * 2.34.1  2023-07-02 [?] Bioconductor\n P Matrix               * 1.5-4.1 2023-05-18 [?] CRAN (R 4.3.1)\n P MatrixGenerics       * 1.12.3  2023-07-30 [?] Bioconductor\n P matrixStats          * 1.0.0   2023-06-02 [?] CRAN (R 4.3.0)\n P microbenchmark       * 1.4.10  2023-04-28 [?] CRAN (R 4.3.0)\n P rhdf5                * 2.44.0  2023-05-08 [?] Bioconductor\n P S4Arrays             * 1.0.6   2023-08-30 [?] Bioconductor\n P S4Vectors            * 0.38.1  2023-05-08 [?] Bioconductor\n P SingleCellExperiment * 1.22.0  2023-05-08 [?] Bioconductor\n P SummarizedExperiment * 1.30.2  2023-06-11 [?] Bioconductor\n P TENxPBMCData         * 1.18.0  2023-04-27 [?] Bioconductor\n P tibble               * 3.2.1   2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr                * 1.3.0   2023-01-24 [?] CRAN (R 4.3.0)\n\n [1] /Users/sandmann/repositories/blog/renv/library/R-4.3/aarch64-apple-darwin20\n [2] /Users/sandmann/Library/Caches/org.R-project.R/R/renv/sandbox/R-4.3/aarch64-apple-darwin20/ac5c2659\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/ParquetMatrix/index.html#footnotes",
    "href": "posts/ParquetMatrix/index.html#footnotes",
    "title": "Adventures with parquet III: single-cell RNA-seq data and comparison with HDF5-backed arrays",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy default, the location of the cache is set via ExperimentHub::getExperimentHubOption(\"CACHE\"). For example, on my system the data is located at /Users/sandmann/Library/Caches/org.R-project.R/R/ExperimentHub.↩︎\nThe performance will depend on the network connection. With my home internet connection, I was able to read the full dataset from the parquet file into memory or extract counts for 50 random genes x cells in 1.8 seconds on average.↩︎"
  },
  {
    "objectID": "posts/dbgap/index.html",
    "href": "posts/dbgap/index.html",
    "title": "Retrieving access-controlled data from NCBI’s dbGAP repository",
    "section": "",
    "text": "Today I learned about different ways to retrieve access-controlled short read data from NCBI’s dbGAP repository. dbGAP hosts both publicly available and Access Controlled data. The latter is usually used to disseminate data from individual human participants and requires a data access application.\nAfter the data access request has been granted, it is time to retrieve the actual data from dbGAP and - in case of short read data - its sister repository the Short Read Archive.\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/dbgap/index.html#tldr",
    "href": "posts/dbgap/index.html#tldr",
    "title": "Retrieving access-controlled data from NCBI’s dbGAP repository",
    "section": "",
    "text": "Today I learned about different ways to retrieve access-controlled short read data from NCBI’s dbGAP repository. dbGAP hosts both publicly available and Access Controlled data. The latter is usually used to disseminate data from individual human participants and requires a data access application.\nAfter the data access request has been granted, it is time to retrieve the actual data from dbGAP and - in case of short read data - its sister repository the Short Read Archive."
  },
  {
    "objectID": "posts/dbgap/index.html#authenticating-with-jwt-or-ngc-files",
    "href": "posts/dbgap/index.html#authenticating-with-jwt-or-ngc-files",
    "title": "Retrieving access-controlled data from NCBI’s dbGAP repository",
    "section": "Authenticating with JWT or NGC files",
    "text": "Authenticating with JWT or NGC files\nThe path to authenticating and downloading dbGAP data differs depending on whether you are using the AWS or GCP cloud proviers, or a local compute infrastructure (or another, unsupported cloud provider) instead.\n\nAuthentication within AWS or GCP cloud environments\nOn these two platforms, you have two paths to access the data:\n\nWith a JWT file: A JWT1 file, introduced with sra-tools version 2.10, allows users to transfer data from dbGAP’s cloud buckets into your own cloud instance. (Because both your and dbGAP’s system’s share the same cloud environment, this is faster than a regular transfer e.g. via https or ftp) 2\n\n\nVia fusera: Alternatively, you can mount dbGAP’s buckets as read-only volumes on your cloud instances via fusera3\n\n\n\n\n\n\n\nThe nf-core/fetchngs workflow\n\n\n\n\n\nThe nf-core/fetchngs workflow supports the retrieval of dbGAP data via JWT file authentication, e.g. when it is executed on AWS or GCP compute instances (see above). As all nf-core workflows, it is easily parallelized, e.g. across an HPC or via an AWS Batch queue. (Highly recommended when you need to retrieve large amounts of data.)\n\n\n\n\n\nAuthenticating outside AWS / GCP\nOn all other compute platforms, including your laptop or your local high- performance cluster (HPC), you need to authenticate with an NGC file (containing your repository key) instead45.\nIn this blog post, I will outline how to use NGC authentication, but make sure to read dbGAP’s official documentation as well.\n\n\nRetrieving dbGAP data with NGC authentication\nIf you are not working on AWS or GCP, and need to rely on NGC authentication, the following steps might be useful.\n\n1. Log into dbGAP\n\nNavigate to the dbGAP login page for controlled access and log in with your eRA credentials.\n\n\n\n2. Install sra-tools from github\nI usually download the latest binary of the sra-tools suite for my operating system from [github]https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit). Alternatively, you can also install it using Bioconda\n\n\n\n\n\n\nNote\n\n\n\nPlease note that the sra-tools package is frequently updated, so make sure you have the latest version).\n\n\nFor example, this code snipped retrieves and decompresses the latest version for Ubuntu Linux into the ~/bin directory:\nmkdir -p ~/bin\npushd ~/bin\nwget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.0.7/sratoolkit.3.0.7-ubuntu64.tar.gz\ntar xfz sratoolkit.3.0.7-ubuntu64.tar.gz\nrm sratoolkit.3.0.7-ubuntu64.tar.gz\npopd\nAfterward, I add the sra directory to my PATH and verify that it’s the version I expected:\nexport PATH=~/bin/sratoolkit.3.0.7-ubuntu64/bin:$PATH\nprefetch --version  # verify that it's the version you downloaded\n\n\n\n\n\n\nNote\n\n\n\nThe best source of information about using the various tools included in the sra-tools is the sra-tools wiki.\n\n\n\n\n3. Configure the sra toolkit\nNext, I configure the toolkit, especially the location of the cache directory. The prefetch command stores all SRA files it downloads in this location, so I make sure it is on a volume that is large enough to hold the expected amount of data.\n./vdb-config -i\n\nIn the Cache section, O specify an existing directory as the public user-repository. This is where prefetch will be download files to (and they will be kept until the cache is cleared!)\n\nMy settings are stored in the ${HOME}/.ncbi/user-settings.mkfg file. For more information and other ways to configure the toolkit, please see its wiki page.\n\n\n4. Log into dbGAP to retrieve the repository key\n\nBack in on the dbGAP website, navigate to the “My Projects” tab\nChoose “get dbGaP repository key” in the “Actions” column.\nDownload the repository key file with the .ngc extension to your system.\n\n\n\n5. Choose the files to download from SRA\n\nIn your dbGAP account, next navigate to the “My requests” tab.\nClick on “Request files” on the right side of the table.\nNavigate to the SRA data (reads and reference alignments) tab.\nClick on SRA Run Selector\nSelect all of the files you would like to download in the table at the bottom of the page.\nToggle the Selected radio button.\n\n\n\n6. Download the .krt file that specifies which files to retrieve\n\nDownload the .krt file by clicking on the green Cart file button.\n\n\n\n7. Initiate the download of the files in SRA format\n\nNow, with both the .ngc and .krt files in hand, we can trigger the download with the sra-tool’s prefetch command. We need to provide both paths to\n\nthe repository key (via --ngc) and\nthe cart file (via --cart)\n\n\nFor example, this code snipped assumes the two files are are in my home directory. (The exact names of your .ngc and .krt files will be different.)\nmkdir -p ~/dbgap\npushd ~/dbgap\nprefetch \\\n  --max-size u \\\n  --ngc ~/prj_123456.ngc \\\n  --cart ~/cart_DAR12345_2023081212345.krt\npopd\nNote: The files are downloaded (and cached) in SRA format into the directory I specified when configuring the sra-toolkit (e.g. the public user-repository). Extracting reads and generating FASTQ files is a separate step.\n\n\n8. Decrypt SRA files and extract reads in FASTQ format\n🚨 The final fastq-files will be approximately 7 times the size of the accession. The fasterq-dump-tool needs temporary space (scratch space) of about 1.5 times the amount of the final fastq-files during the conversion. Overall, the space you need during the conversion is approximately 17 times the size of the accession.\n🚨 The extraction and recompression steps are very CPU intensive, and it is recommended to use multiple cores. (The code below uses all available cores, as determined via the nproc --all command.)\nThe fasterq-dump tool extracts the reads into FASTQ files. It only accepts a single accession at a time, and expects to find the corresponding SRA file in the cache directory. Like the prefetch command above, it requires the .ngc file to verify that I am permitted to decrypt the data.\nTo save disk space I only extract a single SRA file at a time and then compress the FASTQ files with pigz. Afterward I copy the compressed FASTQ files to an AWS S3 bucket and delete the local files before processing the next accession.\n#!/usr/bin/env bash\nset -e\nset -o nounset\n\ndeclare -r CACHEDIR=\"~/cache/sra/\"  # the cache directory with .sra files\ndeclare -r BUCKET=\"s3://my-s3-bucket-for-storing-dbGAP-data\"\n\nfor SRA_FILE in ${CACHEDIR}/*.sra\ndo\n  fasterq-dump -p \\\n    --threads $(nproc --all) \\\n    --ngc ~/prj_123456.ngc \\\n    $(basename $SRA_FILE .sra)\n  pigz \\\n    --processes $(nproc --all) \\\n    *.fastq\n  aws s3 sync \\\n    --exclude \"*\" \\\n    --include \"*.fastq.gz\" \\\n    . \\\n    ${BUCKET}\n  rm *.fastq.gz\ndone"
  },
  {
    "objectID": "posts/dbgap/index.html#footnotes",
    "href": "posts/dbgap/index.html#footnotes",
    "title": "Retrieving access-controlled data from NCBI’s dbGAP repository",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJSON Web Token↩︎\ndbGAP’s official JWT instructions are here.↩︎\ndbGAP’s official fusera instructions are here.↩︎\ndbGAP’s official NGC instructions are here.↩︎\nNGC file authentication also works on cloud instances, e.g. an AWS EC2 instance, but it is slower as it doesn’t take advantage of the fact that your instance and dbGAP’s data bucket are co-located.↩︎"
  },
  {
    "objectID": "posts/web-r/index.html",
    "href": "posts/web-r/index.html",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/web-r/index.html#motivation",
    "href": "posts/web-r/index.html#motivation",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Motivation",
    "text": "Motivation\nSince its first release in March 2023, the WebR project has made incredible progress. It\n\nmakes it possible to run R code in the browser without the need for an R server to execute the code: the R interpreter runs directly on the user’s machine.\n\nI routinely communicate the results of analysis to collaborators as HTML documents, e.g. websites created from Quarto markdown files that combine prose and the analysis code itself. While I incorporate interactive elements (e.g. tables or plots) as much as possible, the reports are static - e.g. they don’t allow users to change parameters on the fly.\nThat’s why I am excited to learn about James J Balamuta’s quarto-webr extension, which embeds webR into quarto documents.\nToday I am making my first steps, with the goal of creating an HTML report that\n\nReads differential gene expression results from multiple experiments from CSV files.\nApplies a user-specified false discovery (FDR) threshold to each experiment.\nCreates a Venn diagram to visualize the overlap between the results.\nShows the results for the genes deemed significantly differentially expressed in all experiments.\n\nSpecifically, I would like users to be able to modify the FDR threshold themselves and to trigger a refresh of the plot & tables."
  },
  {
    "objectID": "posts/web-r/index.html#installation",
    "href": "posts/web-r/index.html#installation",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Installation",
    "text": "Installation\nI already have quarto installed on my system1, and first add the quarto-webr extension:\nquarto add coatless/quarto-webr"
  },
  {
    "objectID": "posts/web-r/index.html#creating-webr-r-code-cells",
    "href": "posts/web-r/index.html#creating-webr-r-code-cells",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Creating webr-r code cells",
    "text": "Creating webr-r code cells\nWith the extension in place, I can now create webr-r code cells in my Quarto documents. To render them, I also need to include the corresponding filter in the YAML header of the file.2\nHere is a minimal example of a Quarto document with the required header:\n---\ntitle: webR in Quarto HTML Documents\nformat: html\nengine: knitr\nfilters:\n  - webr\n---\n\nThis is a webR-enabled code cell in a Quarto HTML document.\n\n```{webr-r}\nfit = lm(mpg ~ am, data = mtcars)\n\nsummary(fit)\n```"
  },
  {
    "objectID": "posts/web-r/index.html#reproducibility",
    "href": "posts/web-r/index.html#reproducibility",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Information\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Debian GNU/Linux 12 (bookworm)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-11-18\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P BiocManager   1.30.22 2023-08-08 [?] RSPM (R 4.3.1)\n R bspm          0.5.5   &lt;NA&gt;       [?] &lt;NA&gt;\n P cli           3.6.1   2023-03-23 [?] CRAN (R 4.3.1)\n P digest        0.6.33  2023-07-07 [?] CRAN (R 4.3.1)\n P evaluate      0.21    2023-05-05 [?] CRAN (R 4.3.1)\n P fastmap       1.1.1   2023-02-24 [?] CRAN (R 4.3.1)\n P htmltools     0.5.6   2023-08-10 [?] CRAN (R 4.3.1)\n P htmlwidgets   1.6.2   2023-03-17 [?] CRAN (R 4.3.1)\n P jsonlite      1.8.7   2023-06-29 [?] CRAN (R 4.3.1)\n P knitr         1.43    2023-05-25 [?] CRAN (R 4.3.1)\n   renv          1.0.2   2023-08-15 [1] RSPM (R 4.3.0)\n P rlang         1.1.1   2023-04-28 [?] CRAN (R 4.3.1)\n P rmarkdown     2.24    2023-08-14 [?] CRAN (R 4.3.1)\n P rstudioapi    0.15.0  2023-07-07 [?] CRAN (R 4.3.1)\n P sessioninfo   1.2.2   2021-12-06 [?] CRAN (R 4.3.1)\n P xfun          0.40    2023-08-09 [?] CRAN (R 4.3.1)\n P yaml          2.3.7   2023-01-23 [?] CRAN (R 4.3.1)\n\n [1] /home/sandmann/repositories/blog/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/sandmann/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/web-r/index.html#footnotes",
    "href": "posts/web-r/index.html#footnotes",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can download the version for your operation system here↩︎\nAlternatively, you can also specify the options globally by adding them to your _quarto.yml file↩︎"
  },
  {
    "objectID": "posts/web-r/index.html#enabling-webr-r-code-cells",
    "href": "posts/web-r/index.html#enabling-webr-r-code-cells",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Enabling webr-r code cells",
    "text": "Enabling webr-r code cells\nWith the extension in place, I can now create webr-r code cells in my Quarto documents. To render them, I also need to include the corresponding filter in the YAML header of the file.2\nHere is a minimal example of a Quarto document with the required header:\n---\ntitle: webR - Hello world\nformat: html\nengine: knitr\nfilters:\n  - webr\n---\n\nThe following code cell will be executed by `WebR` within the user's browser.\n\n```{webr-r}\nprint('Hello mtcars')\n```"
  },
  {
    "objectID": "posts/quarto-webr/index.html",
    "href": "posts/quarto-webr/index.html",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "",
    "text": "Today I learned how to use the quarto-webr extension to turn a Quarto markdown document into an interactive report that executes R code in the browser and lets readers change the code on the fly!\n\n\n\nVenn diagram\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/quarto-webr/index.html#motivation",
    "href": "posts/quarto-webr/index.html#motivation",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Motivation",
    "text": "Motivation\nSince its first release in March 2023, the webR project has made incredible progress. It\n\nmakes it possible to run R code in the browser without the need for an R server to execute the code: the R interpreter runs directly on the user’s machine.\n\nI routinely communicate the results of analysis to collaborators as HTML documents, e.g. websites created from Quarto markdown files that combine prose and the analysis code itself. While I incorporate interactive elements (e.g. tables or plots) as much as possible, the reports are static - e.g. they don’t allow users to change parameters on the fly.\nThat’s why I am excited to learn about James J Balamuta’s quarto-webr extension, which embeds webR into quarto documents.1\nToday I am making my first steps, with the goal of creating an HTML report that\n\nReads differential gene expression results from multiple experiments from CSV files.2\nApplies a user-specified false discovery (FDR) threshold to each experiment.\nCreates a Venn diagram to visualize the overlap between the results.\nShows the results for the genes deemed significantly differentially expressed in all experiments.\n\nSpecifically, I would like users to be able to modify the FDR threshold themselves and to trigger a refresh of the plot & tables."
  },
  {
    "objectID": "posts/quarto-webr/index.html#installation-configuration",
    "href": "posts/quarto-webr/index.html#installation-configuration",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Installation & configuration",
    "text": "Installation & configuration\nI already have quarto installed on my system3, and first add the quarto-webr extension:\nquarto add coatless/quarto-webr\n\nEnabling webr-r code cells\nWith the extension in place, I can now create webr-r code cells in my Quarto documents. To render them, I also need to include the corresponding filter in the YAML header of the file.4\nHere is a minimal example of a Quarto document with the required header:\n---\ntitle: webR - Hello world\nformat: html\nengine: knitr\nfilters:\n  - webr\n---\n\nThe following code cell will be executed by `webR` within the user's browser.\n\n```{webr-r}\nprint('Hello mtcars')\n```\n\n\nUsing additional R packages\nThe webR community has precompiled a large collection of R packages. The quarto-webr extension facilitates their installation: we can simply list required packages in the webr section of the YAML header. For example, I will use the ggvenn and huxtable packages to create the Venn diagram and result tables, respectively.\nwebr:\n  show-startup-mmessage: false\n  packages: ['ggvenn', 'huxtable']\n\n\nSpecifying code cell context\nAt the time of writing, the quarto-webr extension supports code cells with three different contexts5\n\ninteractive cells are executed when the user clicks the Run Code button and display both the code and the output.\nsetup cells are executed automatically and neither their code nor their output is shown.\noutput cells execute automatically but don’t show their code.\n\nI will use setup contexts to download & parse the output of three differential expression analyses, and then use interactive contexts to trigger filtering, plotting and printing the result tables."
  },
  {
    "objectID": "posts/quarto-webr/index.html#reproducibility",
    "href": "posts/quarto-webr/index.html#reproducibility",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Information\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       Debian GNU/Linux 12 (bookworm)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2023-11-21\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P BiocManager   1.30.22 2023-08-08 [?] RSPM (R 4.3.1)\n R bspm          0.5.5   &lt;NA&gt;       [?] &lt;NA&gt;\n P cli           3.6.1   2023-03-23 [?] CRAN (R 4.3.1)\n P digest        0.6.33  2023-07-07 [?] CRAN (R 4.3.1)\n P evaluate      0.21    2023-05-05 [?] CRAN (R 4.3.1)\n P fastmap       1.1.1   2023-02-24 [?] CRAN (R 4.3.1)\n P htmltools     0.5.6   2023-08-10 [?] CRAN (R 4.3.1)\n P htmlwidgets   1.6.2   2023-03-17 [?] CRAN (R 4.3.1)\n P jsonlite      1.8.7   2023-06-29 [?] CRAN (R 4.3.1)\n P knitr         1.43    2023-05-25 [?] CRAN (R 4.3.1)\n   renv          1.0.2   2023-08-15 [1] RSPM (R 4.3.0)\n P rlang         1.1.1   2023-04-28 [?] CRAN (R 4.3.1)\n P rmarkdown     2.24    2023-08-14 [?] CRAN (R 4.3.1)\n P rstudioapi    0.15.0  2023-07-07 [?] CRAN (R 4.3.1)\n P sessioninfo   1.2.2   2021-12-06 [?] CRAN (R 4.3.1)\n P xfun          0.40    2023-08-09 [?] CRAN (R 4.3.1)\n P yaml          2.3.7   2023-01-23 [?] CRAN (R 4.3.1)\n\n [1] /home/sandmann/repositories/blog/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/sandmann/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n R ── Package was removed from disk.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/quarto-webr/index.html#footnotes",
    "href": "posts/quarto-webr/index.html#footnotes",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nquarto-webr would not be possible without the help of George W Stagg, the lead developer of the webR project.↩︎\nThe details of the experiment are not important, as I am just looking for a convenient example to test the quarto-webr extension. If you would like to learn more please check out this previous post. An R script that generates the CSV files using the limma/voom framework is available here↩︎\nContexts are very recent feature, and at the time of writing there are still bugs to iron out e.g. controlling the execution order of loading third party packages↩︎\nAlternatively, you can also specify the options globally by adding them to your _quarto.yml file↩︎\nContexts are very recent feature, and at the time of writing there are still bugs to iron out e.g. controlling the execution order of loading third party packages↩︎"
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html",
    "href": "posts/quarto-webr/quarto_webr_example.html",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "",
    "text": "This interactive website allows users to intersect differential expression results of three different comparisons from an experiment published by Xia et al\nThe experiment was performed in three different batches (e.g. samples were collected on three different days). For this report, the differences between samples from homozygous APP-SAA and wiltdype animals were estimated separately for each batch/day.1\nBy manipulating the code cells include in this website, you can select a set of genes that passes a user-defined FDR threshold on all three days.2\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html#introduction",
    "href": "posts/quarto-webr/quarto_webr_example.html#introduction",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "",
    "text": "This interactive website allows users to intersect differential expression results of three different comparisons from an experiment published by Xia et al\nThe experiment was performed in three different batches (e.g. samples were collected on three different days). For this report, the differences between samples from homozygous APP-SAA and wiltdype animals were estimated separately for each batch/day.1\nBy manipulating the code cells include in this website, you can select a set of genes that passes a user-defined FDR threshold on all three days.2"
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html#step-1-choose-a-false-discovery-fdr-threshold",
    "href": "posts/quarto-webr/quarto_webr_example.html#step-1-choose-a-false-discovery-fdr-threshold",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Step 1: Choose a false-discovery (FDR) threshold",
    "text": "Step 1: Choose a false-discovery (FDR) threshold\n\nPlease choose an FDR threshold by changing the FDR_threshold the code box below.\n\n\nIt will be applied to results from all three comparisons.\nFor example, setting FDR_threshold &lt; 0.1 will retain all genes with a false-discovery rate &lt; 10% in all three experiments.\n\n\nPress the “Run Code” button to create a Venn diagram.\n\n\nYou can save the plot by right-clicking and selecting Save image as.\n\n\nRepeat until you like the results.\n\n\n  🟡 Loading\n    webR..."
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html#step-2-list-the-genes-that-pass-the-fdr-threshold-in-all-three-comparisons",
    "href": "posts/quarto-webr/quarto_webr_example.html#step-2-list-the-genes-that-pass-the-fdr-threshold-in-all-three-comparisons",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Step 2: List the genes that pass the FDR threshold in all three comparisons",
    "text": "Step 2: List the genes that pass the FDR threshold in all three comparisons\nNext, press the Run Code button to see the list of genes that pass your selected FDR threshold.\n\nIf you triple-click on the list of gene symbols, you can copy them into another document.\n\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\n\nStep 3: See the differential expression results for the selected genes\nFor more context, the following sections show you tables with the results from your three experiments for the genes you selected in Step 1."
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html#day-1",
    "href": "posts/quarto-webr/quarto_webr_example.html#day-1",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Day 1",
    "text": "Day 1\nHit Run Code button to see the expression of the selected genes or download results for all genes\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\n\nDay 2\nHit Run Code button to see the expression of the selected genes or download results for all genes\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\n\n\nDay 3\nHit Run Code button to see the expression of the selected genes or download results for all genes\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\n\n  🟡 Loading\n    webR..."
  },
  {
    "objectID": "posts/quarto-webr/quarto_webr_example.html#footnotes",
    "href": "posts/quarto-webr/quarto_webr_example.html#footnotes",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnalyzing each batch separately is not the best way to estimate genotype effects across the full experiment. For this exercise, I just want to obtain three different contrasts to compare in a Venn diagram. You can find more details about a more realistic analysis of this experiment in this blog post.↩︎\nThe FDR does not reveal the direction of differential expression. So it is possible that we will pick up genes that are statistically significantly differentially expressed in opposite directions.↩︎"
  },
  {
    "objectID": "posts/quarto-webr/index.html#putting-it-all-together",
    "href": "posts/quarto-webr/index.html#putting-it-all-together",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow it’s time to write some R code to retrieve the CSV files via http, parse them, generate the Venn diagram and subset the result tables to those genes that pass the chosen FDR threshold.\n\n\n\n\n\n\nFinal Quarto markdown document\n\n\n\n\n\nAlso available as a gist\n---\ntitle: \"Embedding R into Quarto documents with quarto-webr\"\nsubtitle: \"Example: intersecting differential expression results\"\nauthor: \"Thomas Sandmann\"\ndate: '2023/11/18'\nformat: html\nengine: knitr\nwebr: \n  show-startup-message: true\n  packages: ['ggvenn', 'huxtable']\n  channel-type: \"post-message\"\nfilters:\n  - webr\neditor_options: \n  chunk_output_type: console\n---\n\n```{webr-r}\n#| context: setup\nkUrlRoot &lt;- paste0(\n  \"https://tomsing1.github.io/blog/posts/quarto-webr\"\n)\nkResults &lt;- c(\n  \"Day1\" = \"day1_results\",\n  \"Day2\" = \"day2_results\",\n  \"Day3\" = \"day3_results\"\n  )\nresults &lt;- lapply(kResults, \\(result) {\n  url &lt;- paste(kUrlRoot, paste0(result, \".csv\"), sep = \"/\")\n  download.file(url, basename(url))\n  read.csv(basename(url))[, c(\"gene_name\", \"adj.P.Val\", \"logFC\")]\n})\nnames(results) &lt;- names(kResults)\n\n# keep only genes present in all results\nkeep &lt;- Reduce(intersect, lapply(results, \\(x) x$gene_name))\nresults &lt;- lapply(results, \\(result) result[result$gene_name %in% keep, ])\n\n# Helper function to apply an FDR cutoff to a list of dataframes\napply_fdr &lt;- function(x, fdr_cutoff = 0.01){\n  lapply(x, function(df){\n    df[df$adj.P.Val &lt; fdr_cutoff, ]$gene_name\n  })\n}\n```\n  \n## Introduction\n\nThis interactive website allows users to intersect differential expression\nresults of three different comparisons from an experiment published by \n[Xia et al](https://www.biorxiv.org/content/10.1101/2021.01.19.426731v1).\n\nThe experiment was performed in three different batches (e.g. samples were\ncollected on three different days). For this report, the differences\nbetween samples from homozygous APP-SAA and wildtype animals were estimated\n_separately_ for each batch/day: `Day 1`, `Day 2` and `Day 3`.[^1]\n\n[^1]: Analyzing each batch separately is _not_ the best way to estimate genotype\neffects across the full experiment. For this exercise, I just want to obtain\nthree different contrasts to compare in a Venn diagram. You can find more\ndetails about a more realistic analysis of this experiment in \n[this blog post](https://tomsing1.github.io/blog/posts/nextflow-core-quantseq-3-xia/).\n\nBy manipulating the code cells include in this website, you can select\na set of genes that passes a user-defined FDR threshold on all three days.[^2]\n\n[^2]: The FDR does not reveal the direction of differential expression. So it\nis possible that we will pick up genes that are statistically significantly\ndifferentially expressed in _opposite directions_.\n\n## Step 1: Choose a false-discovery (FDR) threshold \n\n1. Please choose an FDR threshold by changing the `FDR_threshold` the code\n  box below.\n  \n  - It will be applied to results from all three comparisons.\n  - For example, setting `FDR_threshold &lt; 0.1` will retain all genes with a\n    false-discovery rate &lt; 10% in _all three experiments_.\n\n2. Press the \"Run Code\" button to create a Venn diagram.\n\n  - You can save the plot by right-clicking and selecting `Save image as`.\n  \n3. Repeat until you like the results.\n\n```{webr-r}\n#| context: interactive\nFDR_threshold &lt;- 0.1\nfiltered &lt;- apply_fdr(results, FDR_threshold)\nggvenn(filtered)\nselected &lt;- sort(Reduce(intersect, filtered))\n```\n\n## Step 2: List the genes that pass the FDR threshold in all three comparisons\n\nNext, press the `Run Code` button to see the list of genes that pass your \nselected FDR threshold.\n\n- If you triple-click on the list of gene symbols, you can copy them into\n  another document.\n\n```{webr-r}\n#| context: interactive\npaste(selected, collapse = \", \")\n```\n\n### Step 3: See the differential expression results for the selected genes\n\nFor more context, the following sections show you tables with the results\nfrom your three experiments for the genes you selected in Step 1.\n\n## Day 1\n\nHit `Run Code` button to see the expression of the selected genes \nor download [results for all genes](https://tomsing1.github.io/blog/posts/quarto-webr/day1_results.csv.gz)\n\n```{webr-r}\n#| context: interactive\nresults[[\"Day1\"]][match(selected, results[[\"Day1\"]]$gene_name), ] |&gt;\n  huxtable::hux()\n```\n\n### Day 2\n\nHit `Run Code` button to see the expression of the selected genes \nor download [results for all genes](https://tomsing1.github.io/blog/posts/quarto-webr/day2_results.csv.gz)\n\n```{webr-r}\n#| context: interactive\nresults[[\"Day2\"]][match(selected, results[[\"Day2\"]]$gene_name), ] |&gt;\n  huxtable::hux()\n```\n\n### Day 3\n\nHit `Run Code` button to see the expression of the selected genes \nor download [results for all genes](https://tomsing1.github.io/blog/posts/quarto-webr/day3_results.csv.gz)\n\n```{webr-r}\n#| context: interactive\nresults[[\"Day3\"]][match(selected, results[[\"Day3\"]]$gene_name), ] |&gt;\n  huxtable::hux()\n```\n\n```{webr-r}\n#| context: interactive\nsessionInfo()\n```"
  },
  {
    "objectID": "posts/quarto-webr/index.html#deployment",
    "href": "posts/quarto-webr/index.html#deployment",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "Deployment",
    "text": "Deployment\nBecause the final report is just a set of static files, deploying it simply requires sharing the rendered HTML file and the associated files.\nYou can see my rendered report on github.io here"
  },
  {
    "objectID": "posts/quarto-webr/index.html#first-impressions",
    "href": "posts/quarto-webr/index.html#first-impressions",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "First impressions",
    "text": "First impressions\n\nSuccess! I was able to render a Quarto markdown document into an interactive HTML report that allows the user to modify the FDR threshold on the fly.\nIt took me quite a while to get everything to work, and I am looking forward to the next release of the quarto-webr extension that promises even better control of each cell’s context.\nEven though my goal was to expose only a single user-specified variable (the FDR threshold), my report contains multiple interactive elements that the user has to trigger in sequence. Wouldn’t it be great if subsequent code cells could detect upstream changes and recalculate their results automatically?\nFor this use case, e.g. sharing interactive analysis reports, it would eventually be helpful to be able to include familiar GUI elements - e.g.  dropdown menus, sliders or tick boxes - similar to what is already possible when a shiny server is available.\n\nIt’s early days for the webR tool chain, but if its incredible progress over the last 6-9 months is any indication, then there will be lots of features to look forward to soon!\nMany thanks to James J Balamuta for creating the quarto-webr extension and to the webR team - especially George Stagg - for opening so many opportunities for the R community!"
  },
  {
    "objectID": "posts/quarto-webr/index.html#tldr",
    "href": "posts/quarto-webr/index.html#tldr",
    "title": "Embedding R into Quarto documents with quarto-webr",
    "section": "",
    "text": "Today I learned how to use the quarto-webr extension to turn a Quarto markdown document into an interactive report that executes R code in the browser and lets readers change the code on the fly!\n\n\n\nVenn diagram"
  }
]