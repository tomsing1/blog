[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "Image credits: tOrange.biz, CC BY 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is Thomas Sandmann’s personal blog, created with Quarto. I am planning to share e.g. “Things I learned today” (TIL) and other pieces of news around Computational Biolgy and Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Faster installations with conda\n\n\n\n\n\n\n\nTIL\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rlist R package\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nCreating custom badges for your README\n\n\n\n\n\n\n\nTIL\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning nextflow: blasting multiple sequences\n\n\n\n\n\n\n\nTIL\n\n\nnextflow\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython type hints\n\n\n\n\n\n\n\nTIL\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nFujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex\n\n\n\n\n\n\n\nLiterature\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRefreshing & exporting temporary AWS credentials\n\n\n\n\n\n\n\nTIL\n\n\nAWS\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling pyroe with conda\n\n\n\n\n\n\n\nTIL\n\n\nconda\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Thomas Sandmann’s blog."
  },
  {
    "objectID": "posts/pyroe-installation/index.html",
    "href": "posts/pyroe-installation/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "It can be installed either using pip or conda, and the latter will install additional dependencies (e.g. bedtools) and include the load_fry() as well.\nTo install pyroe with conda, I first followed bioconda’s instructions to add and configure the required channels:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nand then installed pyroe\nconda install pyroe\nNow I can convert alevin-fry output to one of the following formats: zarr, csvs, h5ad or loom.\npyroe convert --help"
  },
  {
    "objectID": "posts/aws-export-credentials/index.html",
    "href": "posts/aws-export-credentials/index.html",
    "title": "Refreshing & exporting temporary AWS credentials",
    "section": "",
    "text": "Today I learned how to configure and refresh these credentials in the command line, as well how to export them either as environmental variables or write them to the credentials file where tools that do not interact with AWS SSO natively can access them.\n\nConfiguring an AWS SSO profile\nFirst, we need to configure a named profile for use with AWS SSO. The following AWS CLI version 2 command will interactively walk you through the necessary steps:\naws configure sso\nThe information you provide will be written to the config file, located in the ~/.aws directory on Mac OS. Here is an example:\n[profile my-dev-profile]\nsso_start_url = https://my-sso-portal.awsapps.com/start\nsso_region = us-east-1\nsso_account_id = 123456789011\nsso_role_name = readOnly\nregion = us-west-2\noutput = json\n\n\nLogging into the AWS SSO profile\nNow we can log into AWS SSO and request temporary credentials:\naws sso login --profile my-dev-profile\nThis command will try to open a web browser for you and prompt you to confirm the login. Alternatively, you can copy & paste the displayed URL and manually enter the confirmation code output by the command.\nIf the login was successful, you can now adopt the my-dev-profile when using the AWS CLI, e.g.\naws s3 ls --profile my-dev-profile\nThe AWS SSO endpoint recognizes many environmental variables that you can use to specify defaults, e.g.\n\nAWS_PROFILE: The profile to use (e.g. my-dev-profile)\nAWS_SHARED_CREDENTIALS_FILE: the location of the shared credentials files (default on Mac OS: ~/.aws/.credentials)\nAWS_CONFIG_FILE: the location of the AWS CLI configuration file (default on Mac OS: ~/.aws.config)\n\n\n\nAccessing temporary credentials\nThe AWS CLI and many of the AWS SKDs will automatically detect and use SSO credentials. But other tools might not (yet) be compatible with this authentication route. Instead, they might\n\nread credentials for a profile from the credentials file\nrely on environmental variables, e.g. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n\nTo expose the temporary credentials, Ben Kehoe has made the aws-export-credentials tool available.\n\n\nInstalling aws-export-credentials\nThe recommended way to install aws-export-credentials is via pipx because it will automatically make it available in your PATH.\n\nIf you don’t have pipx available on your system, install it first.\nNext, install aws-export-credentials by executing the following steps in your shell:\n\npipx ensurepath  # in case you haven't run this before\npipx install aws-export-credentials\naws-export-credentials --version  # verify the installation\n\n\nUpdating the credentials file\nAt the beginning of your workday - or whenever needed - run the following set of commands. (Replace the SSO profile with the one you want to adopt.)\nPROFILE=\"my-dev-profile\"\n\n# retrieve new credentials from AWS\naws sso login --profile \"${PROFILE}\"\n\n# write the temporary credentials to the ~/.aws/credentials file\naws-export-credentials \\\n  --profile \"${PROFILE}\" \\\n  --credentials-file-profile \"${PROFILE}\"\nThis will refresh the credentials (via aws sso login) and then write them to the my-dev-profile profiles in the ~/.aws/.credentials file. Now we can access them e.g. in the aws.s3 R package:\nlibrary(aws.s3)\nlibrary(aws.signature)\naws.signature::use_credentials(profile = \"my-dev-profile\")\naws.s3::bucketlist()\n\n\nExposing environmental variables\nSome tools only recognize environmental variables. Luckily, aws-export-credentials can automate this process, too:\nexport $(aws-export-credentials --profile my-dev-profile --env-export)\nwill export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables in your shell session.\n\n\nSourcing credentials with an external process\nFinally, you can also include a command that looks up credentials as a credential_process in your config file. (More information here) But that’s not a use case I have explored, yet."
  },
  {
    "objectID": "posts/fujita_2022/index.html",
    "href": "posts/fujita_2022/index.html",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "",
    "text": "Fujita et al, Figure 1A/B: (A) Schema of the study. (B) UMAP visualization of 1,509,626 nuclei from 424 donors. Each of the seven major cell types is labeled with a different color.”\n\n\n\nThis large sample size enabled them to assess the effect of genetic variation (e.g. single-nucleotide variants) on gene expression - one cell type at a time. The authors created pseudo-bulk gene expression profiles for each patient for 7 cell types and 81 cell subtypes.\nBecause neurons are highly abundant in the DLPFC, the largest number of nuclei originated from neurons, and the statistical power to detect eQTLs was lower in rarer cell types (e.g. microglia). This highlights the potential of enrichment methods, e.g. by fluorescent activate nuclei sorting (FANS) approached. (See e.g. (Kamath et al. 2022), who specifically enriched dopaminergic neurons or (Sadick et al. 2022), who enriched astrocytes and oligodendrocytes.)\nFujita et al were able to identify ~ 10,000 eGenes1, about half of which were shared across cell types. For example, they identified a novel eQTL (rs128648) for the APOE gene specifically in microglia.\nHaving identified novel eQTL relationships in vivo, the authors then used bulk RNA-seq measurements from a panel of induced pluripotent stem cells that had been differentiated either into neurons (iNeurons) or astrocytes (iAstrocytes) to test whether they could also observe the variants’ effects in vitro.\nDespite a relatively small sample size, a subset of eQTLs were replicated. But the the authors also point out unexpected discrepancies in the MAPT locus where they observed variant effects in the opposite direction from what they had observed by snRNA-seq.\nGene expression was significantly heritable in most cell types (except for those from which only small numbers of nuclei had been sampled). This allowed the authors to use their snRNA-seq dataset to impute cell type specific gene expression for large GWAS studies, e.g. for Alzheimer’s Disease, ALS, Parkinson’s Disease, and schizophrenia. This TWAS analysis detected e.g. 48 novel loci associated with AD in microglia, 22 of which had not been implicated previously.\nIn summary, this work by Fujita et al is an impressive achievement, demonstrating that single-cell/single-nuclei approaches have now become sufficiently scalable to power human genetics analyses.\nThe authors have already made the raw data for their study available on the AD Knowledge Portal. Thank you for sharing your data!\n\n\n\n\n\nReferences\n\nFujita, Masashi, Zongmei Gao, Lu Zeng, Cristin McCabe, Charles C. White, Bernard Ng, Gilad Sahar Green, et al. n.d. “Cell-Subtype Specific Effects of Genetic Variation in the Aging and Alzheimer Cortex.” https://doi.org/10.1101/2022.11.07.515446.\n\n\nKamath, Tushar, Abdulraouf Abdulraouf, S. J. Burris, Jonah Langlieb, Vahid Gazestani, Naeem M. Nadaf, Karol Balderrama, Charles Vanderburg, and Evan Z. Macosko. 2022. “Single-Cell Genomic Profiling of Human Dopamine Neurons Identifies a Population That Selectively Degenerates in Parkinson’s Disease.” Nature Neuroscience, May, 1–8. https://doi.org/10.1038/s41593-022-01061-1.\n\n\nSadick, Jessica S., Michael R. O’Dea, Philip Hasel, Taitea Dykstra, Arline Faustin, and Shane A. Liddelow. 2022. “Astrocytes and Oligodendrocytes Undergo Subtype-Specific Transcriptional Changes in Alzheimer’s Disease.” Neuron, April, S0896627322002446. https://doi.org/10.1016/j.neuron.2022.03.008.\n\nFootnotes\n\n\nGene whose expression was significantly associated with one or more genetic variants (FDR < 5%)↩︎"
  },
  {
    "objectID": "posts/python-hints/index.html",
    "href": "posts/python-hints/index.html",
    "title": "Python type hints",
    "section": "",
    "text": "Code Better with Type Hints – Part 1\nCode Better with Type Hints – Part 2\nFastAPI’s typing introduction\nPysheet: typing"
  },
  {
    "objectID": "posts/nextflow-blast-tutorial/index.html",
    "href": "posts/nextflow-blast-tutorial/index.html",
    "title": "Learning nextflow: blasting multiple sequences",
    "section": "",
    "text": "To start learning nextflow, I worked through Andrew Severin’s excellent Creating a NextFlow workflow tutorial. (The tutorial follows the older DSL1 specification of nextflow, but only a few small modifications were needed to run it under DSL2.)\nThe DSL2 code I wrote is here and these are notes I took while working through the tutorial:\n\nTo make a variable a pipeline parameter prepend it with params., then specify them in the command line:\nmain.nf:\n#! /usr/bin/env nextflow\nparams.query=\"file.fasta\"\nprintln \"Querying file $params.query\"\nshell command:\nnextflow run main.nf --query other_file.fasta\nThe -log argument directs logging to the specified file.\nnextflow -log nextflo.log run main.nf \nTo clean up intermediate files automatically upon workflow completion, use the cleanup parameter within a profile.\nprofiles {\n  standard {\n      cleanup = true\n  }\n  debug {\n      cleanup = false\n  }\n}\n\nBy convention the standard profile is implicitly used when no other\nprofile is specified by the user.\nCleaning up intermediate files precludes the use of -resume.\n\nThe nextflow.config file sets the global parameters, e.g.\n\nprocess\nmanifest\nexecutor\nprofiles\ndocker\nsingularity\ntimeline\nreport\netc\n\nContents of the work folder for a nextflow task:\n\n.command.begin is the begin script if you have one\n.command.err is useful when it crashes.\n.command.run is the full nextflow pipeline that was run, this is helpful when trouble shooting a nextflow error rather than the script error.\n.command.sh shows what was run.\n.exitcode will have the exit code in it.\n\nDisplaying help messages\nmain.nf\ndef helpMessage() {\nlog.info \"\"\"\n      Usage:\n      The typical command for running the pipeline is as follows:\n      nextflow run main.nf --query QUERY.fasta --dbDir \"blastDatabaseDirectory\" --dbName \"blastPrefixName\"\n\n      Mandatory arguments:\n       --query                        Query fasta file of sequences you wish to BLAST\n       --dbDir                        BLAST database directory (full path required)\n       [...]\n\"\"\"\n}\n\n// Show help message\nif (params.help) {\n    helpMessage()\n    exit 0\n}\nshell command:\nnextflow run main.nf --help\nThe publishDir directive accepts arguments like mode and pattern to fine tune its behavior, e.g.\noutput:\nfile(\"${label}/short_summary.specific.*.txt\")\npublishDir \"${params.outdir}/BUSCOResults/${label}/\", mode: 'copy', pattern: \"${label}/short_summary.specific.*.txt\"\nDSL2 allows piping, e.g.\nworkflow {\n  res = Channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true) |\n      runBlast\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\nAdd a timeline report to the output with\ntimeline {\n    enabled = true\n    file = \"$params.outdir/timeline.html\"\n}\n(in nextflow.config).\nAdd a detailed execution report with\nreport {\nenabled = true\nfile = \"$params.outdir/report.html\"\n}\n(in nextflow.config).\nInclude a profile-specific configuration file\nnextflow.config\nprofiles {\n    slurm { includeConfig './configs/slurm.config' }\n}\nconfigs/slurm.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 24:00:00'\n}\nand use it via nextflow run main.nf -profile slurm\nSimilarly, refer to a test profile, specified in a separate file:\nnextflow.config\ntest { includeConfig './configs/test.config' }\nAdding a manifest to nextflow.config\nmanifest {\n    name = 'isugifNF/tutorial'\n    author = 'Andrew Severin'\n    homePage = 'www.bioinformaticsworkbook.org'\n    description = 'nextflow bash'\n    mainScript = 'main.nf'\n    version = '1.0.0'\n}\nUsing a label for a process allows granular control of a process’ configuration\nmain.nf\nprocess runBlast { \n    label 'blast'\n}\nnextflow.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 02:00:00'\n    withLabel: blast { module = 'blast-plus' }\n}\n\nThe label has to be placed before the input section.\n\nLoading a module specifically for a process\nprocess runBlast {\n\n    module = 'blast-plus'\n    publishDir \"${params.outdir}/blastout\"\n\n    input:\n    path queryFile from queryFile_ch\n    .\n    .\n    . // these three dots mean I didn't paste the whole process.\n}\nEnabling docker in the nextflow.config\ndocker { docker.enabled = true }\n\nThe docker container can be specified in the process, e.g.\n\ncontainer = 'ncbi/blast'\nor\ncontainer = `quay.io/biocontainers/blast/2.2.31--pl526he19e7b1_5`\n\nWe can include additional options to pass to the container as well:\n\ncontainerOptions = \"--bind $launchDir/$params.outdir/config:/augustus/config\"\nprojectDir refers to the directory where the main workflow script is located. (It used to be called baseDir.)\nRefering to local directories from within a docker container: create a channel\n\nWorking in containers, we need a way to pass the database file location directly into the runBlast process without the need of the local path.\n\nRepeating a process over each element of a channel with each: input repeaters\nTurning a queue channel into a value channel, which can be used multiple times.\n\nA value channel is implicitly created by a process when it is invoked with a simple value.\nA value channel is also implicitly created as output for a process whose inputs are all value channels.\nA queue channel can be converted into a value channel by returning a single value, using e.g. first, last, collect, count, min, max, reduce, sum, etc. For example: the runBlast process receives three inputs in the following example:\n\nthe queryFile_ch queue channel, with multiple sequences.\nthe dbDir_ch value channel, created by calling .first(), which is reused for all elements of queryFile_ch\nthe dbName_ch value channel, which is also reused for all elements of queryFile_ch\n\n\nworkflow {\n  channel.fromPath(params.dbDir).first()\n  .set { dbDir_ch }\n\n  channel.from(params.dbName).first()\n  .set { dbName_ch }\n\n  queryFile_ch = channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true)\n     res = runBlast(queryFile_ch, dbDir_ch, dbName_ch)\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\n\n\nAdditional resources\n\nSoftware Carpentry course\nNextflow cheat sheet\nAwesome nextflow"
  },
  {
    "objectID": "posts/custom-badges/index.html",
    "href": "posts/custom-badges/index.html",
    "title": "Creating custom badges for your README",
    "section": "",
    "text": "Predefined badges\nMany open source software packages display key pieces of information as badges (aka shields) in their github README, indicating e.g. code coverage, unit test results, version numbers, license, etc.\nThe shields.io website provides many different ready-to-use badges, covering topics such as test results, code coverage, social media logos, activity, and many more.\n     \nBadges can show up to date information. For example, this badge shows the last commit to the github repository for this blog: . They can be returned either in svg (recommended) or png formats, from the img.shields.io and raster.shields.io servers, respectively.\n\n\nCustom badges\nIn addition to predefined outputs, you can also generate your own, entirely custom badges. They can be static like this one  or dynamically retrieve information from a JSON endpoint of your choice.\n\n\nAdding badges to a README.md file\nTo embed badges into your README.md, simply wrap its URL in markdown and surround it with the badges: start and badges: end tags:\n<!-- badges: start -->\n![](https://img.shields.io/github/last-commit/tomsing1/blog)\n<!-- badges: end -->"
  },
  {
    "objectID": "posts/rslist-r-package/index.html",
    "href": "posts/rslist-r-package/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Luckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it’s super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won’t regret it!"
  },
  {
    "objectID": "posts/conda-speedup/index.html",
    "href": "posts/conda-speedup/index.html",
    "title": "Faster installations with conda",
    "section": "",
    "text": "To take advantage of the speed-ups implemented in the latest conda release, you need to update your conda installation (if you already have one):\nconda info\nconda update -n base conda\nand then switch to the new dependency solver:\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\nThat’s it - enjoy parallelized downloads & faster installations!"
  }
]