[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "Image credits: tOrange.biz, CC BY 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is Thomas Sandmann’s personal blog, created with Quarto. I am planning to share e.g. “Things I learned today” (TIL) and other pieces of news around Computational Biolgy and Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Full text search in Postgres - the R way\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\npostgres\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating R the easy way: using rig command line tool\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 normconf: lightning talks\n\n\n\n\n\n\n\nTIL\n\n\nconference\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rlist R package\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rlist R package\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nCreating custom badges for your README\n\n\n\n\n\n\n\nTIL\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning nextflow: blasting multiple sequences\n\n\n\n\n\n\n\nTIL\n\n\nnextflow\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython type hints\n\n\n\n\n\n\n\nTIL\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nFujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex\n\n\n\n\n\n\n\nLiterature\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRefreshing & exporting temporary AWS credentials\n\n\n\n\n\n\n\nTIL\n\n\nAWS\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling pyroe with conda\n\n\n\n\n\n\n\nTIL\n\n\nconda\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Thomas Sandmann’s blog."
  },
  {
    "objectID": "posts/pyroe-installation/index.html",
    "href": "posts/pyroe-installation/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "It can be installed either using pip or conda, and the latter will install additional dependencies (e.g. bedtools) and include the load_fry() as well.\nTo install pyroe with conda, I first followed bioconda’s instructions to add and configure the required channels:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nand then installed pyroe\nconda install pyroe\nNow I can convert alevin-fry output to one of the following formats: zarr, csvs, h5ad or loom.\npyroe convert --help"
  },
  {
    "objectID": "posts/aws-export-credentials/index.html",
    "href": "posts/aws-export-credentials/index.html",
    "title": "Refreshing & exporting temporary AWS credentials",
    "section": "",
    "text": "Today I learned how to configure and refresh these credentials in the command line, as well how to export them either as environmental variables or write them to the credentials file where tools that do not interact with AWS SSO natively can access them.\n\nConfiguring an AWS SSO profile\nFirst, we need to configure a named profile for use with AWS SSO. The following AWS CLI version 2 command will interactively walk you through the necessary steps:\naws configure sso\nThe information you provide will be written to the config file, located in the ~/.aws directory on Mac OS. Here is an example:\n[profile my-dev-profile]\nsso_start_url = https://my-sso-portal.awsapps.com/start\nsso_region = us-east-1\nsso_account_id = 123456789011\nsso_role_name = readOnly\nregion = us-west-2\noutput = json\n\n\nLogging into the AWS SSO profile\nNow we can log into AWS SSO and request temporary credentials:\naws sso login --profile my-dev-profile\nThis command will try to open a web browser for you and prompt you to confirm the login. Alternatively, you can copy & paste the displayed URL and manually enter the confirmation code output by the command.\nIf the login was successful, you can now adopt the my-dev-profile when using the AWS CLI, e.g.\naws s3 ls --profile my-dev-profile\nThe AWS SSO endpoint recognizes many environmental variables that you can use to specify defaults, e.g.\n\nAWS_PROFILE: The profile to use (e.g. my-dev-profile)\nAWS_SHARED_CREDENTIALS_FILE: the location of the shared credentials files (default on Mac OS: ~/.aws/.credentials)\nAWS_CONFIG_FILE: the location of the AWS CLI configuration file (default on Mac OS: ~/.aws.config)\n\n\n\nAccessing temporary credentials\nThe AWS CLI and many of the AWS SKDs will automatically detect and use SSO credentials. But other tools might not (yet) be compatible with this authentication route. Instead, they might\n\nread credentials for a profile from the credentials file\nrely on environmental variables, e.g. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n\nTo expose the temporary credentials, Ben Kehoe has made the aws-export-credentials tool available.\n\n\nInstalling aws-export-credentials\nThe recommended way to install aws-export-credentials is via pipx because it will automatically make it available in your PATH.\n\nIf you don’t have pipx available on your system, install it first.\nNext, install aws-export-credentials by executing the following steps in your shell:\n\npipx ensurepath  # in case you haven't run this before\npipx install aws-export-credentials\naws-export-credentials --version  # verify the installation\n\n\nUpdating the credentials file\nAt the beginning of your workday - or whenever needed - run the following set of commands. (Replace the SSO profile with the one you want to adopt.)\nPROFILE=\"my-dev-profile\"\n\n# retrieve new credentials from AWS\naws sso login --profile \"${PROFILE}\"\n\n# write the temporary credentials to the ~/.aws/credentials file\naws-export-credentials \\\n  --profile \"${PROFILE}\" \\\n  --credentials-file-profile \"${PROFILE}\"\nThis will refresh the credentials (via aws sso login) and then write them to the my-dev-profile profiles in the ~/.aws/.credentials file. Now we can access them e.g. in the aws.s3 R package:\nlibrary(aws.s3)\nlibrary(aws.signature)\naws.signature::use_credentials(profile = \"my-dev-profile\")\naws.s3::bucketlist()\n\n\nExposing environmental variables\nSome tools only recognize environmental variables. Luckily, aws-export-credentials can automate this process, too:\nexport $(aws-export-credentials --profile my-dev-profile --env-export)\nwill export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables in your shell session.\n\n\nSourcing credentials with an external process\nFinally, you can also include a command that looks up credentials as a credential_process in your config file. (More information here) But that’s not a use case I have explored, yet."
  },
  {
    "objectID": "posts/fujita_2022/index.html",
    "href": "posts/fujita_2022/index.html",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "",
    "text": "Fujita et al, Figure 1A/B: (A) Schema of the study. (B) UMAP visualization of 1,509,626 nuclei from 424 donors. Each of the seven major cell types is labeled with a different color.”\n\n\n\nThis large sample size enabled them to assess the effect of genetic variation (e.g. single-nucleotide variants) on gene expression - one cell type at a time. The authors created pseudo-bulk gene expression profiles for each patient for 7 cell types and 81 cell subtypes.\nBecause neurons are highly abundant in the DLPFC, the largest number of nuclei originated from neurons, and the statistical power to detect eQTLs was lower in rarer cell types (e.g. microglia). This highlights the potential of enrichment methods, e.g. by fluorescent activate nuclei sorting (FANS) approached. (See e.g. (Kamath et al. 2022), who specifically enriched dopaminergic neurons or (Sadick et al. 2022), who enriched astrocytes and oligodendrocytes.)\nFujita et al were able to identify ~ 10,000 eGenes1, about half of which were shared across cell types. For example, they identified a novel eQTL (rs128648) for the APOE gene specifically in microglia.\nHaving identified novel eQTL relationships in vivo, the authors then used bulk RNA-seq measurements from a panel of induced pluripotent stem cells that had been differentiated either into neurons (iNeurons) or astrocytes (iAstrocytes) to test whether they could also observe the variants’ effects in vitro.\nDespite a relatively small sample size, a subset of eQTLs were replicated. But the the authors also point out unexpected discrepancies in the MAPT locus where they observed variant effects in the opposite direction from what they had observed by snRNA-seq.\nGene expression was significantly heritable in most cell types (except for those from which only small numbers of nuclei had been sampled). This allowed the authors to use their snRNA-seq dataset to impute cell type specific gene expression for large GWAS studies, e.g. for Alzheimer’s Disease, ALS, Parkinson’s Disease, and schizophrenia. This TWAS analysis detected e.g. 48 novel loci associated with AD in microglia, 22 of which had not been implicated previously.\nIn summary, this work by Fujita et al is an impressive achievement, demonstrating that single-cell/single-nuclei approaches have now become sufficiently scalable to power human genetics analyses.\nThe authors have already made the raw data for their study available on the AD Knowledge Portal. Thank you for sharing your data!\n\n\n\n\n\nReferences\n\nFujita, Masashi, Zongmei Gao, Lu Zeng, Cristin McCabe, Charles C. White, Bernard Ng, Gilad Sahar Green, et al. n.d. “Cell-Subtype Specific Effects of Genetic Variation in the Aging and Alzheimer Cortex.” https://doi.org/10.1101/2022.11.07.515446.\n\n\nKamath, Tushar, Abdulraouf Abdulraouf, S. J. Burris, Jonah Langlieb, Vahid Gazestani, Naeem M. Nadaf, Karol Balderrama, Charles Vanderburg, and Evan Z. Macosko. 2022. “Single-Cell Genomic Profiling of Human Dopamine Neurons Identifies a Population That Selectively Degenerates in Parkinson’s Disease.” Nature Neuroscience, May, 1–8. https://doi.org/10.1038/s41593-022-01061-1.\n\n\nSadick, Jessica S., Michael R. O’Dea, Philip Hasel, Taitea Dykstra, Arline Faustin, and Shane A. Liddelow. 2022. “Astrocytes and Oligodendrocytes Undergo Subtype-Specific Transcriptional Changes in Alzheimer’s Disease.” Neuron, April, S0896627322002446. https://doi.org/10.1016/j.neuron.2022.03.008.\n\nFootnotes\n\n\nGene whose expression was significantly associated with one or more genetic variants (FDR < 5%)↩︎"
  },
  {
    "objectID": "posts/python-hints/index.html",
    "href": "posts/python-hints/index.html",
    "title": "Python type hints",
    "section": "",
    "text": "Code Better with Type Hints – Part 1\nCode Better with Type Hints – Part 2\nFastAPI’s typing introduction\nPysheet: typing"
  },
  {
    "objectID": "posts/nextflow-blast-tutorial/index.html",
    "href": "posts/nextflow-blast-tutorial/index.html",
    "title": "Learning nextflow: blasting multiple sequences",
    "section": "",
    "text": "To start learning nextflow, I worked through Andrew Severin’s excellent Creating a NextFlow workflow tutorial. (The tutorial follows the older DSL1 specification of nextflow, but only a few small modifications were needed to run it under DSL2.)\nThe DSL2 code I wrote is here and these are notes I took while working through the tutorial:\n\nTo make a variable a pipeline parameter prepend it with params., then specify them in the command line:\nmain.nf:\n#! /usr/bin/env nextflow\nparams.query=\"file.fasta\"\nprintln \"Querying file $params.query\"\nshell command:\nnextflow run main.nf --query other_file.fasta\nThe -log argument directs logging to the specified file.\nnextflow -log nextflo.log run main.nf \nTo clean up intermediate files automatically upon workflow completion, use the cleanup parameter within a profile.\nprofiles {\n  standard {\n      cleanup = true\n  }\n  debug {\n      cleanup = false\n  }\n}\n\nBy convention the standard profile is implicitly used when no other\nprofile is specified by the user.\nCleaning up intermediate files precludes the use of -resume.\n\nThe nextflow.config file sets the global parameters, e.g.\n\nprocess\nmanifest\nexecutor\nprofiles\ndocker\nsingularity\ntimeline\nreport\netc\n\nContents of the work folder for a nextflow task:\n\n.command.begin is the begin script if you have one\n.command.err is useful when it crashes.\n.command.run is the full nextflow pipeline that was run, this is helpful when trouble shooting a nextflow error rather than the script error.\n.command.sh shows what was run.\n.exitcode will have the exit code in it.\n\nDisplaying help messages\nmain.nf\ndef helpMessage() {\nlog.info \"\"\"\n      Usage:\n      The typical command for running the pipeline is as follows:\n      nextflow run main.nf --query QUERY.fasta --dbDir \"blastDatabaseDirectory\" --dbName \"blastPrefixName\"\n\n      Mandatory arguments:\n       --query                        Query fasta file of sequences you wish to BLAST\n       --dbDir                        BLAST database directory (full path required)\n       [...]\n\"\"\"\n}\n\n// Show help message\nif (params.help) {\n    helpMessage()\n    exit 0\n}\nshell command:\nnextflow run main.nf --help\nThe publishDir directive accepts arguments like mode and pattern to fine tune its behavior, e.g.\noutput:\nfile(\"${label}/short_summary.specific.*.txt\")\npublishDir \"${params.outdir}/BUSCOResults/${label}/\", mode: 'copy', pattern: \"${label}/short_summary.specific.*.txt\"\nDSL2 allows piping, e.g.\nworkflow {\n  res = Channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true) |\n      runBlast\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\nAdd a timeline report to the output with\ntimeline {\n    enabled = true\n    file = \"$params.outdir/timeline.html\"\n}\n(in nextflow.config).\nAdd a detailed execution report with\nreport {\nenabled = true\nfile = \"$params.outdir/report.html\"\n}\n(in nextflow.config).\nInclude a profile-specific configuration file\nnextflow.config\nprofiles {\n    slurm { includeConfig './configs/slurm.config' }\n}\nconfigs/slurm.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 24:00:00'\n}\nand use it via nextflow run main.nf -profile slurm\nSimilarly, refer to a test profile, specified in a separate file:\nnextflow.config\ntest { includeConfig './configs/test.config' }\nAdding a manifest to nextflow.config\nmanifest {\n    name = 'isugifNF/tutorial'\n    author = 'Andrew Severin'\n    homePage = 'www.bioinformaticsworkbook.org'\n    description = 'nextflow bash'\n    mainScript = 'main.nf'\n    version = '1.0.0'\n}\nUsing a label for a process allows granular control of a process’ configuration\nmain.nf\nprocess runBlast { \n    label 'blast'\n}\nnextflow.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 02:00:00'\n    withLabel: blast { module = 'blast-plus' }\n}\n\nThe label has to be placed before the input section.\n\nLoading a module specifically for a process\nprocess runBlast {\n\n    module = 'blast-plus'\n    publishDir \"${params.outdir}/blastout\"\n\n    input:\n    path queryFile from queryFile_ch\n    .\n    .\n    . // these three dots mean I didn't paste the whole process.\n}\nEnabling docker in the nextflow.config\ndocker { docker.enabled = true }\n\nThe docker container can be specified in the process, e.g.\n\ncontainer = 'ncbi/blast'\nor\ncontainer = `quay.io/biocontainers/blast/2.2.31--pl526he19e7b1_5`\n\nWe can include additional options to pass to the container as well:\n\ncontainerOptions = \"--bind $launchDir/$params.outdir/config:/augustus/config\"\nprojectDir refers to the directory where the main workflow script is located. (It used to be called baseDir.)\nRefering to local directories from within a docker container: create a channel\n\nWorking in containers, we need a way to pass the database file location directly into the runBlast process without the need of the local path.\n\nRepeating a process over each element of a channel with each: input repeaters\nTurning a queue channel into a value channel, which can be used multiple times.\n\nA value channel is implicitly created by a process when it is invoked with a simple value.\nA value channel is also implicitly created as output for a process whose inputs are all value channels.\nA queue channel can be converted into a value channel by returning a single value, using e.g. first, last, collect, count, min, max, reduce, sum, etc. For example: the runBlast process receives three inputs in the following example:\n\nthe queryFile_ch queue channel, with multiple sequences.\nthe dbDir_ch value channel, created by calling .first(), which is reused for all elements of queryFile_ch\nthe dbName_ch value channel, which is also reused for all elements of queryFile_ch\n\n\nworkflow {\n  channel.fromPath(params.dbDir).first()\n  .set { dbDir_ch }\n\n  channel.from(params.dbName).first()\n  .set { dbName_ch }\n\n  queryFile_ch = channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true)\n     res = runBlast(queryFile_ch, dbDir_ch, dbName_ch)\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\n\n\nAdditional resources\n\nSoftware Carpentry course\nNextflow cheat sheet\nAwesome nextflow"
  },
  {
    "objectID": "posts/custom-badges/index.html",
    "href": "posts/custom-badges/index.html",
    "title": "Creating custom badges for your README",
    "section": "",
    "text": "Predefined badges\nMany open source software packages display key pieces of information as badges (aka shields) in their github README, indicating e.g. code coverage, unit test results, version numbers, license, etc.\nThe shields.io website provides many different ready-to-use badges, covering topics such as test results, code coverage, social media logos, activity, and many more.\n     \nBadges can show up to date information. For example, this badge shows the last commit to the github repository for this blog: . They can be returned either in svg (recommended) or png formats, from the img.shields.io and raster.shields.io servers, respectively.\n\n\nCustom badges\nIn addition to predefined outputs, you can also generate your own, entirely custom badges. They can be static like this one  or dynamically retrieve information from a JSON endpoint of your choice.\n\n\nAdding badges to a README.md file\nTo embed badges into your README.md, simply wrap its URL in markdown and surround it with the badges: start and badges: end tags:\n<!-- badges: start -->\n![](https://img.shields.io/github/last-commit/tomsing1/blog)\n<!-- badges: end -->"
  },
  {
    "objectID": "posts/rslist-r-package/index.html",
    "href": "posts/rslist-r-package/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Luckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it’s super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won’t regret it!"
  },
  {
    "objectID": "posts/conda-speedup/index.html",
    "href": "posts/conda-speedup/index.html",
    "title": "2022 normconf: lightning talks",
    "section": "",
    "text": "The full list of lightning talks is available here but here are my favorites:\n\nJacquelin Nolis: Alaska challenged my preconceived notions of storing sunset data\nJenny Bryan: How to name files like a normie\nChelsea Parlett: Why Are You The Way That You Are: Sklearn Quirks\nZachary Chetchavat: Hotkeys for Spreadsheets Cookbook, Practical Solutions from CTRL-Arrow, to F4\nShoili Pal: Data Science Intake Forms\nAmanda Fioritto: Qualify: The SQL Filtering Pattern You Never Knew You Needed\nJuulia Suvilehto: Trying to convince academics to use git\nAnuvabh Dutt: Config files for fast and reproducible ML experiments\nJane Adams: How to make six figures in an hour (slides)\nBryan Bischof: Toss that (model) in an endpoint\nSophia Yang: PyScript: Run Python in your HTML\nVictor Geislinger: Staying Alive: Persistent SSH Sessions w/ tmux\nTom Baldwin: Putting Git’s commit hash in version, two ways"
  },
  {
    "objectID": "posts/r-update-with-rig/index.html",
    "href": "posts/r-update-with-rig/index.html",
    "title": "Updating R the easy way: using rig command line tool",
    "section": "",
    "text": "I had previously installed rig with brew\nbrew tap r-lib/rig\nbrew install --cask rig\nso I first checked if there were any updates available for rig itself:\nbrew upgrade --cask rig\nThis command updated rig from version 0.5.0 to 0.5.2.\nThen I listed the R versions currently installed on my system:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.1)\nAt this point, I was using R release 4.2.1. Next, I updated to the latest release\nrig install\n\n[INFO] Downloading https://cloud.r-project.org/bin/macosx/base/R-4.2.2.pkg -> /tmp/rig/x86_64-R-4.2.2.pkg\n[INFO] Running installer\n[INFO] > installer: Package name is R 4.2.2 for macOS\n[INFO] > installer: Installing at base path /\n[INFO] > installer: The install was successful.\n[INFO] Forgetting installed versions\n[INFO] Fixing permissions\n[INFO] Adding R-* quick links (if needed)\n[INFO] Setting default CRAN mirror\n[INFO] Installing pak for R 4.2 (if not installed yet)\nOnce the rig install command had completed, my system had updated itself to R version 4.2.2:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.2)\nNow a new R session starts with R 4.2.2\n>R\n\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nThank you, Gábor!"
  },
  {
    "objectID": "posts/2022-normconf/index.html",
    "href": "posts/2022-normconf/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Luckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it’s super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won’t regret it!"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html",
    "href": "posts/postgres-full-text-search/index.html",
    "title": "Full text search in Postgres - the R way",
    "section": "",
    "text": "I have been learning how to organize, search and modify data in a Postgres database by working through Anthony DeBarros’ excellent book Practical SQL.\nBecause I currently perform most of my data analyses in R, I am using the great RPostgres, DBI and glue packages to interface with Postgres - without ever leaving my R session.\nToday I learned how to create a full text search index and how to search it with one or more search terms."
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "href": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "title": "Full text search in Postgres - the R way",
    "section": "Connecting to Postgres",
    "text": "Connecting to Postgres\nFor this example, I created a toy database full_text_search in my local Postgres server. I connect to it with the DBI::dbConnect command, and by passing it the RPostgres::Postgres() driver.\n\nlibrary(DBI)\nlibrary(glue)\nlibrary(RPostgres)\n\n# Connect to a (prexisting) postgres database called `full_text_search`\ncon <- DBI::dbConnect(\n  dbname = \"full_text_search\",\n  drv = RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432L,\n  user = \"postgres\"\n  )\n\nBecause this is a toy example, I start with a fresh table datasets. (In case it already exists from previous experimentation, I drop the table if necessary).\nLet’s define four fields for the table:\n\nid: the unique identifier\nname: the short name of each entry\ntitle: a longer title\ndescription: a paragraph describing the entry\ncreated: a date and time the entry was added to the database\n\n\n# drop the `datasets` table if it already exists\nif (DBI::dbExistsTable(con, \"datasets\")) DBI::dbRemoveTable(con, \"datasets\")\n\n# create the empty `datasets` table\nsql <- glue_sql(\"\n      CREATE TABLE IF NOT EXISTS datasets (\n      id bigserial PRIMARY KEY,\n      name text,\n      title text,\n      description text,\n      created timestamp with time zone default current_timestamp not null\n    );\", .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbReadTable(con, \"datasets\")\n\n[1] id          name        title       description created    \n<0 rows> (or 0-length row.names)\n\n\nInitially, our new database is empty. Let’s populate them with three entries, each describing a popular dataset shipped with R’s built-in datasets package.\n\n# some example entries\nbuildin_datasets <- list(\n  mtcars = list(\n    \"name\" = \"mtcars\", \n    \"title\" = \"The built-in mtcars dataset from the datasets R package.\",\n    \"description\" = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The data was extracted from the 1974 Motor Trend US magazine, and \ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973–74 models).\")\n  ), \n  airmiles = list(\n    name = \"airmiles\",\n    title = \"The built-in airmiles dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The revenue passenger miles flown by commercial airlines in the United\nStates for each year from 1937 to 1960.\")\n  ),\n  attitude = list(\n    name = \"attitude\", \n    title = \"The built-in attitude dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"From a survey of the clerical employees of a large financial\norganization, the data are aggregated from the questionnaires of the\napproximately 35 employees for each of 30 (randomly selected) departments. \nThe numbers give the percent proportion of favourable responses to seven\nquestions in each department.\")\n  )\n)\n\nNext, we loop over each element of the list and use the glue_sql() command to unpack both the names (names(dataset)) and the values of each field for this entry. Then we update the datasets table with this new information.\nAfterward, we retrieve the name and title fields to verify the correct import:\n\nfor (dataset in buildin_datasets) {\n  sql <- glue_sql(\n    \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({dataset*});\", \n    .con = con)\n  res <- suppressMessages(DBI::dbSendStatement(con, sql))\n  DBI::dbClearResult(res)\n}\nDBI::dbGetQuery(con, \"SELECT name, title from datasets;\")\n\n      name                                                     title\n1   mtcars  The built-in mtcars dataset from the datasets R package.\n2 airmiles The built-in airmiles dataset from the datasets R package\n3 attitude The built-in attitude dataset from the datasets R package\n\n\nMy goal is to enable full-text search for the description field. First, we need to add a tsvector field and populate it with the tokenized contents of each description.\n\n# create a column to hold tokens for full text search\nsql <- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbListFields(con, \"datasets\")\n\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n\n\nAt this point, the search_description_text field is still empty. Let’s copy the descriptions into it - and tokenize them at the same time. For illustration, we retrieve the tokens for the first dataset:\n\n# copy the description into search tokens\nsql <- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 3\n\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n\n\nTo speed up the full-text search, we add a Generalized Inverted Index (GIN) index for the search_description_text column as well:\n\n# create the search index\nsql <- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nNow we are ready to perform our first search. Let’s look up the term data in the description fields. (The ts_headline command returns the location of the match, e.g. the context it was found in.)\n\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbFetch(res)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n\nDBI::dbClearResult(res)\n\nWe can also combine search terms, e.g. searching for either employee or motor terms:\n\n# using multiple search terms\nterm <- \"employee | motor\"  # OR\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbFetch(res)\n\n  id     name                                            ts_headline\n1  1   mtcars                from the 1974 <Motor> Trend US magazine\n2  3 attitude clerical <employees> of a large financial organization\n\nDBI::dbClearResult(res)\n\nSimilarly, we can narrow our search by requiring both data and employee terms to appear in the same description:\n\nterm <- \"data & employee\"  # AND\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbFetch(res)\n\n  id     name                                            ts_headline\n1  3 attitude clerical <employees> of a large financial organization\n\nDBI::dbClearResult(res)\n\nThat’s it. Thanks again to Anthony DeBarros’ for his excellent introduction to Practical SQL!"
  }
]