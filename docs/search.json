[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "Image credits: tOrange.biz, CC BY 4.0, via Wikimedia Commons"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is Thomas Sandmann‚Äôs personal blog, created with Quarto. I am planning to share e.g.¬†‚ÄúThings I learned today‚Äù (TIL) and other pieces of news around Computational Biolgy and Data Science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Figure size, layout & tabsets with Quarto\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull text search in Postgres - the R way\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\npostgres\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating R the easy way: using rig command line tool\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022 normconf: lightning talks\n\n\n\n\n\n\n\nTIL\n\n\nconference\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rlist R package\n\n\n\n\n\n\n\nTIL\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nCreating custom badges for your README\n\n\n\n\n\n\n\nTIL\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning nextflow: blasting multiple sequences\n\n\n\n\n\n\n\nTIL\n\n\nnextflow\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython type hints\n\n\n\n\n\n\n\nTIL\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nFujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex\n\n\n\n\n\n\n\nLiterature\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRefreshing & exporting temporary AWS credentials\n\n\n\n\n\n\n\nTIL\n\n\nAWS\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling pyroe with conda\n\n\n\n\n\n\n\nTIL\n\n\nconda\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nThomas Sandmann\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Thomas Sandmann‚Äôs blog."
  },
  {
    "objectID": "posts/pyroe-installation/index.html",
    "href": "posts/pyroe-installation/index.html",
    "title": "Installing pyroe with conda",
    "section": "",
    "text": "It can be installed either using pip or conda, and the latter will install additional dependencies (e.g.¬†bedtools) and include the load_fry() as well.\nTo install pyroe with conda, I first followed bioconda‚Äôs instructions to add and configure the required channels:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\nand then installed pyroe\nconda install pyroe\nNow I can convert alevin-fry output to one of the following formats: zarr, csvs, h5ad or loom.\npyroe convert --help"
  },
  {
    "objectID": "posts/aws-export-credentials/index.html",
    "href": "posts/aws-export-credentials/index.html",
    "title": "Refreshing & exporting temporary AWS credentials",
    "section": "",
    "text": "Today I learned how to configure and refresh these credentials in the command line, as well how to export them either as environmental variables or write them to the credentials file where tools that do not interact with AWS SSO natively can access them.\n\nConfiguring an AWS SSO profile\nFirst, we need to configure a named profile for use with AWS SSO. The following AWS CLI version 2 command will interactively walk you through the necessary steps:\naws configure sso\nThe information you provide will be written to the config file, located in the ~/.aws directory on Mac OS. Here is an example:\n[profile my-dev-profile]\nsso_start_url = https://my-sso-portal.awsapps.com/start\nsso_region = us-east-1\nsso_account_id = 123456789011\nsso_role_name = readOnly\nregion = us-west-2\noutput = json\n\n\nLogging into the AWS SSO profile\nNow we can log into AWS SSO and request temporary credentials:\naws sso login --profile my-dev-profile\nThis command will try to open a web browser for you and prompt you to confirm the login. Alternatively, you can copy & paste the displayed URL and manually enter the confirmation code output by the command.\nIf the login was successful, you can now adopt the my-dev-profile when using the AWS CLI, e.g.\naws s3 ls --profile my-dev-profile\nThe AWS SSO endpoint recognizes many environmental variables that you can use to specify defaults, e.g.\n\nAWS_PROFILE: The profile to use (e.g.¬†my-dev-profile)\nAWS_SHARED_CREDENTIALS_FILE: the location of the shared credentials files (default on Mac OS: ~/.aws/.credentials)\nAWS_CONFIG_FILE: the location of the AWS CLI configuration file (default on Mac OS: ~/.aws.config)\n\n\n\nAccessing temporary credentials\nThe AWS CLI and many of the AWS SKDs will automatically detect and use SSO credentials. But other tools might not (yet) be compatible with this authentication route. Instead, they might\n\nread credentials for a profile from the credentials file\nrely on environmental variables, e.g.¬†AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n\nTo expose the temporary credentials, Ben Kehoe has made the aws-export-credentials tool available.\n\n\nInstalling aws-export-credentials\nThe recommended way to install aws-export-credentials is via pipx because it will automatically make it available in your PATH.\n\nIf you don‚Äôt have pipx available on your system, install it first.\nNext, install aws-export-credentials by executing the following steps in your shell:\n\npipx ensurepath  # in case you haven't run this before\npipx install aws-export-credentials\naws-export-credentials --version  # verify the installation\n\n\nUpdating the credentials file\nAt the beginning of your workday - or whenever needed - run the following set of commands. (Replace the SSO profile with the one you want to adopt.)\nPROFILE=\"my-dev-profile\"\n\n# retrieve new credentials from AWS\naws sso login --profile \"${PROFILE}\"\n\n# write the temporary credentials to the ~/.aws/credentials file\naws-export-credentials \\\n  --profile \"${PROFILE}\" \\\n  --credentials-file-profile \"${PROFILE}\"\nThis will refresh the credentials (via aws sso login) and then write them to the my-dev-profile profiles in the ~/.aws/.credentials file. Now we can access them e.g.¬†in the aws.s3 R package:\nlibrary(aws.s3)\nlibrary(aws.signature)\naws.signature::use_credentials(profile = \"my-dev-profile\")\naws.s3::bucketlist()\n\n\nExposing environmental variables\nSome tools only recognize environmental variables. Luckily, aws-export-credentials can automate this process, too:\nexport $(aws-export-credentials --profile my-dev-profile --env-export)\nwill export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY variables in your shell session.\n\n\nSourcing credentials with an external process\nFinally, you can also include a command that looks up credentials as a credential_process in your config file. (More information here) But that‚Äôs not a use case I have explored, yet."
  },
  {
    "objectID": "posts/fujita_2022/index.html",
    "href": "posts/fujita_2022/index.html",
    "title": "Fujita et al: Cell-subtype specific effects of genetic variation in the aging and Alzheimer cortex",
    "section": "",
    "text": "Fujita et al, Figure 1A/B: (A) Schema of the study. (B) UMAP visualization of 1,509,626 nuclei from 424 donors. Each of the seven major cell types is labeled with a different color.‚Äù\n\n\n\nThis large sample size enabled them to assess the effect of genetic variation (e.g.¬†single-nucleotide variants) on gene expression - one cell type at a time. The authors created pseudo-bulk gene expression profiles for each patient for 7 cell types and 81 cell subtypes.\nBecause neurons are highly abundant in the DLPFC, the largest number of nuclei originated from neurons, and the statistical power to detect eQTLs was lower in rarer cell types (e.g.¬†microglia). This highlights the potential of enrichment methods, e.g.¬†by fluorescent activate nuclei sorting (FANS) approached. (See e.g. (Kamath et al. 2022), who specifically enriched dopaminergic neurons or (Sadick et al. 2022), who enriched astrocytes and oligodendrocytes.)\nFujita et al were able to identify ~ 10,000 eGenes1, about half of which were shared across cell types. For example, they identified a novel eQTL (rs128648) for the APOE gene specifically in microglia.\nHaving identified novel eQTL relationships in vivo, the authors then used bulk RNA-seq measurements from a panel of induced pluripotent stem cells that had been differentiated either into neurons (iNeurons) or astrocytes (iAstrocytes) to test whether they could also observe the variants‚Äô effects in vitro.\nDespite a relatively small sample size, a subset of eQTLs were replicated. But the the authors also point out unexpected discrepancies in the MAPT locus where they observed variant effects in the opposite direction from what they had observed by snRNA-seq.\nGene expression was significantly heritable in most cell types (except for those from which only small numbers of nuclei had been sampled). This allowed the authors to use their snRNA-seq dataset to impute cell type specific gene expression for large GWAS studies, e.g.¬†for Alzheimer‚Äôs Disease, ALS, Parkinson‚Äôs Disease, and schizophrenia. This TWAS analysis detected e.g.¬†48 novel loci associated with AD in microglia, 22 of which had not been implicated previously.\nIn summary, this work by Fujita et al is an impressive achievement, demonstrating that single-cell/single-nuclei approaches have now become sufficiently scalable to power human genetics analyses.\nThe authors have already made the raw data for their study available on the AD Knowledge Portal. Thank you for sharing your data!\n\n\n\n\n\nReferences\n\nFujita, Masashi, Zongmei Gao, Lu Zeng, Cristin McCabe, Charles C. White, Bernard Ng, Gilad Sahar Green, et al. n.d. ‚ÄúCell-Subtype Specific Effects of Genetic Variation in the Aging and Alzheimer Cortex.‚Äù https://doi.org/10.1101/2022.11.07.515446.\n\n\nKamath, Tushar, Abdulraouf Abdulraouf, S. J. Burris, Jonah Langlieb, Vahid Gazestani, Naeem M. Nadaf, Karol Balderrama, Charles Vanderburg, and Evan Z. Macosko. 2022. ‚ÄúSingle-Cell Genomic Profiling of Human Dopamine Neurons Identifies a Population That Selectively Degenerates in Parkinson‚Äôs Disease.‚Äù Nature Neuroscience, May, 1‚Äì8. https://doi.org/10.1038/s41593-022-01061-1.\n\n\nSadick, Jessica S., Michael R. O‚ÄôDea, Philip Hasel, Taitea Dykstra, Arline Faustin, and Shane A. Liddelow. 2022. ‚ÄúAstrocytes and Oligodendrocytes Undergo Subtype-Specific Transcriptional Changes in Alzheimer‚Äôs Disease.‚Äù Neuron, April, S0896627322002446. https://doi.org/10.1016/j.neuron.2022.03.008.\n\nFootnotes\n\n\nGene whose expression was significantly associated with one or more genetic variants (FDR < 5%)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/python-hints/index.html",
    "href": "posts/python-hints/index.html",
    "title": "Python type hints",
    "section": "",
    "text": "Code Better with Type Hints ‚Äì Part 1\nCode Better with Type Hints ‚Äì Part 2\nFastAPI‚Äôs typing introduction\nPysheet: typing"
  },
  {
    "objectID": "posts/nextflow-blast-tutorial/index.html",
    "href": "posts/nextflow-blast-tutorial/index.html",
    "title": "Learning nextflow: blasting multiple sequences",
    "section": "",
    "text": "To start learning nextflow, I worked through Andrew Severin‚Äôs excellent Creating a NextFlow workflow tutorial. (The tutorial follows the older DSL1 specification of nextflow, but only a few small modifications were needed to run it under DSL2.)\nThe DSL2 code I wrote is here and these are notes I took while working through the tutorial:\n\nTo make a variable a pipeline parameter prepend it with params., then specify them in the command line:\nmain.nf:\n#! /usr/bin/env nextflow\nparams.query=\"file.fasta\"\nprintln \"Querying file $params.query\"\nshell command:\nnextflow run main.nf --query other_file.fasta\nThe -log argument directs logging to the specified file.\nnextflow -log nextflo.log run main.nf \nTo clean up intermediate files automatically upon workflow completion, use the cleanup parameter within a profile.\nprofiles {\n  standard {\n      cleanup = true\n  }\n  debug {\n      cleanup = false\n  }\n}\n\nBy convention the standard profile is implicitly used when no other\nprofile is specified by the user.\nCleaning up intermediate files precludes the use of -resume.\n\nThe nextflow.config file sets the global parameters, e.g.\n\nprocess\nmanifest\nexecutor\nprofiles\ndocker\nsingularity\ntimeline\nreport\netc\n\nContents of the work folder for a nextflow task:\n\n.command.begin is the begin script if you have one\n.command.err is useful when it crashes.\n.command.run is the full nextflow pipeline that was run, this is helpful when trouble shooting a nextflow error rather than the script error.\n.command.sh shows what was run.\n.exitcode will have the exit code in it.\n\nDisplaying help messages\nmain.nf\ndef helpMessage() {\nlog.info \"\"\"\n      Usage:\n      The typical command for running the pipeline is as follows:\n      nextflow run main.nf --query QUERY.fasta --dbDir \"blastDatabaseDirectory\" --dbName \"blastPrefixName\"\n\n      Mandatory arguments:\n       --query                        Query fasta file of sequences you wish to BLAST\n       --dbDir                        BLAST database directory (full path required)\n       [...]\n\"\"\"\n}\n\n// Show help message\nif (params.help) {\n    helpMessage()\n    exit 0\n}\nshell command:\nnextflow run main.nf --help\nThe publishDir directive accepts arguments like mode and pattern to fine tune its behavior, e.g.\noutput:\nfile(\"${label}/short_summary.specific.*.txt\")\npublishDir \"${params.outdir}/BUSCOResults/${label}/\", mode: 'copy', pattern: \"${label}/short_summary.specific.*.txt\"\nDSL2 allows piping, e.g.\nworkflow {\n  res = Channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true) |\n      runBlast\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\nAdd a timeline report to the output with\ntimeline {\n    enabled = true\n    file = \"$params.outdir/timeline.html\"\n}\n(in nextflow.config).\nAdd a detailed execution report with\nreport {\nenabled = true\nfile = \"$params.outdir/report.html\"\n}\n(in nextflow.config).\nInclude a profile-specific configuration file\nnextflow.config\nprofiles {\n    slurm { includeConfig './configs/slurm.config' }\n}\nconfigs/slurm.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 24:00:00'\n}\nand use it via nextflow run main.nf -profile slurm\nSimilarly, refer to a test profile, specified in a separate file:\nnextflow.config\ntest { includeConfig './configs/test.config' }\nAdding a manifest to nextflow.config\nmanifest {\n    name = 'isugifNF/tutorial'\n    author = 'Andrew Severin'\n    homePage = 'www.bioinformaticsworkbook.org'\n    description = 'nextflow bash'\n    mainScript = 'main.nf'\n    version = '1.0.0'\n}\nUsing a label for a process allows granular control of a process‚Äô configuration\nmain.nf\nprocess runBlast { \n    label 'blast'\n}\nnextflow.config\nprocess {\n    executor = 'slurm'\n    clusterOptions =  '-N 1 -n 16 -t 02:00:00'\n    withLabel: blast { module = 'blast-plus' }\n}\n\nThe label has to be placed before the input section.\n\nLoading a module specifically for a process\nprocess runBlast {\n\n    module = 'blast-plus'\n    publishDir \"${params.outdir}/blastout\"\n\n    input:\n    path queryFile from queryFile_ch\n    .\n    .\n    . // these three dots mean I didn't paste the whole process.\n}\nEnabling docker in the nextflow.config\ndocker { docker.enabled = true }\n\nThe docker container can be specified in the process, e.g.\n\ncontainer = 'ncbi/blast'\nor\ncontainer = `quay.io/biocontainers/blast/2.2.31--pl526he19e7b1_5`\n\nWe can include additional options to pass to the container as well:\n\ncontainerOptions = \"--bind $launchDir/$params.outdir/config:/augustus/config\"\nprojectDir refers to the directory where the main workflow script is located. (It used to be called baseDir.)\nRefering to local directories from within a docker container: create a channel\n\nWorking in containers, we need a way to pass the database file location directly into the runBlast process without the need of the local path.\n\nRepeating a process over each element of a channel with each: input repeaters\nTurning a queue channel into a value channel, which can be used multiple times.\n\nA value channel is implicitly created by a process when it is invoked with a simple value.\nA value channel is also implicitly created as output for a process whose inputs are all value channels.\nA queue channel can be converted into a value channel by returning a single value, using e.g.¬†first, last, collect, count, min, max, reduce, sum, etc. For example: the runBlast process receives three inputs in the following example:\n\nthe queryFile_ch queue channel, with multiple sequences.\nthe dbDir_ch value channel, created by calling .first(), which is reused for all elements of queryFile_ch\nthe dbName_ch value channel, which is also reused for all elements of queryFile_ch\n\n\nworkflow {\n  channel.fromPath(params.dbDir).first()\n  .set { dbDir_ch }\n\n  channel.from(params.dbName).first()\n  .set { dbName_ch }\n\n  queryFile_ch = channel\n      .fromPath(params.query)\n      .splitFasta(by: 1, file:true)\n     res = runBlast(queryFile_ch, dbDir_ch, dbName_ch)\n  res.collectFile(name: 'blast_output_combined.txt', storeDir: params.outdir)\n}\n\n\nAdditional resources\n\nSoftware Carpentry course\nNextflow cheat sheet\nAwesome nextflow"
  },
  {
    "objectID": "posts/custom-badges/index.html",
    "href": "posts/custom-badges/index.html",
    "title": "Creating custom badges for your README",
    "section": "",
    "text": "Predefined badges\nMany open source software packages display key pieces of information as badges (aka shields) in their github README, indicating e.g.¬†code coverage, unit test results, version numbers, license, etc.\nThe shields.io website provides many different ready-to-use badges, covering topics such as test results, code coverage, social media logos, activity, and many more.\n     \nBadges can show up to date information. For example, this badge shows the last commit to the github repository for this blog: . They can be returned either in svg (recommended) or png formats, from the img.shields.io and raster.shields.io servers, respectively.\n\n\nCustom badges\nIn addition to predefined outputs, you can also generate your own, entirely custom badges. They can be static like this one  or dynamically retrieve information from a JSON endpoint of your choice.\n\n\nAdding badges to a README.md file\nTo embed badges into your README.md, simply wrap its URL in markdown and surround it with the badges: start and badges: end tags:\n<!-- badges: start -->\n![](https://img.shields.io/github/last-commit/tomsing1/blog)\n<!-- badges: end -->"
  },
  {
    "objectID": "posts/rslist-r-package/index.html",
    "href": "posts/rslist-r-package/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Luckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it‚Äôs super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won‚Äôt regret it!"
  },
  {
    "objectID": "posts/conda-speedup/index.html",
    "href": "posts/conda-speedup/index.html",
    "title": "2022 normconf: lightning talks",
    "section": "",
    "text": "The full list of lightning talks is available here but here are my favorites:\n\nJacquelin Nolis: Alaska challenged my preconceived notions of storing sunset data\nJenny Bryan: How to name files like a normie\nChelsea Parlett: Why Are You The Way That You Are: Sklearn Quirks\nZachary Chetchavat: Hotkeys for Spreadsheets Cookbook, Practical Solutions from CTRL-Arrow, to F4\nShoili Pal: Data Science Intake Forms\nAmanda Fioritto: Qualify: The SQL Filtering Pattern You Never Knew You Needed\nJuulia Suvilehto: Trying to convince academics to use git\nAnuvabh Dutt: Config files for fast and reproducible ML experiments\nJane Adams: How to make six figures in an hour (slides)\nBryan Bischof: Toss that (model) in an endpoint\nSophia Yang: PyScript: Run Python in your HTML\nVictor Geislinger: Staying Alive: Persistent SSH Sessions w/ tmux\nTom Baldwin: Putting Git‚Äôs commit hash in version, two ways"
  },
  {
    "objectID": "posts/r-update-with-rig/index.html",
    "href": "posts/r-update-with-rig/index.html",
    "title": "Updating R the easy way: using rig command line tool",
    "section": "",
    "text": "I had previously installed rig with brew\nbrew tap r-lib/rig\nbrew install --cask rig\nso I first checked if there were any updates available for rig itself:\nbrew upgrade --cask rig\nThis command updated rig from version 0.5.0 to 0.5.2.\nThen I listed the R versions currently installed on my system:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.1)\nAt this point, I was using R release 4.2.1. Next, I updated to the latest release\nrig install\n\n[INFO] Downloading https://cloud.r-project.org/bin/macosx/base/R-4.2.2.pkg -> /tmp/rig/x86_64-R-4.2.2.pkg\n[INFO] Running installer\n[INFO] > installer: Package name is R 4.2.2 for macOS\n[INFO] > installer: Installing at base path /\n[INFO] > installer: The install was successful.\n[INFO] Forgetting installed versions\n[INFO] Fixing permissions\n[INFO] Adding R-* quick links (if needed)\n[INFO] Setting default CRAN mirror\n[INFO] Installing pak for R 4.2 (if not installed yet)\nOnce the rig install command had completed, my system had updated itself to R version 4.2.2:\nrig list\n  4.1   (R 4.1.3)\n* 4.2   (R 4.2.2)\nNow a new R session starts with R 4.2.2\n>R\n\nR version 4.2.2 (2022-10-31) -- \"Innocent and Trusting\"\nCopyright (C) 2022 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nThank you, G√°bor!"
  },
  {
    "objectID": "posts/2022-normconf/index.html",
    "href": "posts/2022-normconf/index.html",
    "title": "The rlist R package",
    "section": "",
    "text": "Luckily, there is help: the rlist R package offers lots of great functionality to extract, combine, filter, select and convert nested lists. It works with JSON arrays / files out of the box as well, so it‚Äôs super useful when you deal with the response from REST APIs, for example.\nAvailable from a your nearest CRAN mirror.\nCheck it out, you won‚Äôt regret it!"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html",
    "href": "posts/postgres-full-text-search/index.html",
    "title": "Full text search in Postgres - the R way",
    "section": "",
    "text": "I have been learning how to organize, search and modify data in a Postgres database by working through Anthony DeBarros‚Äô excellent book Practical SQL.\nBecause I currently perform most of my data analyses in R, I am using the great RPostgres, DBI and glue packages to interface with Postgres - without ever leaving my R session.\nToday I learned how to create a full text search index and how to search it with one or more search terms."
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "href": "posts/postgres-full-text-search/index.html#connecting-to-postgres",
    "title": "Full text search in Postgres - the R way",
    "section": "Connecting to Postgres",
    "text": "Connecting to Postgres\nFor this example, I created a toy database full_text_search in my local Postgres server. I connect to it with the DBI::dbConnect command, and by passing it the RPostgres::Postgres() driver.\n\nlibrary(DBI)\nlibrary(glue)\nlibrary(RPostgres)\nlibrary(sessioninfo)\n\n# Connect to a (prexisting) postgres database called `full_text_search`\ncon <- DBI::dbConnect(\n  dbname = \"full_text_search\",\n  drv = RPostgres::Postgres(),\n  host = \"localhost\",\n  port = 5432L,\n  user = \"postgres\"\n  )"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "href": "posts/postgres-full-text-search/index.html#creating-and-populating-a-table",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating and populating a table",
    "text": "Creating and populating a table\nBecause this is a toy example, I start with a fresh table datasets. (In case it already exists from previous experimentation, I drop the table if necessary).\nLet‚Äôs define four fields for the table:\n\nid: the unique identifier\nname: the short name of each entry\ntitle: a longer title\ndescription: a paragraph describing the entry\ncreated: a date and time the entry was added to the database\n\n\n# drop the `datasets` table if it already exists\nif (DBI::dbExistsTable(con, \"datasets\")) DBI::dbRemoveTable(con, \"datasets\")\n\n# create the empty `datasets` table\nsql <- glue_sql(\"\n      CREATE TABLE IF NOT EXISTS datasets (\n      id bigserial PRIMARY KEY,\n      name text,\n      title text,\n      description text,\n      created timestamp with time zone default current_timestamp not null\n    );\", .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbReadTable(con, \"datasets\")\n\n[1] id          name        title       description created    \n<0 rows> (or 0-length row.names)\n\n\nInitially, our new database is empty. Let‚Äôs populate them with three entries, each describing a popular dataset shipped with R‚Äôs built-in datasets package.\n\n# some example entries\nbuildin_datasets <- list(\n  mtcars = list(\n    \"name\" = \"mtcars\", \n    \"title\" = \"The built-in mtcars dataset from the datasets R package.\",\n    \"description\" = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The data was extracted from the 1974 Motor Trend US magazine, and \ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973‚Äì74 models).\")\n  ), \n  airmiles = list(\n    name = \"airmiles\",\n    title = \"The built-in airmiles dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"The revenue passenger miles flown by commercial airlines in the United\nStates for each year from 1937 to 1960.\")\n  ),\n  attitude = list(\n    name = \"attitude\", \n    title = \"The built-in attitude dataset from the datasets R package\",\n    description = gsub(\n      \"\\r?\\n|\\r\", \" \", \n      \"From a survey of the clerical employees of a large financial\norganization, the data are aggregated from the questionnaires of the\napproximately 35 employees for each of 30 (randomly selected) departments. \nThe numbers give the percent proportion of favourable responses to seven\nquestions in each department.\")\n  )\n)\n\nNext, we loop over each element of the list and use the glue_sql() command to unpack both the names (names(dataset)) and the values of each field for this entry. Then we update the datasets table with this new information.\nAfterward, we retrieve the name and title fields to verify the correct import:\n\nfor (dataset in buildin_datasets) {\n  sql <- glue_sql(\n    \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({dataset*});\", \n    .con = con)\n  res <- suppressMessages(DBI::dbSendStatement(con, sql))\n  DBI::dbClearResult(res)\n}\nDBI::dbGetQuery(con, \"SELECT name, title from datasets;\")\n\n      name                                                     title\n1   mtcars  The built-in mtcars dataset from the datasets R package.\n2 airmiles The built-in airmiles dataset from the datasets R package\n3 attitude The built-in attitude dataset from the datasets R package"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-a-tokenized-index-for-full-text-searches",
    "href": "posts/postgres-full-text-search/index.html#creating-a-tokenized-index-for-full-text-searches",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating a tokenized index for full-text searches",
    "text": "Creating a tokenized index for full-text searches\nMy goal is to enable full-text search for the description field. First, we need to add a tsvector field and populate it with the tokenized contents of each description.\n\n# create a column to hold tokens for full text search\nsql <- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbClearResult(res)\nDBI::dbListFields(con, \"datasets\")\n\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n\n\nAt this point, the search_description_text field is still empty. Let‚Äôs copy the descriptions into it - and tokenize them at the same time. For illustration, we retrieve the tokens for the first dataset:\n\n# copy the description into search tokens\nsql <- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 3\n\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n\n\nTo speed up the full-text search, we add a Generalized Inverted Index (GIN) index for the search_description_text column as well:\n\n# create the search index\nsql <- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#searching",
    "href": "posts/postgres-full-text-search/index.html#searching",
    "title": "Full text search in Postgres - the R way",
    "section": "Searching!",
    "text": "Searching!\nOur goal is to enable full-text search for the description field. Let‚Äôs look up the term data. To perform full-text search, both the records to search and our query need to be tokinzed first, with the to_tsvector and to_tsquery functions, respectively.\nHere is an example of the tokens that are generated:\n\nsql <- glue_sql(\n  \"SELECT to_tsvector('This is a my test phrase, and what \n                       a beautiful phrase it is.')\n   to_tsquery\", con = con)\nDBI::dbGetQuery(con, sql)\n\n                          to_tsquery\n1 'beauti':10 'phrase':6,11 'test':5\n\n\nThe following query correctly returns all records whose descriptions contain the word data:\n\n# search the description field\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name\n1  1   mtcars\n2  3 attitude\n\n\nWe can enrich the output by returning the output of the ts_headline function, highlighting the location / context of the the matched term:\n\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n\n\nWe can also combine search terms, e.g.¬†searching for either employee or motor terms:\n\n# using multiple search terms\nterm <- \"employee | motor\"  # OR\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars                from the 1974 <Motor> Trend US magazine\n2  3 attitude clerical <employees> of a large financial organization\n\n\nSimilarly, we can narrow our search by requiring both data and employee terms to appear in the same description:\n\nterm <- \"data & employee\"  # AND\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector(description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  3 attitude clerical <employees> of a large financial organization"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#reproducibility",
    "href": "posts/postgres-full-text-search/index.html#reproducibility",
    "title": "Full text search in Postgres - the R way",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nsessioninfo::session_info()\n\n‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2022-12-12\n pandoc   2.19.2 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/tools/ (via rmarkdown)\n\n‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n package     * version date (UTC) lib source\n askpass       1.1     2019-01-13 [1] CRAN (R 4.2.0)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.2.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.2.0)\n blob          1.2.3   2022-04-10 [1] CRAN (R 4.2.0)\n cli           3.4.1   2022-09-23 [1] CRAN (R 4.2.0)\n credentials   1.3.2   2021-11-29 [1] CRAN (R 4.2.0)\n DBI         * 1.1.3   2022-06-18 [1] CRAN (R 4.2.0)\n digest        0.6.30  2022-10-18 [1] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.0)\n evaluate      0.18    2022-11-07 [1] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.0)\n glue        * 1.6.2   2022-02-24 [1] CRAN (R 4.2.0)\n hms           1.1.2   2022-08-19 [1] CRAN (R 4.2.0)\n htmltools     0.5.4   2022-12-07 [1] CRAN (R 4.2.0)\n htmlwidgets   1.5.4   2021-09-08 [1] CRAN (R 4.2.0)\n jsonlite      1.8.4   2022-12-06 [1] CRAN (R 4.2.0)\n knitr         1.41    2022-11-18 [1] CRAN (R 4.2.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.2.0)\n lubridate     1.9.0   2022-11-06 [1] CRAN (R 4.2.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.0)\n openssl       2.0.5   2022-12-06 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.0)\n Rcpp          1.0.9   2022-07-08 [1] CRAN (R 4.2.0)\n rlang         1.0.6   2022-09-24 [1] CRAN (R 4.2.0)\n rmarkdown     2.18    2022-11-09 [1] CRAN (R 4.2.0)\n RPostgres   * 1.4.4   2022-05-02 [1] CRAN (R 4.2.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n stringi       1.7.8   2022-07-11 [1] CRAN (R 4.2.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.2.0)\n sys           3.4.1   2022-10-18 [1] CRAN (R 4.2.0)\n timechange    0.1.1   2022-11-04 [1] CRAN (R 4.2.0)\n vctrs         0.5.1   2022-11-16 [1] CRAN (R 4.2.0)\n xfun          0.35    2022-11-16 [1] CRAN (R 4.2.0)\n yaml          2.3.6   2022-10-18 [1] CRAN (R 4.2.0)\n\n [1] /Users/sandmann/Library/R/x86_64/4.2/library\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#adding-new-entries",
    "href": "posts/postgres-full-text-search/index.html#adding-new-entries",
    "title": "Full text search in Postgres - the R way",
    "section": "Adding new entries",
    "text": "Adding new entries\nWe have successfully indexed and searched the existing three entries in our database. But what if we add new information? Let‚Äôs create a new record for the euro dataset.\n\nnew_data = list(\n  name = \"euro\", \n  title = \"The built-in euro dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"The data set euro contains the value of 1 Euro in all currencies\nparticipating in the European monetary union (Austrian Schilling ATS, \nBelgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, \nFrench Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, \nDutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were \nfixed by the European Union on December 31, 1998. To convert old prices to \nEuro prices, divide by the respective rate and round to 2 digits.\")\n)\n\nTo enter this record, we not only have to populate the name, title and description fields - but also the list of tokens derived from the description in the search_description_text column. In other words, we have to execute the to_tsvector function inside our INSERT statement:\n\nsql <- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*}, search_description_text)\n   VALUES ({new_data*}, to_tsvector({new_data[['description']]}));\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nNow, when we search for the term data, we find both the original and the new record:\n\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nres <- suppressMessages(DBI::dbSendStatement(con, sql))\nDBI::dbFetch(res)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n\nDBI::dbClearResult(res)\n\nThat‚Äôs it. Thanks again to Anthony DeBarros‚Äô for his excellent introduction to Practical SQL!"
  },
  {
    "objectID": "posts/postgres-full-text-search/index.html#creating-indices",
    "href": "posts/postgres-full-text-search/index.html#creating-indices",
    "title": "Full text search in Postgres - the R way",
    "section": "Creating indices",
    "text": "Creating indices\nIn the examples above, we performed tokenization of the search term and the description field at run time, e.g.¬†when the query was executed. As our database grows, this will soon become too cumbersome and degrade performance.\nAdding an index to our database will maintain full-text search speed even with large datasets. We have two different options:\n\nCreate an index based on an expression.\nCreate a new field to hold the output of the to_tsvector function, and then index this new field.\n\n\nCreating an expression index\nA simple way to create a full-text index is to include the to_tsvector() expression in the definition of the index itself. Here, we add a Generalized Inverted Index (GIN) index for the description column:\n\nsql = glue_sql(\n  \"CREATE INDEX description_idx ON datasets \n  USING gin(to_tsvector('english', description));\",\n  con = con\n)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nThe same type of query we issued above will now take advantage of the description_idx:\n\n# search the description field using its index\nterm <- \"questioning\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                       ts_headline\n1  3 attitude responses to seven <questions> in each department\n\n\nThe description fields of new records, e.g those that are added later, will automatically be added to the index. Let‚Äôs create a new record for the euro dataset, for example.\n\nnew_data = list(\n  name = \"euro\", \n  title = \"The built-in euro dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"The data set euro contains the value of 1 Euro in all currencies\nparticipating in the European monetary union (Austrian Schilling ATS, \nBelgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, \nFrench Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, \nDutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were \nfixed by the European Union on December 31, 1998. To convert old prices to \nEuro prices, divide by the respective rate and round to 2 digits.\")\n)\nsql <- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*})\n   VALUES ({new_data*});\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nThis new record will now be included in the search results for the term data, for example:\n\n# search the description field using its index\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n    ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n\n\n\n\nAdding a tokenized field for full-text searches\nAlternatively, another option is to create a new column to hold the output of the to_tsvector() function, and then to index it for future use. Let‚Äôs create a new column search_description_text:\n\n# create a column to hold tokens for full text search\nsql <- glue_sql(\n  \"ALTER TABLE datasets\n   ADD COLUMN search_description_text tsvector;\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\nDBI::dbListFields(con, \"datasets\")\n\n[1] \"id\"                      \"name\"                   \n[3] \"title\"                   \"description\"            \n[5] \"created\"                 \"search_description_text\"\n\n\nNext, we tokenize the descriptions field, and store the output in our new search_description_text column:\n\nsql <- glue_sql(\n  \"UPDATE datasets\n   SET search_description_text = to_tsvector('english', description);\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 4\n\n\nHere are the tokens generated from the description of the first record, for example:\n\nDBI::dbGetQuery(con, \n                \"SELECT name, search_description_text from datasets LIMIT 1;\")\n\n    name\n1 mtcars\n                                                                                                                                                                                          search_description_text\n1 '10':17 '1973':27 '1974':7 '32':25 '74':28 'aspect':18 'automobil':20,26 'compris':13 'consumpt':15 'data':2 'design':21 'extract':4 'fuel':14 'magazin':11 'model':29 'motor':8 'perform':23 'trend':9 'us':10\n\n\nAs before, we can add an index - but this time, we index the pre-tokenized search_description_text column instead:\n\n# create the search index\nsql <- glue_sql(\n  \"CREATE INDEX search_description_idx\n   ON datasets\n   USING gin(search_description_text);\",\n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 0\n\n\nTime to run our search again. When we search the search_description_text field, we can omit the to_tsvector() call, because its has been tokenized already:\n\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n\n\nüö® But beware: because we have precalculated the tokens, any new records added to the database will not automatically be processed, nor will they be indexed!\nLet‚Äôs add a final record, the morely dataset:\n\nmore_data = list(\n  name = \"morley\", \n  title = \"The built-in morley dataset from the datasets R package\",\n  description = gsub(\n    \"\\r?\\n|\\r\", \" \", \n    \"A classical data of Michelson (but not this one with Morley) on \nmeasurements done in 1879 on the speed of light. The data consists of five \nexperiments, each consisting of 20 consecutive ‚Äòruns‚Äô. The response is the speed\nof light measurement, suitably coded (km/sec, with 299000 subtracted).\")\n)\n\nTo enter this record, we not only have to populate the name, title and description fields - but also the list of tokens derived from the description in the search_description_text column. In other words, we have to execute the to_tsvector function inside our INSERT statement:\n\nsql <- glue_sql(\n  \"INSERT INTO datasets ({`names(dataset)`*}, search_description_text)\n   VALUES ({more_data*}, to_tsvector({more_data[['description']]}));\", \n  .con = con)\nDBI::dbExecute(con, sql)\n\n[1] 1\n\n\nNow, our query returns both the original matches and the new record:\n\n# search the description field and show the matching location\nterm <- \"data\"\nsql <- glue_sql(\n  \"SELECT id, name,\n  ts_headline(description, to_tsquery('english', {term}),\n     'StartSel = <,\n      StopSel = >,\n      MinWords = 5,\n      MaxWords = 7,\n      MaxFragments = 1')\n  FROM datasets\n  WHERE search_description_text @@ to_tsquery('english', {term})\n  ORDER BY created;\",\n  .con = con)\nDBI::dbGetQuery(con, sql)\n\n  id     name                                            ts_headline\n1  1   mtcars               <data> was extracted from the 1974 Motor\n2  3 attitude financial organization, the <data> are aggregated from\n3  4     euro                     <data> set euro contains the value\n4  5   morley            classical <data> of Michelson (but not this\n\n\n\n\nChoosing between indexing strategies\nAccording to the Postgres documentation:\n\nOne advantage of the separate-column approach over an expression index is that it is not necessary to explicitly specify the text search configuration in queries in order to make use of the index. Another advantage is that searches will be faster, since it will not be necessary to redo the to_tsvector calls to verify index matches. The expression-index approach is simpler to set up, however, and it requires less disk space since the tsvector representation is not stored explicitly.\n\nThat‚Äôs it. Thanks again to Anthony DeBarros‚Äô for his excellent introduction to Practical SQL!"
  },
  {
    "objectID": "posts/quarto-figure-size-and-layout/index.html",
    "href": "posts/quarto-figure-size-and-layout/index.html",
    "title": "Figure size, layout & tabsets with Quarto",
    "section": "",
    "text": "In this document, I am experimenting with various attributes that organize the layout, size and placement of figures of Quarto document. For more details, please check out the official documentation, especially the topics on figures and article layout.\n\n\n\n\n\n\nNote\n\n\n\nFor illustration, I am displaying both the code that generates a simple plot as well as the attributes that determine how it is rendered, e.g.¬†the ::: tags interspersed with the code blocks, and the #| attributes within individual code cells. See the documentation on executable blocks for details.\n\n\nFirst, let‚Äôs generate a simple plot, so we can see the effect of different attributes on how it is rendered in subsequent code cells.\nTo start, we render the output without specifying any custom attributes, e.g. using the default settings for this Quarto website:\n\nlibrary(ggplot2)\ntheme_set(theme_linedraw(base_size = 14))\np <- ggplot(mtcars, aes(x = mpg, y = drat)) + \n  geom_point(color = \"skyblue\", size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  theme(panel.grid = element_blank())\np\n\n\n\n\n\nWidth and height of individual figures\nThe fig-width and fig-height attributes specify the dimensions of the image file that is generated. The out-width attribute determines the size at which that image is displayed in the rendered HTML page.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\nFor example, the same image can be displayed at 50% of the width of the enclosing <div>.\n#| fig-width: 4\n#| figh-height: 5\n#| out-width: \"25%\"\n#| fig-align: \"center\"\n\np\n\n\n\n\n\n\n\n\n\n\nLayout: columns and rows\nThe layout-ncol and layout-nrow attributes govern the placement of multiple figures within the same element. For example, we can place two figures next to each other, in two column.\nThe fig-align attributes specify the figure alignment within each column.\n\n\n\n\n\n\nTip\n\n\n\nThe out-width attribute is always relative to its enclosing element, e.g.¬†here out-width: \"50%\" refers to half of the width of a column, not the page width.\n\n\n::: {layout-ncol=2}\n\n\n\n#| out-width: \"50%\"\n#| fig-align: \"center\"\n\n\n#| out-width: \"30%\"\n#| fig-align: \"right\"\n\n\n\n\np\n\n\n\n\n\n\n\n\n\np\n\n\n\n\n\n\n\n\n\n\n:::\n\n\nTabsets\nTabsets can be used to organize contents, e.g.¬†by hiding content until the other clicks on the tab‚Äôs header.\nThe layout of the first tabset contains just one column and row.\n::: {.panel-tabset}\n\npanel 1panel 2\n\n\n\np\n\n\n\n\n\n\n\n\n\n\nThe second panel is subdivided into two columns. (Note the use of the :::: tag, nested within the ::: parent tag.)\n:::: {layout-ncol=2}\n\n\n\np\n\n\n\n\n\np\n\n\n\n\n\n\n\n::::\n\n\n\n\n\n\n:::"
  }
]