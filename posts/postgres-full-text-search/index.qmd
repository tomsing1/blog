---
title: "Full text search in Postgres - the R way"
author: "Thomas Sandmann"
date: "2022-12-12"
freeze: true
categories: [TIL, R, postgres]
editor: 
  markdown: 
    wrap: 72
---

I have been learning how to organize, search and modify data in a 
[Postgres](https://www.postgresql.org/) 
database by working through 
[Anthony DeBarros'](https://www.wsj.com/news/author/anthony-debarros) 
excellent book 
[Practical SQL](https://nostarch.com/practical-sql-2nd-edition).

Because I currently perform most of my data analyses in R, I am using the 
great 
[RPostgres](https://cran.r-project.org/package=RPostgres),
[DBI](https://cran.r-project.org/package=DBI)
and
[glue](https://cran.r-project.org/package=glue)
packages to interface with Postgres - without ever leaving my R session. 

Today I learned how to create a full text search index and how to search it
with one or more search terms.

## Connecting to Postgres

For this example, I created a toy database `full_text_search` in my local
Postgres server. I connect to it with the `DBI::dbConnect` command, and by 
passing it the `RPostgres::Postgres()` driver.

```{r}
library(DBI)
library(glue)
library(RPostgres)
library(sessioninfo)

# Connect to a (prexisting) postgres database called `full_text_search`
con <- DBI::dbConnect(
  dbname = "full_text_search",
  drv = RPostgres::Postgres(),
  host = "localhost",
  port = 5432L,
  user = "postgres"
  )
```

## Creating and populating a table

Because this is a toy example, I start with a fresh table `datasets`. (In case
it already exists from previous experimentation, I drop the table if necessary).

Let's define four fields for the table:

- `id`: the unique identifier
- `name`: the short name of each entry
- `title`: a longer title
- `description`: a paragraph describing the entry
- `created`: a date and time the entry was added to the database 

```{r}
# drop the `datasets` table if it already exists
if (DBI::dbExistsTable(con, "datasets")) DBI::dbRemoveTable(con, "datasets")

# create the empty `datasets` table
sql <- glue_sql("
      CREATE TABLE IF NOT EXISTS datasets (
      id bigserial PRIMARY KEY,
      name text,
      title text,
      description text,
      created timestamp with time zone default current_timestamp not null
    );", .con = con)
res <- suppressMessages(DBI::dbSendStatement(con, sql))
DBI::dbClearResult(res)
DBI::dbReadTable(con, "datasets")
```

Initially, our new database is empty. Let's populate them with three entries,
each describing a popular dataset shipped with R's built-in
[datasets](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html)
package.

```{r}
# some example entries
buildin_datasets <- list(
  mtcars = list(
    "name" = "mtcars", 
    "title" = "The built-in mtcars dataset from the datasets R package.",
    "description" = gsub(
      "\r?\n|\r", " ", 
      "The data was extracted from the 1974 Motor Trend US magazine, and 
comprises fuel consumption and 10 aspects of automobile design and
performance for 32 automobiles (1973â€“74 models).")
  ), 
  airmiles = list(
    name = "airmiles",
    title = "The built-in airmiles dataset from the datasets R package",
    description = gsub(
      "\r?\n|\r", " ", 
      "The revenue passenger miles flown by commercial airlines in the United
States for each year from 1937 to 1960.")
  ),
  attitude = list(
    name = "attitude", 
    title = "The built-in attitude dataset from the datasets R package",
    description = gsub(
      "\r?\n|\r", " ", 
      "From a survey of the clerical employees of a large financial
organization, the data are aggregated from the questionnaires of the
approximately 35 employees for each of 30 (randomly selected) departments. 
The numbers give the percent proportion of favourable responses to seven
questions in each department.")
  )
)
```

Next, we loop over each element of the list and use the `glue_sql()` command
to unpack both the names (`names(dataset)`) and the values of each field for 
this entry. Then we update the `datasets` table with this new information.

Afterward, we retrieve the `name` and `title` fields to verify the
correct import:

```{r}
for (dataset in buildin_datasets) {
  sql <- glue_sql(
    "INSERT INTO datasets ({`names(dataset)`*})
   VALUES ({dataset*});", 
    .con = con)
  res <- suppressMessages(DBI::dbSendStatement(con, sql))
  DBI::dbClearResult(res)
}
DBI::dbGetQuery(con, "SELECT name, title from datasets;")
```

## Searching!

Our goal is to enable full-text search for the `description` field. 
Let's look up the term `data`. To perform full-text search, both the records to
search and our query need to be tokinzed first, with the `to_tsvector` and
`to_tsquery` functions, respectively.

Here is an example of the tokens that are generated:

```{r}
sql <- glue_sql(
  "SELECT to_tsvector('This is a my test phrase, and what 
                       a beautiful phrase it is.')
   to_tsquery", con = con)
DBI::dbGetQuery(con, sql)
```

The following query correctly returns all records whose descriptions
contain the word `data`:

```{r}
# search the description field
term <- "data"
sql <- glue_sql(
  "SELECT id, name
  FROM datasets
  WHERE to_tsvector(description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

We can enrich the output by returning the output of the `ts_headline` function,
highlighting the location / context of the the matched term:

```{r}
# search the description field and show the matching location
term <- "data"
sql <- glue_sql(
  "SELECT id, name,
    ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE to_tsvector(description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

We can also combine search terms, e.g. searching for either `employee` _or_ 
`motor` terms:

```{r}
# using multiple search terms
term <- "employee | motor"  # OR
sql <- glue_sql(
  "SELECT id, name,
  ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE to_tsvector(description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

Similarly, we can narrow our search by requiring both `data` _and_ `employee`
terms to appear in the same description:

```{r}
term <- "data & employee"  # AND
sql <- glue_sql(
  "SELECT id, name,
  ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE to_tsvector(description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

## Creating indices

In the examples above, we performed tokenization of the search term and the 
`description` field at run time, e.g. when the query was executed. As our
database grows, this will soon become too cumbersome and degrade performance.

Adding an index to our database will maintain full-text search speed even with
large datasets. We have two different options:

1. Create an [index based on an expression](https://www.postgresql.org/docs/8.3/indexes-expressional.html).
2. Create a new field to hold the output of the `to_tsvector` function, and then
[index this new field](https://www.postgresql.org/docs/8.3/textsearch-tables.html).

### Creating an expression index

A simple way to create a full-text index is to include the `to_tsvector()`
expression in the definition of the index itself. Here, we add a
[Generalized Inverted Index (GIN)](https://www.postgresql.org/docs/current/gin-intro.html#:~:text=GIN%20stands%20for%20Generalized%20Inverted,appear%20within%20the%20composite%20items.)
index for the `description` column:

```{r}
sql = glue_sql(
  "CREATE INDEX description_idx ON datasets 
  USING gin(to_tsvector('english', description));",
  con = con
)
DBI::dbExecute(con, sql)
```

The same type of query we issued above will now take advantage of the 
`description_idx`:

```{r}
# search the description field using its index
term <- "questioning"
sql <- glue_sql(
  "SELECT id, name,
    ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

The `description` fields of new records, e.g those that are added later, will
automatically be added to the index. Let's create a new record for the `euro` 
dataset, for example.

```{r}
new_data = list(
  name = "euro", 
  title = "The built-in euro dataset from the datasets R package",
  description = gsub(
    "\r?\n|\r", " ", 
    "The data set euro contains the value of 1 Euro in all currencies
participating in the European monetary union (Austrian Schilling ATS, 
Belgian Franc BEF, German Mark DEM, Spanish Peseta ESP, Finnish Markka FIM, 
French Franc FRF, Irish Punt IEP, Italian Lira ITL, Luxembourg Franc LUF, 
Dutch Guilder NLG and Portuguese Escudo PTE). These conversion rates were 
fixed by the European Union on December 31, 1998. To convert old prices to 
Euro prices, divide by the respective rate and round to 2 digits.")
)
sql <- glue_sql(
  "INSERT INTO datasets ({`names(dataset)`*})
   VALUES ({new_data*});", 
  .con = con)
DBI::dbExecute(con, sql)
```

This new record will now be included in the search results for the term `data`,
for example:

```{r}
# search the description field using its index
term <- "data"
sql <- glue_sql(
  "SELECT id, name,
    ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE to_tsvector('english', description) @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

### Adding a tokenized field for full-text searches

Alternatively, another option is to create a new column to hold the output of 
the `to_tsvector()` function, and then to index it for future use. 
Let's create a new column `search_description_text`:

```{r}
# create a column to hold tokens for full text search
sql <- glue_sql(
  "ALTER TABLE datasets
   ADD COLUMN search_description_text tsvector;", 
  .con = con)
DBI::dbExecute(con, sql)
DBI::dbListFields(con, "datasets")
```

Next, we tokenize the `descriptions` field, and store the output in our
new `search_description_text` column:

```{r}
sql <- glue_sql(
  "UPDATE datasets
   SET search_description_text = to_tsvector('english', description);", 
  .con = con)
DBI::dbExecute(con, sql)
```

Here are the tokens generated from the `description` of the first record, for
example:

```{r}
DBI::dbGetQuery(con, 
                "SELECT name, search_description_text from datasets LIMIT 1;")
```

As before, we can add an index - but this time, we index the pre-tokenized
`search_description_text` column instead:

```{r}
# create the search index
sql <- glue_sql(
  "CREATE INDEX search_description_idx
   ON datasets
   USING gin(search_description_text);",
  .con = con)
DBI::dbExecute(con, sql)
```

Time to run our search again. When we search the `search_description_text` 
field, we can omit the `to_tsvector()` call, because its has been tokenized
already:

```{r}
# search the description field and show the matching location
term <- "data"
sql <- glue_sql(
  "SELECT id, name,
  ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE search_description_text @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

ðŸš¨ But _beware_: because we have precalculated the 
tokens, any new records added to the database will _not_ automatically be 
processed, nor will they be indexed!

Let's add a final record, the `morely` dataset:

```{r}
more_data = list(
  name = "morley", 
  title = "The built-in morley dataset from the datasets R package",
  description = gsub(
    "\r?\n|\r", " ", 
    "A classical data of Michelson (but not this one with Morley) on 
measurements done in 1879 on the speed of light. The data consists of five 
experiments, each consisting of 20 consecutive â€˜runsâ€™. The response is the speed
of light measurement, suitably coded (km/sec, with 299000 subtracted).")
)
```

To enter _this_ record, we not only have to populate the `name`, `title` and
`description` fields - but also the list of tokens derived from the 
`description` in the `search_description_text` column. In other words, we have
to execute the `to_tsvector` function inside our `INSERT` statement:

```{r}
sql <- glue_sql(
  "INSERT INTO datasets ({`names(dataset)`*}, search_description_text)
   VALUES ({more_data*}, to_tsvector({more_data[['description']]}));", 
  .con = con)
DBI::dbExecute(con, sql)
```

Now, our query returns both the original matches and the new record:

```{r}
# search the description field and show the matching location
term <- "data"
sql <- glue_sql(
  "SELECT id, name,
  ts_headline(description, to_tsquery('english', {term}),
     'StartSel = <,
      StopSel = >,
      MinWords = 5,
      MaxWords = 7,
      MaxFragments = 1')
  FROM datasets
  WHERE search_description_text @@ to_tsquery('english', {term})
  ORDER BY created;",
  .con = con)
DBI::dbGetQuery(con, sql)
```

### Choosing between indexing strategies

According to the
[Postgres documentation](https://www.postgresql.org/docs/8.3/textsearch-tables.html):

> One advantage of the separate-column approach over an expression index is that
it is not necessary to explicitly specify the text search configuration in 
queries in order to make use of the index. Another advantage is that searches 
will be faster, since it will not be necessary to redo the to_tsvector calls to
verify index matches. The expression-index approach is simpler to set up, 
however, and it requires less disk space since the tsvector representation is 
not stored explicitly.

That's it. Thanks again to 
[Anthony DeBarros'](https://www.wsj.com/news/author/anthony-debarros) 
for his excellent introduction to 
[Practical SQL](https://nostarch.com/practical-sql-2nd-edition)!

## Reproducibility

<details>
```{r}
sessioninfo::session_info()
```
</details>